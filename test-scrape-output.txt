{
  "content": [
    {
      "type": "text",
      "text": "# Scraped Content (29 URLs)\n\n**Token Allocation:** 1,103 tokens/URL (29 URLs, 32,000 total budget)\n**Status:** ‚úÖ 29 successful | ‚ùå 0 failed | üì¶ 1 batch(es)\n\n---\n\n## https://github.com/yigitkonur/llm-ocr\n\nGitHub - yigitkonur/llm-ocr: An open-source OCR API that leverages OpenAI's powerful language models with optimized performance techniques like parallel processing and batching to deliver high-quality text extraction from complex PDF documents. Ideal for businesses seeking efficient document digitization and data extraction solutions.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fllm-ocr)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fllm-ocr)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fllm-ocr)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[llm-ocr](/yigitkonur/llm-ocr)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fllm-ocr) You must be signed in to change notification settings\n-   [Fork 62](/login?return_to=%2Fyigitkonur%2Fllm-ocr)\n-   [Star 875](/login?return_to=%2Fyigitkonur%2Fllm-ocr)\n    \n\nAn open-source OCR API that leverages OpenAI's powerful language models with optimized performance techniques like parallel processing and batching to deliver high-quality text extraction from complex PDF documents. Ideal for businesses seeking efficient document digitization and data extraction solutions.\n\n### License\n\n[View license](/yigitkonur/llm-ocr/blob/main/LICENSE.md)\n\n[875 stars](/yigitkonur/llm-ocr/stargazers) [62 forks](/yigitkonur/llm-ocr/forks) [Branches](/yigitkonur/llm-ocr/branches) [Tags](/yigitkonur/llm-ocr/tags) [Activity](/yigitkonur/llm-ocr/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fllm-ocr)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fllm-ocr) You must be signed in to change notification settings\n\n# yigitkonur/llm-ocr\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/llm-ocr/branches)[Tags](/yigitkonur/llm-ocr/tags)\n\n[](/yigitkonur/llm-ocr/branches)[](/yigitkonur/llm-ocr/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[15 Commits](/yigitkonur/llm-ocr/commits/main/)\n\n[](/yigitkonur/llm-ocr/commits/main/)\n\n[swift\\_ocr](/yigitkonur/llm-ocr/tree/main/swift_ocr \"swift_ocr\")\n\n[swift\\_ocr](/yigitkonur/llm-ocr/tree/main/swift_ocr \"swift_ocr\")\n\n[.env.example](/yigitkonur/llm-ocr/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/llm-ocr/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/llm-ocr/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/llm-ocr/blob/main/.gitignore \".gitignore\")\n\n[LICENSE.md](/yigitkonur/llm-ocr/blob/main/LICENSE.md \"LICENSE.md\")\n\n[LICENSE.md](/yigitkonur/llm-ocr/blob/main/LICENSE.md \"LICENSE.md\")\n\n[README.md](/yigitkonur/llm-ocr/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/llm-ocr/blob/main/README.md \"README.md\")\n\n[main.py](/yigitkonur/llm-ocr/blob/main/main.py \"main.py\")\n\n[main.py](/yigitkonur/llm-ocr/blob/main/main.py \"main.py\")\n\n[pyproject.toml](/yigitkonur/llm-ocr/blob/main/pyproject.toml \"pyproject.toml\")\n\n[pyproject.toml](/yigitkonur/llm-ocr/blob/main/pyproject.toml \"pyproject.toml\")\n\n[requirements.txt](/yigitkonur/llm-ocr/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/llm-ocr/blob/main/requirements.txt \"requirements.txt\")\n\nView all files\n\n## Repository files navigation\n\n# ‚ö° Swift OCR ‚ö°\n\n[](#-swift-ocr-)\n\n### Stop squinting at PDFs. Start extracting clean markdown.\n\n[](#stop-squinting-at-pdfs-start-extracting-clean-markdown)\n\n**_The LLM-powered OCR engine that turns any PDF into beautifully formatted Markdown. It reads your documents like a human, handles messy layouts, and outputs text your AI can actually understand._**\n\n[![python](https://camo.githubusercontent.com/887e84deb889f9087fc5ec45dbdbfa27734a9547584c48bda0099f6ec04d49c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e382b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) [![fastapi](https://camo.githubusercontent.com/08c490aafb7be47e3d2b5ef602adc5c5c100fbd014c9679e08853db2e31b983e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d302e3130302b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/a36267201062be85c080591602f3434fd0318b82d469b78c94652b58afeeb543/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4147504c5f76332d4639413832352e7376673f7374796c653d666c61742d737175617265)](https://www.gnu.org/licenses/agpl-3.0) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![gpt-4/5 vision](https://camo.githubusercontent.com/590245edbe11240084c3c14aabb1ce3e134d1183979403eedda82d43690249e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f4750542d2d342f355f566973696f6e2d706f77657265645f62795f4f70656e41492d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/590245edbe11240084c3c14aabb1ce3e134d1183979403eedda82d43690249e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f4750542d2d342f355f566973696f6e2d706f77657265645f62795f4f70656e41492d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![markdown output](https://camo.githubusercontent.com/9d66074480f637ba347cd92eaf6bb09df8e2e680816297b86905dead44a3e5ab/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f939d5f6d61726b646f776e5f6f75747075742d7461626c65732c5f686561646572732c5f6c697374732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/9d66074480f637ba347cd92eaf6bb09df8e2e680816297b86905dead44a3e5ab/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f939d5f6d61726b646f776e5f6f75747075742d7461626c65732c5f686561646572732c5f6c697374732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**üí∞ Cost Breakdown**](#-cost-breakdown-stupidly-cheap) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üèóÔ∏è Project Structure**](#%EF%B8%8F-project-structure)\n\n* * *\n\n**Swift OCR** is the document processor your AI assistant wishes it had. Stop feeding your LLM screenshots and praying it reads them correctly. This tool acts like a professional transcriber, reading every page of your PDF, intelligently handling tables, headers, and mixed layouts, then packaging everything into perfectly structured Markdown so your AI can actually work with it.\n\n### üß†\n\n[](#)\n\n**GPT-4 Vision**  \nHuman-level reading accuracy\n\n### ‚ö°\n\n[](#-1)\n\n**Parallel Processing**  \nMulti-page PDFs in seconds\n\n### üìù\n\n[](#-2)\n\n**Clean Markdown**  \nTables, headers, lists‚Äîall formatted\n\nHow it slaps:\n\n-   **You:** `curl -X POST \"http://localhost:8000/ocr\" -F \"file=@messy_document.pdf\"`\n-   **Swift OCR:** Converts pages ‚Üí Sends to GPT-4 Vision ‚Üí Formats as Markdown\n-   **You:** Get perfectly structured text with tables, headers, and lists intact.\n-   **Result:** Your AI finally understands that 50-page contract. ‚òï\n\n* * *\n\n## üìπ Demo\n\n[](#-demo)\n\nvideo.mp4\n\n_Demo video showcasing the conversion of NASA's Apollo 17 flight documents‚Äîcomplete with unorganized, horizontally and vertically oriented pages‚Äîinto well-structured Markdown format without breaking a sweat._\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nManually extracting text from PDFs is a vibe-killer. Swift OCR makes traditional OCR look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Swift OCR Way (Glory)**\n\n1.  Run Tesseract. Get garbled text.\n2.  Tables? What tables? Just random words now.\n3.  Manually fix formatting for 2 hours.\n4.  Feed broken context to your AI.\n5.  Get a useless answer. Cry.\n\n1.  Upload PDF to Swift OCR.\n2.  Get perfectly formatted Markdown.\n3.  Tables intact. Headers preserved.\n4.  Feed clean context to your AI.\n5.  Get genius-level answers. Go grab a coffee. ‚òï\n\nWe're not just running basic OCR. We're using **GPT-4 Vision** to actually _understand_ your documents‚Äîhandling rotated pages, complex tables, mixed layouts, and even describing images for accessibility.\n\n* * *\n\n## üí∞ Cost Breakdown: Stupidly Cheap\n\n[](#-cost-breakdown-stupidly-cheap)\n\nOur solution offers an optimal balance of affordability and accuracy that makes enterprise OCR solutions look like highway robbery.\n\nMetric\n\nValue\n\n**Avg tokens/page**\n\n~1,500 (including prompt)\n\n**GPT-4o input cost**\n\n$5 per million tokens\n\n**GPT-4o output cost**\n\n$15 per million tokens\n\n**Cost per 1,000 pages**\n\n**~$15**\n\n### üí° Want It Even Cheaper?\n\n[](#-want-it-even-cheaper)\n\nOptimization\n\nCost per 1,000 pages\n\n**GPT-4o (default)**\n\n~$15\n\n**GPT-4o mini**\n\n~$8\n\n**Batch API**\n\n~$4\n\n### üÜö Market Comparison\n\n[](#-market-comparison)\n\nSolution\n\nCost per 1,000 pages\n\nTables?\n\nMarkdown?\n\n**Swift OCR**\n\n**$15**\n\n‚úÖ Perfect\n\n‚úÖ Native\n\nCloudConvert (PDFTron)\n\n~$30\n\n‚ö†Ô∏è Basic\n\n‚ùå No\n\nAdobe Acrobat API\n\n~$50+\n\n‚úÖ Good\n\n‚ùå No\n\nTesseract (free)\n\n$0\n\n‚ùå Broken\n\n‚ùå No\n\n> **Bottom line:** Half the cost of competitors, 10x the quality. It's not just about being cheaper‚Äîit's about getting output you can actually use.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   **Python 3.8+**\n-   **Azure OpenAI** account (with GPT-4 Vision deployment)\n\n### Installation\n\n[](#installation)\n\n# Clone the repo\ngit clone https://github.com/yigitkonur/swift-ocr-llm-powered-pdf-to-markdown.git\ncd swift-ocr-llm-powered-pdf-to-markdown\n\n# Create virtual environment (recommended)\npython3 -m venv venv\nsource venv/bin/activate  # Windows: venv\\\\Scripts\\\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n### Configure Environment\n\n[](#configure-environment)\n\nCreate a `.env` file in the root directory:\n\n# Required\nOPENAI\\_API\\_KEY\\=your\\_openai\\_api\\_key\nAZURE\\_OPENAI\\_ENDPOINT\\=https://your-resource.openai.azure.com/\nOPENAI\\_DEPLOYMENT\\_ID\\=your\\_gpt4\\_vision\\_deployment\n\n# Optional (sensible defaults)\nOPENAI\\_API\\_VERSION\\=gpt-4o\nBATCH\\_SIZE\\=1                        # Images per OCR request (1-10)\nMAX\\_CONCURRENT\\_OCR\\_REQUESTS\\=5       # Parallel OCR calls\nMAX\\_CONCURRENT\\_PDF\\_CONVERSION\\=4     # Parallel page rendering\n\n### Run It\n\n[](#run-it)\n\n# Option 1: Classic uvicorn (backward compatible)\nuvicorn main:app --reload\n\n# Option 2: Using the new package\nuvicorn swift\\_ocr.app:app --reload\n\n# Option 3: As a Python module\npython -m swift\\_ocr\n\n# Option 4: With CLI arguments\npython -m swift\\_ocr --host 0.0.0.0 --port 8080 --workers 4\n\nüéâ **API is now live at `http://127.0.0.1:8000`**\n\n> **‚ú® Pro tip:** Check out the auto-generated docs at `http://127.0.0.1:8000/docs`\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\n### API Endpoint\n\n[](#api-endpoint)\n\n**POST** `/ocr`\n\nAccept a PDF file upload OR a URL to a PDF. Returns beautifully formatted Markdown.\n\n### Examples\n\n[](#examples)\n\n**Upload a PDF file:**\n\ncurl -X POST \"http://127.0.0.1:8000/ocr\" \\\\\n  -F \"file=@/path/to/your/document.pdf\"\n\n**Process a PDF from URL:**\n\ncurl -X POST \"http://127.0.0.1:8000/ocr\" \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\"url\": \"https://example.com/document.pdf\"}'\n\n### Response\n\n[](#response)\n\n{\n  \"text\": \"\\# Document Title\\\\n\\\\n\\## Section 1\\\\n\\\\nExtracted text with \\*\\*formatting\\*\\* preserved...\\\\n\\\\n| Column 1 | Column 2 |\\\\n|----------|----------|\\\\n| Data     | Data     |\"\n}\n\n### Response (v2.0+)\n\n[](#response-v20)\n\nThe new response includes additional metadata:\n\n{\n  \"text\": \"\\# Document Title\\\\n\\\\n\\## Section 1\\\\n\\\\nExtracted text...\",\n  \"status\": \"success\",\n  \"pages\\_processed\": 5,\n  \"processing\\_time\\_ms\": 1234\n}\n\n### Health Check\n\n[](#health-check)\n\ncurl http://127.0.0.1:8000/health\n\n{\n  \"status\": \"healthy\",\n  \"version\": \"2.0.0\",\n  \"timestamp\": \"2024-01-01T00:00:00Z\",\n  \"openai\\_configured\": true\n}\n\n### Error Codes\n\n[](#error-codes)\n\nCode\n\nMeaning\n\n`200`\n\nSuccess‚ÄîMarkdown text returned\n\n`400`\n\nBad request (no file/URL, or both provided)\n\n`422`\n\nValidation error\n\n`429`\n\nRate limited‚Äîretry with backoff\n\n`500`\n\nProcessing error\n\n`504`\n\nTimeout downloading PDF\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üß† GPT-4 Vision**  \n`Human-level OCR`\n\nUses OpenAI's most capable vision model to read documents\n\nActually understands context, not just character shapes\n\n**‚ö° Parallel Processing**  \n`Multiprocessing + async`\n\nConverts PDF pages and calls OCR in parallel\n\n50-page PDF in seconds, not minutes\n\n**üìä Table Preservation**  \n`Markdown tables`\n\nDetects and formats tables as proper Markdown\n\nYour data stays structured, not flattened to gibberish\n\n**üîÑ Smart Batching**  \n`Configurable batch size`\n\nGroups pages to optimize API calls vs accuracy\n\nBalance speed and cost for your use case\n\n**üõ°Ô∏è Retry with Backoff**  \n`Exponential backoff`\n\nAutomatically retries on rate limits and timeouts\n\nHandles API hiccups without crashing\n\n**üìÑ Flexible Input**  \n`File upload or URL`\n\nAccept PDFs directly or fetch from any URL\n\nWorks with your existing workflow\n\n**üñºÔ∏è Image Descriptions**  \n`Accessibility-friendly`\n\nDescribes non-text elements: `[Image: description]`\n\nContext your AI can actually use\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\nAll settings are managed via environment variables. Tune these for your workload:\n\nVariable\n\nDefault\n\nDescription\n\n`OPENAI_API_KEY`\n\n‚Äî\n\nYour Azure OpenAI API key\n\n`AZURE_OPENAI_ENDPOINT`\n\n‚Äî\n\nYour Azure OpenAI endpoint URL\n\n`OPENAI_DEPLOYMENT_ID`\n\n‚Äî\n\nYour GPT-4 Vision deployment ID\n\n`OPENAI_API_VERSION`\n\n`gpt-4o`\n\nAPI version\n\n`BATCH_SIZE`\n\n`1`\n\nPages per OCR request (1-10). Higher = faster but less accurate\n\n`MAX_CONCURRENT_OCR_REQUESTS`\n\n`5`\n\nParallel OCR calls. Increase for throughput\n\n`MAX_CONCURRENT_PDF_CONVERSION`\n\n`4`\n\nParallel page renders. Match your CPU cores\n\n### Performance Tuning Tips\n\n[](#performance-tuning-tips)\n\n-   **High accuracy, slower:** `BATCH_SIZE=1`\n-   **Balanced:** `BATCH_SIZE=5`, `MAX_CONCURRENT_OCR_REQUESTS=10`\n-   **Maximum throughput:** `BATCH_SIZE=10`, `MAX_CONCURRENT_OCR_REQUESTS=20` (watch rate limits!)\n\n* * *\n\n## üèóÔ∏è Project Structure\n\n[](#Ô∏è-project-structure)\n\nWorld-class Python engineering with atomic modules and clean separation of concerns:\n\n```\nswift_ocr/\n‚îú‚îÄ‚îÄ __init__.py              # Package init with version\n‚îú‚îÄ‚îÄ __main__.py              # CLI entry point (python -m swift_ocr)\n‚îú‚îÄ‚îÄ app.py                   # FastAPI app factory\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # Pydantic Settings (type-safe config)\n‚îú‚îÄ‚îÄ core/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Custom exception hierarchy\n‚îÇ   ‚îú‚îÄ‚îÄ logging.py           # Structured logging setup\n‚îÇ   ‚îî‚îÄ‚îÄ retry.py             # Exponential backoff utilities\n‚îú‚îÄ‚îÄ schemas/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ ocr.py               # Pydantic request/response models\n‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ ocr.py               # OpenAI Vision OCR service\n‚îÇ   ‚îî‚îÄ‚îÄ pdf.py               # PDF conversion service\n‚îî‚îÄ‚îÄ api/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ deps.py              # Dependency injection\n    ‚îú‚îÄ‚îÄ exceptions.py        # FastAPI exception handlers\n    ‚îú‚îÄ‚îÄ router.py            # Route aggregation\n    ‚îî‚îÄ‚îÄ routes/\n        ‚îú‚îÄ‚îÄ __init__.py\n        ‚îú‚îÄ‚îÄ health.py        # Health check endpoints\n        ‚îî‚îÄ‚îÄ ocr.py           # OCR endpoints\n```\n\n**Key architectural decisions**\n\nPattern\n\nImplementation\n\nBenefit\n\n**Pydantic Settings**\n\n`config/settings.py`\n\nType-safe config with `.env` support and validation\n\n**Dependency Injection**\n\n`api/deps.py`\n\nTestable, swappable services\n\n**Custom Exceptions**\n\n`core/exceptions.py`\n\nRich error context with proper HTTP status codes\n\n**Retry with Backoff**\n\n`core/retry.py`\n\nHandles rate limits and transient failures\n\n**App Factory**\n\n`app.py`\n\nConfigurable app creation for testing\n\n**Typed Throughout**\n\n`py.typed` marker\n\nFull mypy compatibility\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**\"Missing required environment variables\"**\n\nCheck your `.env` file has all three required variables: `OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_DEPLOYMENT_ID`\n\n**Rate limit errors (429)**\n\nReduce `MAX_CONCURRENT_OCR_REQUESTS` or `BATCH_SIZE`. The retry logic will handle temporary limits automatically.\n\n**Timeout errors**\n\nLarge PDFs take time. The system has exponential backoff built in‚Äîgive it a moment.\n\n**Garbled output**\n\nMake sure your PDF isn't password-protected or corrupted. Try opening it locally first.\n\n**Tables not formatting correctly**\n\nSome extremely complex tables may need `BATCH_SIZE=1` for best accuracy.\n\n**\"Failed to initialize OpenAI client\"**\n\nVerify your Azure endpoint URL format: `https://your-resource.openai.azure.com/`\n\n* * *\n\n## üìú License\n\n[](#-license)\n\nThis project uses **PyMuPDF** for PDF processing, which requires the **GNU AGPL v3.0** license.\n\n> **Want MIT instead?** Fork this project and swap PyMuPDF for `pdf2image` + Poppler. The rest of the code is yours to use freely.\n\n```\nGNU AFFERO GENERAL PUBLIC LICENSE\nVersion 3, 19 November 2007\n\nCopyright (C) 2024 Yiƒüit Konur\n```\n\nSee [LICENSE.md](/yigitkonur/llm-ocr/blob/main/LICENSE.md) for the full license text.\n\n* * *\n\n**Built with üî• because manually transcribing PDFs is a soul-crushing waste of time.**\n\n[Report Bug](https://github.com/yigitkonur/swift-ocr-llm-powered-pdf-to-markdown/issues) ‚Ä¢ [Request Feature](https://github.com/yigitkonur/swift-ocr-llm-powered-pdf-to-markdown/issues)\n\n## About\n\nAn open-source OCR API that leverages OpenAI's powerful language models with optimized performance techniques like parallel processing and batching to deliver high-quality text extraction from complex PDF documents. Ideal for businesses seeking efficient document digitization and data extraction solutions.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[View license](#License-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/llm-ocr/activity)\n\n### Stars\n\n[**875** stars](/yigitkonur/llm-ocr/stargazers)\n\n### Watchers\n\n[**7** watching](/yigitkonur/llm-ocr/watchers)\n\n### Forks\n\n[**62** forks](/yigitkonur/llm-ocr/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fllm-ocr&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/llm-ocr/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=llm-ocr)\n\nNo packages published  \n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## [Contributors 2](/yigitkonur/llm-ocr/graphs/contributors)\n\n¬†¬†### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/llm-ocr/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/rust-10k-req-demo\n\nGitHub - yigitkonur/rust-10k-req-demo: A high-performance Rust tool for sending API requests (to LLMs in my case) with built-in weighted load balancing, retry mechanisms, and rate limiting. Using hyper for fast request handling, it manages large volumes of asynchronous requests and is optimized for 10K request per second.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Frust-10k-req-demo)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Frust-10k-req-demo)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Frust-10k-req-demo)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[rust-10k-req-demo](/yigitkonur/rust-10k-req-demo)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Frust-10k-req-demo) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Frust-10k-req-demo)\n-   [Star 17](/login?return_to=%2Fyigitkonur%2Frust-10k-req-demo)\n    \n\nA high-performance Rust tool for sending API requests (to LLMs in my case) with built-in weighted load balancing, retry mechanisms, and rate limiting. Using hyper for fast request handling, it manages large volumes of asynchronous requests and is optimized for 10K request per second.\n\n### License\n\n[MIT license](/yigitkonur/rust-10k-req-demo/blob/main/LICENSE)\n\n[17 stars](/yigitkonur/rust-10k-req-demo/stargazers) [1 fork](/yigitkonur/rust-10k-req-demo/forks) [Branches](/yigitkonur/rust-10k-req-demo/branches) [Tags](/yigitkonur/rust-10k-req-demo/tags) [Activity](/yigitkonur/rust-10k-req-demo/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Frust-10k-req-demo)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Frust-10k-req-demo) You must be signed in to change notification settings\n\n# yigitkonur/rust-10k-req-demo\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/rust-10k-req-demo/branches)[Tags](/yigitkonur/rust-10k-req-demo/tags)\n\n[](/yigitkonur/rust-10k-req-demo/branches)[](/yigitkonur/rust-10k-req-demo/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[8 Commits](/yigitkonur/rust-10k-req-demo/commits/main/)\n\n[](/yigitkonur/rust-10k-req-demo/commits/main/)\n\n[benches](/yigitkonur/rust-10k-req-demo/tree/main/benches \"benches\")\n\n[benches](/yigitkonur/rust-10k-req-demo/tree/main/benches \"benches\")\n\n[examples](/yigitkonur/rust-10k-req-demo/tree/main/examples \"examples\")\n\n[examples](/yigitkonur/rust-10k-req-demo/tree/main/examples \"examples\")\n\n[src](/yigitkonur/rust-10k-req-demo/tree/main/src \"src\")\n\n[src](/yigitkonur/rust-10k-req-demo/tree/main/src \"src\")\n\n[.gitignore](/yigitkonur/rust-10k-req-demo/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/rust-10k-req-demo/blob/main/.gitignore \".gitignore\")\n\n[Cargo.toml](/yigitkonur/rust-10k-req-demo/blob/main/Cargo.toml \"Cargo.toml\")\n\n[Cargo.toml](/yigitkonur/rust-10k-req-demo/blob/main/Cargo.toml \"Cargo.toml\")\n\n[LICENSE](/yigitkonur/rust-10k-req-demo/blob/main/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/rust-10k-req-demo/blob/main/LICENSE \"LICENSE\")\n\n[README.md](/yigitkonur/rust-10k-req-demo/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/rust-10k-req-demo/blob/main/README.md \"README.md\")\n\nView all files\n\n## Repository files navigation\n\n# üî• Blaze API üî•\n\n[](#-blaze-api-)\n\n### Stop waiting for API responses. Start blazing through them.\n\n[](#stop-waiting-for-api-responses-start-blazing-through-them)\n\n**_The ultimate batch API client for your LLM workloads. It load-balances across endpoints, retries intelligently, and processes 10,000+ requests per second on a laptop._**\n\n[![crates.io](https://camo.githubusercontent.com/f70f896d47dfd3dcd1629f79429eed4f004688cbcbe044708461e07977bc73e2/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f626c617a652d6170692e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](https://crates.io/crates/blaze-api) [![rust](https://camo.githubusercontent.com/27cf849475ce885b34707dfd00b865f70a95e44b2f4ef8ebfff397225d543d77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f727573742d312e37352b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](https://opensource.org/licenses/MIT) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![zero config](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![10k rps](https://camo.githubusercontent.com/8dbb38120b2c0d87ba8c33fdef473939a7d6f4bf5df325b644fb3c60f4783a42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f31304b2b5f7265712f7365632d6f6e5f6d6f646573745f68617264776172652d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/8dbb38120b2c0d87ba8c33fdef473939a7d6f4bf5df325b644fb3c60f4783a42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f31304b2b5f7265712f7365632d6f6e5f6d6f646573745f68617264776172652d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üÜö Why Blaze**](#-why-blaze-slaps-other-methods)\n\n* * *\n\n**Blaze API** is the batch processor your LLM workloads deserve. Stop writing brittle Python scripts that crash at 100 req/sec. This tool acts like a fleet of pro API consumers, intelligently distributing requests across endpoints, handling failures gracefully, and maxing out your API capacity without breaking a sweat.\n\n### ‚ö°\n\n[](#)\n\n**Blazing Fast**  \n10K+ req/sec on 8 cores\n\n### üéØ\n\n[](#-1)\n\n**Smart Load Balancing**  \nWeighted distribution across endpoints\n\n### üîÑ\n\n[](#-2)\n\n**Auto Retry**  \nExponential backoff with jitter\n\n### üìä\n\n[](#-3)\n\n**Real-time Stats**  \nProgress, RPS, latency tracking\n\nHow it slaps:\n\n-   **You:** `blaze -i requests.jsonl -o results.jsonl`\n-   **Blaze:** Load balances, retries failures, tracks progress, writes results.\n-   **You:** Go grab a coffee while 100K requests complete. ‚òï\n-   **Result:** Perfectly formatted JSONL with every response. Zero babysitting.\n\n* * *\n\n## üí• Why Blaze Slaps Other Methods\n\n[](#-why-blaze-slaps-other-methods)\n\nManually scripting API requests is a vibe-killer. Blaze makes other methods look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Blaze Way (Glory)**\n\n1.  Write Python script with asyncio.\n2.  Hit GIL limits at 500 req/sec.\n3.  Script crashes, lose progress.\n4.  Add retry logic, still flaky.\n5.  Manually restart, pray it works.\n\n1.  `blaze -i data.jsonl -o out.jsonl`\n2.  Watch the progress bar fly.\n3.  Failures auto-retry with backoff.\n4.  Results stream to disk instantly.\n5.  Go grab a coffee. ‚òï\n\nWe're not just sending requests. We're building a **high-throughput, fault-tolerant pipeline** with weighted load balancing, connection pooling, and intelligent retry logic that actually respects your API provider's limits.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\nPlatform\n\nMethod\n\nCommand\n\nü¶Ä **All**\n\nCargo\n\n`cargo install blaze-api`\n\nüçé **macOS**\n\nHomebrew\n\n`brew install yigitkonur/tap/blaze`\n\nüêß **Linux**\n\nBinary\n\nSee [releases](https://github.com/yigitkonur/blaze-api/releases)\n\nü™ü **Windows**\n\nBinary\n\nSee [releases](https://github.com/yigitkonur/blaze-api/releases)\n\n### ü¶Ä From Source (Recommended for Development)\n\n[](#-from-source-recommended-for-development)\n\n# Clone and build\ngit clone https://github.com/yigitkonur/blaze-api.git\ncd blaze-api\ncargo build --release\n\n# Binary is at ./target/release/blaze\n\n### üì¶ From crates.io\n\n[](#-from-cratesio)\n\ncargo install blaze-api\n\n> **‚ú® Zero Config:** After installation, `blaze` is ready to go. Just point it at your JSONL file!\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\nThe workflow is dead simple.\n\n### Basic Usage\n\n[](#basic-usage)\n\n# Process requests and save results\nblaze --input requests.jsonl --output results.jsonl\n\n# Short flags work too\nblaze -i requests.jsonl -o results.jsonl\n\n# High-throughput mode (10K req/sec)\nblaze -i data.jsonl -o out.jsonl --rate 10000 --workers 200\n\n### With Custom Endpoints\n\n[](#with-custom-endpoints)\n\n# Use a config file for multiple endpoints\nblaze -i requests.jsonl -o results.jsonl --config endpoints.json\n\n# Or set via environment\nexport BLAZE\\_ENDPOINT\\_URL=\"https://api.openai.com/v1/completions\"\nexport BLAZE\\_API\\_KEY=\"sk-...\"\nexport BLAZE\\_MODEL=\"gpt-4\"\nblaze -i requests.jsonl -o results.jsonl\n\n### Input Format\n\n[](#input-format)\n\nYour `requests.jsonl` file should have one JSON object per line:\n\n{\"input\": \"What is the capital of France?\"}\n{\"input\": \"Explain quantum computing in simple terms.\"}\n{\"input\": \"Write a haiku about Rust programming.\"}\n\nOr with custom request bodies:\n\n{\"body\": {\"messages\": \\[{\"role\": \"user\", \"content\": \"Hello!\"}\\], \"model\": \"gpt-4\"}}\n{\"body\": {\"messages\": \\[{\"role\": \"system\", \"content\": \"You are helpful.\"}, {\"role\": \"user\", \"content\": \"Hi!\"}\\]}}\n\n### Output Format\n\n[](#output-format)\n\nResults are written as JSONL:\n\n{\"input\": \"What is the capital of France?\", \"response\": {\"choices\": \\[...\\]}, \"metadata\": {\"endpoint\": \"...\", \"latency\\_ms\": 234, \"attempts\": 1}}\n{\"input\": \"Explain quantum computing...\", \"response\": {\"choices\": \\[...\\]}, \"metadata\": {\"endpoint\": \"...\", \"latency\\_ms\": 189, \"attempts\": 1}}\n\nErrors go to `errors.jsonl`:\n\n{\"input\": \"...\", \"error\": \"HTTP 429: Rate limit exceeded\", \"status\\_code\": 429, \"attempts\": 3}\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**‚ö° Async Everything**  \n`Tokio runtime`\n\nNon-blocking I/O with work-stealing scheduler\n\nSaturates your CPU cores efficiently\n\n**üéØ Weighted Load Balancing**  \n`Smart distribution`\n\nRoute traffic based on endpoint capacity\n\nMax out multiple API keys simultaneously\n\n**üîÑ Exponential Backoff**  \n`With jitter`\n\nIntelligent retry with randomized delays\n\nRespects rate limits, avoids thundering herd\n\n**üìä Real-time Progress**  \n`Live stats`\n\nRPS, success rate, latency, ETA\n\nKnow exactly what's happening\n\n**üîå Connection Pooling**  \n`HTTP/2 keep-alive`\n\nReuses connections across requests\n\nEliminates TCP handshake overhead\n\n**üíæ Streaming Output**  \n`Immediate writes`\n\nResults written as they complete\n\nNever lose progress on crashes\n\n**üè• Health Tracking**  \n`Per-endpoint`\n\nAutomatic failover on errors\n\nUnhealthy endpoints get cooled off\n\n**üîß Flexible Config**  \n`CLI + ENV + JSON`\n\nConfigure via args, env vars, or files\n\nFits any workflow\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\n### CLI Flags\n\n[](#cli-flags)\n\n```\nUSAGE:\n    blaze [OPTIONS] --input <FILE>\n\nOPTIONS:\n    -i, --input <FILE>        Path to JSONL input file [env: BLAZE_INPUT]\n    -o, --output <FILE>       Path for successful responses [env: BLAZE_OUTPUT]\n    -e, --errors <FILE>       Path for error responses [default: errors.jsonl]\n    -r, --rate <N>            Max requests per second [default: 1000]\n    -w, --workers <N>         Concurrent workers [default: 50]\n    -t, --timeout <SECS>      Request timeout [default: 30]\n    -a, --max-attempts <N>    Max retry attempts [default: 3]\n    -c, --config <FILE>       Endpoint config file (JSON)\n    -v, --verbose             Enable debug logging\n        --json-logs           Output logs as JSON\n        --no-progress         Disable progress bar\n        --dry-run             Validate config without processing\n    -h, --help                Print help\n    -V, --version             Print version\n```\n\n### Environment Variables\n\n[](#environment-variables)\n\nAll options can be set via environment variables with `BLAZE_` prefix:\n\nexport BLAZE\\_INPUT=\"requests.jsonl\"\nexport BLAZE\\_OUTPUT=\"results.jsonl\"\nexport BLAZE\\_RATE=\"5000\"\nexport BLAZE\\_WORKERS=\"100\"\nexport BLAZE\\_ENDPOINT\\_URL=\"https://api.example.com/v1/completions\"\nexport BLAZE\\_API\\_KEY=\"your-api-key\"\nexport BLAZE\\_MODEL=\"gpt-4\"\n\n### Configuration File\n\n[](#configuration-file)\n\nFor multiple endpoints, create `endpoints.json`:\n\n{\n  \"endpoints\": \\[\n    {\n      \"url\": \"https://api.openai.com/v1/completions\",\n      \"weight\": 2,\n      \"api\\_key\": \"sk-key-1\",\n      \"model\": \"gpt-4\",\n      \"max\\_concurrent\": 100\n    },\n    {\n      \"url\": \"https://api.openai.com/v1/completions\",\n      \"weight\": 1,\n      \"api\\_key\": \"sk-key-2\",\n      \"model\": \"gpt-4\",\n      \"max\\_concurrent\": 50\n    }\n  \\],\n  \"request\": {\n    \"timeout\": \"30s\",\n    \"rate\\_limit\": 5000,\n    \"workers\": 100\n  },\n  \"retry\": {\n    \"max\\_attempts\": 3,\n    \"initial\\_backoff\": \"100ms\",\n    \"max\\_backoff\": \"10s\",\n    \"multiplier\": 2.0\n  }\n}\n\nThen run:\n\nblaze -i requests.jsonl -o results.jsonl --config endpoints.json\n\n* * *\n\n## üìà Performance Tips\n\n[](#-performance-tips)\n\n### Maximize Throughput\n\n[](#maximize-throughput)\n\n# For maximum speed (adjust based on your API limits)\nblaze -i data.jsonl -o out.jsonl \\\\\n  --rate 10000 \\\\\n  --workers 200 \\\\\n  --timeout 60\n\n### Balance Load Across Keys\n\n[](#balance-load-across-keys)\n\n{\n  \"endpoints\": \\[\n    {\"url\": \"...\", \"api\\_key\": \"key-1\", \"weight\": 3, \"max\\_concurrent\": 150},\n    {\"url\": \"...\", \"api\\_key\": \"key-2\", \"weight\": 2, \"max\\_concurrent\": 100},\n    {\"url\": \"...\", \"api\\_key\": \"key-3\", \"weight\": 1, \"max\\_concurrent\": 50}\n  \\]\n}\n\n### Handle Rate Limits Gracefully\n\n[](#handle-rate-limits-gracefully)\n\n{\n  \"retry\": {\n    \"max\\_attempts\": 5,\n    \"initial\\_backoff\": \"500ms\",\n    \"max\\_backoff\": \"30s\",\n    \"multiplier\": 2.0\n  }\n}\n\n* * *\n\n## üõ†Ô∏è For Developers & Tinkerers\n\n[](#Ô∏è-for-developers--tinkerers)\n\n### Building from Source\n\n[](#building-from-source)\n\ngit clone https://github.com/yigitkonur/blaze-api.git\ncd blaze-api\n\n# Debug build\ncargo build\n\n# Release build (optimized)\ncargo build --release\n\n# Run tests\ncargo test\n\n# Run benchmarks\ncargo bench\n\n### Using as a Library\n\n[](#using-as-a-library)\n\nuse blaze\\_api::{Config, EndpointConfig, Processor};\n\n#\\[tokio::main\\]\nasync fn main() -> anyhow::Result<()\\> {\n    let config = Config {\n        endpoints: vec!\\[EndpointConfig {\n            url: \"https://api.example.com/v1/completions\".to\\_string(),\n            weight: 1,\n            api\\_key: Some(\"your-key\".to\\_string()),\n            model: Some(\"gpt-4\".to\\_string()),\n            max\\_concurrent: 100,\n        }\\],\n        ..Default::default()\n    };\n\n    let processor = Processor::new(config)?;\n    let result = processor.process\\_file(\n        \"requests.jsonl\".into(),\n        Some(\"results.jsonl\".into()),\n        \"errors.jsonl\".into(),\n        true,\n    ).await?;\n\n    result.print\\_summary();\n    Ok(())\n}\n\n### Project Structure\n\n[](#project-structure)\n\n```\nsrc/\n‚îú‚îÄ‚îÄ lib.rs        # Library entry point\n‚îú‚îÄ‚îÄ main.rs       # CLI binary\n‚îú‚îÄ‚îÄ config.rs     # Configuration management\n‚îú‚îÄ‚îÄ client.rs     # HTTP client with retry logic\n‚îú‚îÄ‚îÄ endpoint.rs   # Load balancer implementation\n‚îú‚îÄ‚îÄ processor.rs  # Main processing orchestration\n‚îú‚îÄ‚îÄ request.rs    # Request/response types\n‚îú‚îÄ‚îÄ tracker.rs    # Statistics tracking\n‚îî‚îÄ‚îÄ error.rs      # Error types\n```\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**\"Too many open files\"**\n\nIncrease ulimit: `ulimit -n 65535`\n\n**Connection timeouts**\n\nIncrease `--timeout` or reduce `--workers`\n\n**Rate limit errors (429)**\n\nLower `--rate` or add more API keys\n\n**Memory usage high**\n\nReduce `--workers` for large requests\n\n**Progress bar not showing**\n\nDon't pipe output, or use `--no-progress --json-logs`\n\n**Build Issues:**\n\nProblem\n\nSolution\n\n**OpenSSL errors**\n\nInstall OpenSSL dev: `apt install libssl-dev` or use `--features rustls`\n\n**Rust version error**\n\nUpdate Rust: `rustup update stable` (requires 1.75+)\n\n* * *\n\n## ü§ù Contributing\n\n[](#-contributing)\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n# Fork the repo, then:\ngit clone https://github.com/YOUR\\_USERNAME/blaze-api.git\ncd blaze-api\ncargo test\n# Make your changes\ncargo fmt\ncargo clippy\ncargo test\n# Submit PR\n\n* * *\n\n## üìÑ License\n\n[](#-license)\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n* * *\n\n**Built with üî• because waiting for API responses is a soul-crushing waste of time.**\n\n[‚¨Ü Back to Top](#-blaze-api-)\n\n## About\n\nA high-performance Rust tool for sending API requests (to LLMs in my case) with built-in weighted load balancing, retry mechanisms, and rate limiting. Using hyper for fast request handling, it manages large volumes of asynchronous requests and is optimized for 10K request per second.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/rust-10k-req-demo/activity)\n\n### Stars\n\n[**17** stars](/yigitkonur/rust-10k-req-demo/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/rust-10k-req-demo/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/rust-10k-req-demo/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Frust-10k-req-demo&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/rust-10k-req-demo/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=rust-10k-req-demo)\n\nNo packages published  \n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [Rust 100.0%](/yigitkonur/rust-10k-req-demo/search?l=rust)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/telnyx-llm-call\n\nGitHub - yigitkonur/telnyx-llm-call: A script that uses Telnyx API to make bulk calls to a given list of numbers and analyzes the audio recordings with Whisper. This way, we don‚Äôt have to read all the transcriptions.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ftelnyx-llm-call)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ftelnyx-llm-call)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Ftelnyx-llm-call)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[telnyx-llm-call](/yigitkonur/telnyx-llm-call)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Ftelnyx-llm-call) You must be signed in to change notification settings\n-   [Fork 23](/login?return_to=%2Fyigitkonur%2Ftelnyx-llm-call)\n-   [Star 136](/login?return_to=%2Fyigitkonur%2Ftelnyx-llm-call)\n    \n\nA script that uses Telnyx API to make bulk calls to a given list of numbers and analyzes the audio recordings with Whisper. This way, we don‚Äôt have to read all the transcriptions.\n\n[136 stars](/yigitkonur/telnyx-llm-call/stargazers) [23 forks](/yigitkonur/telnyx-llm-call/forks) [Branches](/yigitkonur/telnyx-llm-call/branches) [Tags](/yigitkonur/telnyx-llm-call/tags) [Activity](/yigitkonur/telnyx-llm-call/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Ftelnyx-llm-call)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Ftelnyx-llm-call) You must be signed in to change notification settings\n\n# yigitkonur/telnyx-llm-call\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/telnyx-llm-call/branches)[Tags](/yigitkonur/telnyx-llm-call/tags)\n\n[](/yigitkonur/telnyx-llm-call/branches)[](/yigitkonur/telnyx-llm-call/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/telnyx-llm-call/commits/main/)\n\n[](/yigitkonur/telnyx-llm-call/commits/main/)\n\n[src/telnyx\\_transcribe](/yigitkonur/telnyx-llm-call/tree/main/src/telnyx_transcribe \"This path skips through empty directories\")\n\n[src/telnyx\\_transcribe](/yigitkonur/telnyx-llm-call/tree/main/src/telnyx_transcribe \"This path skips through empty directories\")\n\n[tests](/yigitkonur/telnyx-llm-call/tree/main/tests \"tests\")\n\n[tests](/yigitkonur/telnyx-llm-call/tree/main/tests \"tests\")\n\n[.env.example](/yigitkonur/telnyx-llm-call/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/telnyx-llm-call/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/telnyx-llm-call/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/telnyx-llm-call/blob/main/.gitignore \".gitignore\")\n\n[README.md](/yigitkonur/telnyx-llm-call/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/telnyx-llm-call/blob/main/README.md \"README.md\")\n\n[pyproject.toml](/yigitkonur/telnyx-llm-call/blob/main/pyproject.toml \"pyproject.toml\")\n\n[pyproject.toml](/yigitkonur/telnyx-llm-call/blob/main/pyproject.toml \"pyproject.toml\")\n\n[requirements.txt](/yigitkonur/telnyx-llm-call/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/telnyx-llm-call/blob/main/requirements.txt \"requirements.txt\")\n\nView all files\n\n## Repository files navigation\n\n# üéôÔ∏è telnyx-transcribe üéôÔ∏è\n\n[](#Ô∏è-telnyx-transcribe-Ô∏è)\n\n### Stop manually transcribing calls. Start automating everything.\n\n[](#stop-manually-transcribing-calls-start-automating-everything)\n\n**_The ultimate call-and-transcribe toolkit. It dials your list, plays your audio, records everything, and transcribes it all using OpenAI Whisper ‚Äî automatically._**\n\n[![pypi](https://camo.githubusercontent.com/12cdf33998a3ef4c8d3a8371e886cfdbca6a332a50401355e328ff7ea4b91b93/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f74656c6e79782d7472616e7363726962652e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](https://pypi.org/project/telnyx-transcribe/) [![python](https://camo.githubusercontent.com/4808218212ece46251343f2523af0fdfb557a5bef0edc616a82c5480e8cd2da2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](https://opensource.org/licenses/MIT) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![automated](https://camo.githubusercontent.com/f2dc5aa70c726f1da67ecfc436103d09e4ef67c12b953b8d41c3212d779a363c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f66756c6c795f6175746f6d617465642d6469616c5f746f5f7472616e7363726970742d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/f2dc5aa70c726f1da67ecfc436103d09e4ef67c12b953b8d41c3212d779a363c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f66756c6c795f6175746f6d617465642d6469616c5f746f5f7472616e7363726970742d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![whisper powered](https://camo.githubusercontent.com/75a3bf0139d4e175e81731e26845b6c8d4ac6cc94bb83cb81cff42de12113e27/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f776869737065725f706f77657265642d73746174655f6f665f6172745f5354542d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/75a3bf0139d4e175e81731e26845b6c8d4ac6cc94bb83cb81cff42de12113e27/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f776869737065725f706f77657265642d73746174655f6f665f6172745f5354542d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-other-methods)\n\n* * *\n\n**telnyx-transcribe** is the automated phone assistant your workflow has been missing. Stop manually calling people, recording conversations, and typing out transcripts. This tool does it all ‚Äî dials your list, plays your audio message, records the responses, and delivers beautiful transcriptions using OpenAI's Whisper, the most accurate speech-to-text model available.\n\n### üìû\n\n[](#)\n\n**Bulk Calling**  \nDial hundreds in parallel\n\n### üîä\n\n[](#-1)\n\n**Auto Playback**  \nPlay your audio message\n\n### üéôÔ∏è\n\n[](#Ô∏è)\n\n**Smart Recording**  \nCapture every response\n\n### üß†\n\n[](#-2)\n\n**Whisper Transcription**  \nState-of-the-art accuracy\n\nHow it slaps:\n\n-   **You:** `telnyx-transcribe call numbers.txt`\n-   **Tool:** Dials all numbers, plays audio, records, transcribes.\n-   **You:** Check `results.tsv` for all transcriptions.\n-   **Result:** Hours of work done in minutes. Go grab a coffee. ‚òï\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nManually managing calls and transcriptions is a vibe-killer. `telnyx-transcribe` makes other methods look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The telnyx-transcribe Way (Glory)**\n\n1.  Manually dial each number.\n2.  Play your message, wait for response.\n3.  Fumble with recording software.\n4.  Upload recordings somewhere.\n5.  Manually transcribe or use slow tools.\n6.  Copy results into a spreadsheet.\n\n1.  `telnyx-transcribe call numbers.txt`\n2.  Wait for completion notification.\n3.  Open `results.tsv`.\n4.  All transcriptions, ready to go.\n5.  Go do something actually important. üöÄ\n\nWe're not just making calls. We're building a **fully automated pipeline** with concurrent call handling, automatic webhook processing, intelligent retry logic, and state-of-the-art transcription that handles accents, background noise, and multiple languages like a champ.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\nThe `telnyx-transcribe` command (or just `tt`) will be available in your terminal after installation.\n\nMethod\n\nCommand\n\n**pip**\n\n`pip install telnyx-transcribe`\n\n**pipx**\n\n`pipx install telnyx-transcribe`\n\n**From source**\n\n`pip install -e .`\n\n### Using pip (Recommended)\n\n[](#using-pip-recommended)\n\n# Install the package\npip install telnyx-transcribe\n\n# Verify installation\ntelnyx-transcribe --version\n\n### Using pipx (Isolated Environment)\n\n[](#using-pipx-isolated-environment)\n\n# Install pipx if you don't have it\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\n\n# Install telnyx-transcribe\npipx install telnyx-transcribe\n\n### From Source\n\n[](#from-source)\n\n# Clone the repo\ngit clone https://github.com/yigitkonur/telnyx-transcribe.git\ncd telnyx-transcribe\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n\n# Install in development mode\npip install -e \".\\[dev\\]\"\n\n> **‚ú® Pro Tip:** Use `tt` as a shorthand for `telnyx-transcribe` ‚Äî both commands work!\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\nThe workflow is dead simple.\n\n### üìû Make Calls & Transcribe\n\n[](#-make-calls--transcribe)\n\n**1\\. Create your numbers file** (one E.164 number per line):\n\n```\n+14155551234\n+14155551235\n+14155551236\n```\n\n**2\\. Run the command:**\n\ntelnyx-transcribe call numbers.txt\n\n**3\\. Check your results:**\n\ncat results.tsv\n\nThat's it. All calls made, recorded, and transcribed.\n\n### üéß Standalone Transcription\n\n[](#-standalone-transcription)\n\nAlready have audio files? Transcribe them directly without making calls:\n\n# Transcribe a single file\ntelnyx-transcribe transcribe recording.mp3\n\n# Transcribe all files in a directory\ntelnyx-transcribe transcribe ./recordings/\n\n# With language hint for better accuracy\ntelnyx-transcribe transcribe ./recordings/ --language en\n\n**Supported formats:** MP3, MP4, WAV, M4A, WEBM, OGG, FLAC, MPEG, MPGA\n\n### üåê Run Webhook Server Only\n\n[](#-run-webhook-server-only)\n\nNeed just the webhook server for incoming Telnyx events?\n\n# Start server on default port (5000)\ntelnyx-transcribe server\n\n# Custom port and host\ntelnyx-transcribe server --port 8080 --host 0.0.0.0\n\n### ‚úÖ Validate Configuration\n\n[](#-validate-configuration)\n\nCheck if everything is set up correctly before running:\n\ntelnyx-transcribe validate\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üìû Bulk Calling**  \n`Concurrent dialing`\n\nDials multiple numbers in parallel using thread pool\n\nProcess hundreds of calls in the time of one\n\n**üîä Auto Playback**  \n`Custom audio`\n\nPlays your audio file when call is answered\n\nDeliver consistent messages every time\n\n**üéôÔ∏è Smart Recording**  \n`Automatic capture`\n\nStarts recording immediately on answer\n\nNever miss a response\n\n**üß† Whisper AI**  \n`State-of-the-art STT`\n\nUses OpenAI's Whisper for transcription\n\nHandles accents, noise, multiple languages\n\n**üîÑ Auto Retry**  \n`Exponential backoff`\n\nAutomatically retries failed API calls\n\nResilient to network hiccups\n\n**üìä TSV Output**  \n`Ready for analysis`\n\nStructured output with all call details\n\nImport directly into Excel, Sheets, or scripts\n\n**üåê Webhook Server**  \n`Flask-powered`\n\nHandles Telnyx events in real-time\n\nSeamless integration with Telnyx platform\n\n**‚öôÔ∏è ENV Config**  \n`Zero hardcoding`\n\nAll secrets via environment variables\n\nSecure, 12-factor app compliant\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\nAll configuration is done via environment variables. Create a `.env` file or export them directly.\n\n### Required Variables\n\n[](#required-variables)\n\n# Telnyx API credentials\nTELNYX\\_API\\_KEY=your\\_telnyx\\_api\\_key\nTELNYX\\_CONNECTION\\_ID=your\\_connection\\_id\nTELNYX\\_FROM\\_NUMBER=+1234567890\n\n# OpenAI API key for Whisper\nOPENAI\\_API\\_KEY=your\\_openai\\_api\\_key\n\n# Audio file to play during calls\nAUDIO\\_URL=https://example.com/your-message.mp3\n\n### Optional Variables\n\n[](#optional-variables)\n\n# Server settings\nWEBHOOK\\_HOST=0.0.0.0       # Default: 0.0.0.0\nWEBHOOK\\_PORT=5000          # Default: 5000\n\n# Output settings\nOUTPUT\\_FILE=results.tsv    # Default: results.tsv\n\n# Performance tuning\nMAX\\_WORKERS=5              # Default: 5 concurrent calls\nMAX\\_RETRIES=10             # Default: 10 retries\nRETRY\\_DELAY=2.0            # Default: 2 seconds base delay\n\n# Recording settings\nRECORDING\\_FORMAT=mp3       # Default: mp3\nRECORDING\\_CHANNELS=single  # Default: single\n\n### Quick Setup\n\n[](#quick-setup)\n\n# Copy the example env file\ncp .env.example .env\n\n# Edit with your credentials\nnano .env\n\n# Validate configuration\ntelnyx-transcribe validate\n\n* * *\n\n## üîë API Key Setup Guides\n\n[](#-api-key-setup-guides)\n\n**üìû Telnyx API ‚Äî Pay-as-you-go calling**\n\n### What you get\n\n[](#what-you-get)\n\n-   Programmable voice API for making/receiving calls\n-   Call control, recording, and webhook events\n\n### Setup Steps\n\n[](#setup-steps)\n\n1.  Go to [portal.telnyx.com](https://portal.telnyx.com)\n2.  Sign up and verify your account\n3.  Create a **Call Control Application**:\n    -   Navigate to \"Call Control\" ‚Üí \"Applications\"\n    -   Create new application\n    -   Set your webhook URL (e.g., `https://your-server.com/webhook`)\n4.  Get a phone number:\n    -   Navigate to \"Numbers\" ‚Üí \"Buy Numbers\"\n    -   Purchase a number with voice capability\n    -   Assign it to your Call Control application\n5.  Get your credentials:\n    -   **API Key**: \"API Keys\" section\n    -   **Connection ID**: Your Call Control application's connection ID\n    -   **From Number**: The number you purchased\n\n### Add to `.env`:\n\n[](#add-to-env)\n\nTELNYX\\_API\\_KEY=KEY0123456789...\nTELNYX\\_CONNECTION\\_ID=1234567890\nTELNYX\\_FROM\\_NUMBER=+14155551234\n\n**üß† OpenAI API ‚Äî Whisper transcription**\n\n### What you get\n\n[](#what-you-get-1)\n\n-   Access to Whisper speech-to-text model\n-   State-of-the-art transcription accuracy\n-   Multi-language support\n\n### Setup Steps\n\n[](#setup-steps-1)\n\n1.  Go to [platform.openai.com](https://platform.openai.com)\n2.  Sign up or log in\n3.  Navigate to [API Keys](https://platform.openai.com/api-keys)\n4.  Click \"Create new secret key\"\n5.  Copy the key (starts with `sk-`)\n\n### Add to `.env`:\n\n[](#add-to-env-1)\n\nOPENAI\\_API\\_KEY=sk-...\n\n### Pricing\n\n[](#pricing)\n\n-   Whisper API: $0.006 per minute of audio\n-   A 5-minute recording costs ~$0.03\n\n* * *\n\n## üèóÔ∏è Project Structure\n\n[](#Ô∏è-project-structure)\n\n```\ntelnyx-transcribe/\n‚îú‚îÄ‚îÄ src/telnyx_transcribe/\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py               # Typer CLI commands\n‚îÇ   ‚îú‚îÄ‚îÄ app.py               # Flask application factory\n‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings management\n‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Data models (Call, TranscriptionResult)\n‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Custom exceptions\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ call_service.py          # Telnyx call management\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcription_service.py # OpenAI Whisper integration\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ output_service.py        # TSV/CSV output handling\n‚îÇ   ‚îú‚îÄ‚îÄ webhooks/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.py      # Telnyx webhook processing\n‚îÇ   ‚îî‚îÄ‚îÄ utils/\n‚îÇ       ‚îú‚îÄ‚îÄ logging.py       # Logging configuration\n‚îÇ       ‚îî‚îÄ‚îÄ console.py       # Rich console output\n‚îú‚îÄ‚îÄ tests/                   # Test suite\n‚îú‚îÄ‚îÄ pyproject.toml          # Project configuration\n‚îú‚îÄ‚îÄ requirements.txt        # Dependencies\n‚îú‚îÄ‚îÄ .env.example            # Example environment file\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n* * *\n\n## üõ†Ô∏è For Developers & Tinkerers\n\n[](#Ô∏è-for-developers--tinkerers)\n\n### Running Tests\n\n[](#running-tests)\n\n# Install dev dependencies\npip install -e \".\\[dev\\]\"\n\n# Run tests\npytest\n\n# Run with coverage\npytest --cov=telnyx\\_transcribe\n\n# Type checking\nmypy src/\n\n# Linting\nruff check src/\n\n### Using as a Library\n\n[](#using-as-a-library)\n\nfrom telnyx\\_transcribe import Settings\nfrom telnyx\\_transcribe.services import CallService, TranscriptionService\n\n\\# Configure settings\nsettings \\= Settings(\n    telnyx\\_api\\_key\\=\"your\\_key\",\n    openai\\_api\\_key\\=\"your\\_key\",\n    \\# ... other settings\n)\n\n\\# Use services directly\ncall\\_service \\= CallService(settings)\ncall \\= call\\_service.initiate\\_call(\"+14155551234\")\n\ntranscription\\_service \\= TranscriptionService(settings)\nresult \\= transcription\\_service.transcribe\\_file(\"recording.mp3\")\nprint(result.text)\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**`command not found: telnyx-transcribe`**\n\nRestart your terminal or run `pip install --upgrade telnyx-transcribe`\n\n**`TELNYX_API_KEY is required`**\n\nCreate a `.env` file with your credentials. Run `telnyx-transcribe validate` to check.\n\n**Webhook events not received**\n\nEnsure your webhook URL is publicly accessible. Use ngrok for local testing: `ngrok http 5000`\n\n**Transcription fails**\n\nCheck your `OPENAI_API_KEY` is valid and has credits. Verify audio format is supported.\n\n**Calls not connecting**\n\nVerify `TELNYX_FROM_NUMBER` is assigned to your Call Control application. Check number format (E.164).\n\n**Recording URL not available**\n\nTelnyx needs time to process recordings. The webhook handles this automatically with retries.\n\n**Debugging tips:**\n\n# Run with verbose logging\ntelnyx-transcribe --verbose call numbers.txt\n\n# Check your configuration\ntelnyx-transcribe validate\n\n# Test webhook server locally\ntelnyx-transcribe server --port 5000\n# Then use ngrok: ngrok http 5000\n\n* * *\n\n## üìú Backstory\n\n[](#-backstory)\n\nThis project started from a simple need ‚Äî automate phone-based surveys and transcribe the responses. What began as a quick script evolved into a full-featured, production-ready tool.\n\n> [https://twitter.com/yigitkonur/status/1654827917845860353](https://twitter.com/yigitkonur/status/1654827917845860353)\n\n* * *\n\n## üìÑ License\n\n[](#-license)\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n* * *\n\n**Built with üî• because manually transcribing phone calls is a soul-crushing waste of time.**\n\n[Report Bug](https://github.com/yigitkonur/telnyx-transcribe/issues) ‚Ä¢ [Request Feature](https://github.com/yigitkonur/telnyx-transcribe/issues) ‚Ä¢ [Contribute](https://github.com/yigitkonur/telnyx-transcribe/pulls)\n\n## About\n\nA script that uses Telnyx API to make bulk calls to a given list of numbers and analyzes the audio recordings with Whisper. This way, we don‚Äôt have to read all the transcriptions.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/telnyx-llm-call/activity)\n\n### Stars\n\n[**136** stars](/yigitkonur/telnyx-llm-call/stargazers)\n\n### Watchers\n\n[**4** watching](/yigitkonur/telnyx-llm-call/watchers)\n\n### Forks\n\n[**23** forks](/yigitkonur/telnyx-llm-call/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ftelnyx-llm-call&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/telnyx-llm-call/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=telnyx-llm-call)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/telnyx-llm-call/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/go-native-squid-proxy\n\nGitHub - yigitkonur/go-native-squid-proxy: GoNativeSquidProxy is a high-performance, scalable proxy server fully written in Go, designed to efficiently handle HTTP/HTTPS requests as a modern alternative to Squid.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fgo-native-squid-proxy)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fgo-native-squid-proxy)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fgo-native-squid-proxy)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[go-native-squid-proxy](/yigitkonur/go-native-squid-proxy)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fgo-native-squid-proxy) You must be signed in to change notification settings\n-   [Fork 3](/login?return_to=%2Fyigitkonur%2Fgo-native-squid-proxy)\n-   [Star 8](/login?return_to=%2Fyigitkonur%2Fgo-native-squid-proxy)\n    \n\nGoNativeSquidProxy is a high-performance, scalable proxy server fully written in Go, designed to efficiently handle HTTP/HTTPS requests as a modern alternative to Squid.\n\n### License\n\n[MIT license](/yigitkonur/go-native-squid-proxy/blob/main/LICENSE)\n\n[8 stars](/yigitkonur/go-native-squid-proxy/stargazers) [3 forks](/yigitkonur/go-native-squid-proxy/forks) [Branches](/yigitkonur/go-native-squid-proxy/branches) [Tags](/yigitkonur/go-native-squid-proxy/tags) [Activity](/yigitkonur/go-native-squid-proxy/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fgo-native-squid-proxy)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fgo-native-squid-proxy) You must be signed in to change notification settings\n\n# yigitkonur/go-native-squid-proxy\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/go-native-squid-proxy/branches)[Tags](/yigitkonur/go-native-squid-proxy/tags)\n\n[](/yigitkonur/go-native-squid-proxy/branches)[](/yigitkonur/go-native-squid-proxy/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/go-native-squid-proxy/commits/main/)\n\n[](/yigitkonur/go-native-squid-proxy/commits/main/)\n\n[cmd/proxy](/yigitkonur/go-native-squid-proxy/tree/main/cmd/proxy \"This path skips through empty directories\")\n\n[cmd/proxy](/yigitkonur/go-native-squid-proxy/tree/main/cmd/proxy \"This path skips through empty directories\")\n\n[pkg](/yigitkonur/go-native-squid-proxy/tree/main/pkg \"pkg\")\n\n[pkg](/yigitkonur/go-native-squid-proxy/tree/main/pkg \"pkg\")\n\n[test](/yigitkonur/go-native-squid-proxy/tree/main/test \"test\")\n\n[test](/yigitkonur/go-native-squid-proxy/tree/main/test \"test\")\n\n[.dockerignore](/yigitkonur/go-native-squid-proxy/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/go-native-squid-proxy/blob/main/.dockerignore \".dockerignore\")\n\n[.gitignore](/yigitkonur/go-native-squid-proxy/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/go-native-squid-proxy/blob/main/.gitignore \".gitignore\")\n\n[Dockerfile](/yigitkonur/go-native-squid-proxy/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/go-native-squid-proxy/blob/main/Dockerfile \"Dockerfile\")\n\n[LICENSE](/yigitkonur/go-native-squid-proxy/blob/main/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/go-native-squid-proxy/blob/main/LICENSE \"LICENSE\")\n\n[Makefile](/yigitkonur/go-native-squid-proxy/blob/main/Makefile \"Makefile\")\n\n[Makefile](/yigitkonur/go-native-squid-proxy/blob/main/Makefile \"Makefile\")\n\n[README.md](/yigitkonur/go-native-squid-proxy/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/go-native-squid-proxy/blob/main/README.md \"README.md\")\n\n[config.yaml](/yigitkonur/go-native-squid-proxy/blob/main/config.yaml \"config.yaml\")\n\n[config.yaml](/yigitkonur/go-native-squid-proxy/blob/main/config.yaml \"config.yaml\")\n\n[go.mod](/yigitkonur/go-native-squid-proxy/blob/main/go.mod \"go.mod\")\n\n[go.mod](/yigitkonur/go-native-squid-proxy/blob/main/go.mod \"go.mod\")\n\n[go.sum](/yigitkonur/go-native-squid-proxy/blob/main/go.sum \"go.sum\")\n\n[go.sum](/yigitkonur/go-native-squid-proxy/blob/main/go.sum \"go.sum\")\n\nView all files\n\n## Repository files navigation\n\n# ü¶ë Go Native Squid Proxy ü¶ë\n\n[](#-go-native-squid-proxy-)\n\n### Stop babysitting Squid configs. Start proxying at warp speed.\n\n[](#stop-babysitting-squid-configs-start-proxying-at-warp-speed)\n\n**_A high-performance HTTP/HTTPS proxy server written in pure Go. It's Squid, but actually fast, and you don't need a PhD to configure it._**\n\n[![go](https://camo.githubusercontent.com/f4a34108811d4fe410b3ef1ef58398bd94a218b7963ad238f73ade0cd1ad1827/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f676f2d312e32312b2d3030414444382e7376673f7374796c653d666c61742d737175617265266c6f676f3d676f)](#) [![fasthttp](https://camo.githubusercontent.com/5fb821c73e7875823f0c86ee3decbcc76da616b7a38840aa0ecbf3e431b52277/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f77657265645f62792d66617374687474702d3030414444382e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![zero config](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![prometheus ready](https://camo.githubusercontent.com/b7cb17b4154a101ea995a089e9d407ab4b621f6ae3ecde9da737fc1c43705e91/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938a5f70726f6d6574686575732d6275696c742d2d696e5f6d6574726963732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/b7cb17b4154a101ea995a089e9d407ab4b621f6ae3ecde9da737fc1c43705e91/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938a5f70726f6d6574686575732d6275696c742d2d696e5f6d6574726963732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üê≥ Docker**](#-docker)\n\n* * *\n\n**Go Native Squid Proxy** is the proxy server your infrastructure wishes it had. Stop wrestling with arcane Squid configurations and cryptic error messages. This proxy is built with Go's legendary concurrency, powered by fasthttp for maximum throughput, and designed to just work out of the box.\n\n### ‚ö°\n\n[](#)\n\n**Blazing Fast**  \nBuilt on fasthttp, 10x faster than net/http\n\n### üîí\n\n[](#-1)\n\n**HTTPS Tunneling**  \nFull CONNECT support for TLS passthrough\n\n### üìä\n\n[](#-2)\n\n**Prometheus Ready**  \nMetrics endpoint out of the box\n\nHow it slaps:\n\n-   **You:** `make run`\n-   **Proxy:** Starts instantly, ready to handle 10k+ connections\n-   **You:** `curl -x localhost:8080 https://httpbin.org/ip`\n-   **Result:** Your IP is proxied. Zero config. Just works.\n\n* * *\n\n## üí• Why This Slaps Other Proxies\n\n[](#-why-this-slaps-other-proxies)\n\nSetting up Squid is a nightmare. Go Native Squid Proxy makes legacy proxies look ancient.\n\n**‚ùå The Squid Way (Pain)**\n\n**‚úÖ The Go Native Way (Glory)**\n\n1.  Install Squid via package manager\n2.  Edit 500-line squid.conf\n3.  Google every ACL syntax\n4.  Restart, pray, check logs\n5.  Memory leaks after a week\n\n1.  `make build`\n2.  `./build/proxy`\n3.  Done.\n4.  Go grab a coffee. ‚òï\n\nWe're not just another proxy. We're building a **production-ready, observable, zero-config proxy** with connection pooling, structured logging, and Prometheus metrics baked in.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\nMethod\n\nOne-liner\n\nüî® **Build from source**\n\n`make build && ./build/proxy`\n\nüê≥ **Docker**\n\n`docker run -p 8080:8080 yigitkonur/go-native-squid-proxy`\n\nüì¶ **Go install**\n\n`go install github.com/yigitkonur/go-native-squid-proxy/cmd/proxy@latest`\n\n### üî® Build from Source\n\n[](#-build-from-source)\n\n# Clone the repo\ngit clone https://github.com/yigitkonur/go-native-squid-proxy.git\ncd go-native-squid-proxy\n\n# Build and run\nmake build\n./build/proxy\n\n### üê≥ Docker (Recommended for Production)\n\n[](#-docker-recommended-for-production)\n\n# Build the image\nmake docker-build\n\n# Run it\ndocker run -d \\\\\n  --name proxy \\\\\n  -p 8080:8080 \\\\\n  -p 9090:9090 \\\\\n  go-native-squid-proxy:latest\n\n### üì¶ Go Install\n\n[](#-go-install)\n\ngo install github.com/yigitkonur/go-native-squid-proxy/cmd/proxy@latest\nproxy\n\n> **‚ú® Zero Config:** The proxy starts with sensible defaults. No config file needed!\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\n### Basic Proxying\n\n[](#basic-proxying)\n\n# Start the proxy (default: :8080)\n./build/proxy\n\n# Test HTTP proxying\ncurl -x localhost:8080 http://httpbin.org/ip\n\n# Test HTTPS proxying (CONNECT tunneling)\ncurl -x localhost:8080 https://httpbin.org/ip\n\n# Use with any HTTP client\nexport http\\_proxy=http://localhost:8080\nexport https\\_proxy=http://localhost:8080\nwget https://example.com\n\n### Check Metrics\n\n[](#check-metrics)\n\n# Prometheus metrics are available at :9090/metrics\ncurl http://localhost:9090/metrics\n\n# Key metrics available:\n# - proxy\\_requests\\_total{method, status, type}\n# - proxy\\_request\\_duration\\_seconds{method, type}\n# - proxy\\_active\\_connections\n# - proxy\\_tunnel\\_connections\n# - proxy\\_bytes\\_sent\\_total{type}\n# - proxy\\_bytes\\_received\\_total{type}\n# - proxy\\_errors\\_total{type, reason}\n\n### Command Line Options\n\n[](#command-line-options)\n\n# Show version\n./build/proxy -version\n\n# Use custom config file\n./build/proxy -config /path/to/config.yaml\n\n# Override with environment variables\nPROXY\\_SERVER\\_ADDRESS=\":9999\" ./build/proxy\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**‚ö° fasthttp Engine**  \n10x faster than net/http\n\nUses fasthttp for request handling with zero-allocation design\n\nHandle 100k+ req/sec on modest hardware\n\n**üîí CONNECT Tunneling**  \nFull HTTPS support\n\nImplements HTTP CONNECT method for transparent TLS passthrough\n\nProxy HTTPS without breaking encryption\n\n**üìä Prometheus Metrics**  \nObservable by default\n\nExposes request counts, latencies, connection stats, error rates\n\nPlug into Grafana dashboards instantly\n\n**üîÑ Connection Pooling**  \nEfficient resource use\n\nReuses upstream connections with intelligent pooling\n\nReduce latency, save file descriptors\n\n**üìù Structured Logging**  \nPowered by zap\n\nJSON or console logs with configurable levels\n\nDebug issues fast, grep-friendly logs\n\n**‚öôÔ∏è Env Overrides**  \n12-factor ready\n\nOverride any config with `PROXY_*` env vars\n\nPerfect for containers and K8s\n\n**üõ°Ô∏è Graceful Shutdown**  \nZero dropped requests\n\nHandles SIGTERM/SIGINT with connection draining\n\nZero-downtime deployments\n\n**üéØ Hop-by-Hop Handling**  \nRFC compliant\n\nProperly strips proxy headers per HTTP spec\n\nNo header leakage, clean proxying\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\nConfiguration uses YAML with environment variable overrides.\n\n### Default Config (`config.yaml`)\n\n[](#default-config-configyaml)\n\n# Server settings\nserver:\n  address: \":8080\"           # Proxy listen address\n  read\\_timeout: 30s          # Read timeout\n  write\\_timeout: 30s         # Write timeout\n  idle\\_timeout: 120s         # Keep-alive timeout\n  max\\_conns\\_per\\_ip: 10000    # Max connections per IP\n  max\\_requests\\_per\\_conn: 0   # 0 = unlimited\n\n# Proxy behavior\nproxy:\n  dial\\_timeout: 10s          # Upstream connect timeout\n  response\\_timeout: 60s      # Upstream response timeout\n  max\\_idle\\_conns: 1000       # Pooled connections\n\n# Logging\nlogging:\n  level: \"info\"              # debug, info, warn, error\n  format: \"console\"          # console or json\n  output: \"stdout\"           # stdout, stderr, or file path\n\n# Prometheus metrics\nmetrics:\n  enabled: true\n  address: \":9090\"\n  path: \"/metrics\"\n\n### Environment Variable Overrides\n\n[](#environment-variable-overrides)\n\nAll settings can be overridden with `PROXY_` prefix:\n\n# Override listen address\nPROXY\\_SERVER\\_ADDRESS=\":9999\"\n\n# Override log level\nPROXY\\_LOGGING\\_LEVEL=\"debug\"\n\n# Disable metrics\nPROXY\\_METRICS\\_ENABLED=\"false\"\n\n# Example: run with overrides\nPROXY\\_SERVER\\_ADDRESS=\":3128\" \\\\\nPROXY\\_LOGGING\\_LEVEL=\"debug\" \\\\\n./build/proxy\n\n* * *\n\n## üê≥ Docker\n\n[](#-docker)\n\n### Quick Start\n\n[](#quick-start)\n\n# Run with defaults\ndocker run -d -p 8080:8080 -p 9090:9090 go-native-squid-proxy\n\n# Run with custom config\ndocker run -d \\\\\n  -p 8080:8080 \\\\\n  -p 9090:9090 \\\\\n  -v $(pwd)/config.yaml:/etc/proxy/config.yaml \\\\\n  go-native-squid-proxy\n\n### Docker Compose\n\n[](#docker-compose)\n\nversion: '3.8'\n\nservices:\n  proxy:\n    image: go-native-squid-proxy:latest\n    ports:\n      - \"8080:8080\"   # Proxy\n      - \"9090:9090\"   # Metrics\n    environment:\n      - PROXY\\_LOGGING\\_LEVEL=info\n      - PROXY\\_SERVER\\_MAX\\_CONNS\\_PER\\_IP=50000\n    healthcheck:\n      test: \\[\"CMD\", \"wget\", \"-q\", \"--spider\", \"http://localhost:9090/metrics\"\\]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n    restart: unless-stopped\n\n### Kubernetes\n\n[](#kubernetes)\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: proxy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy\n  template:\n    metadata:\n      labels:\n        app: proxy\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      containers:\n      - name: proxy\n        image: go-native-squid-proxy:latest\n        ports:\n        - containerPort: 8080\n          name: proxy\n        - containerPort: 9090\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 1000m\n            memory: 256Mi\n\n* * *\n\n## üìÅ Project Structure\n\n[](#-project-structure)\n\n```\ngo-native-squid-proxy/\n‚îú‚îÄ‚îÄ cmd/\n‚îÇ   ‚îî‚îÄ‚îÄ proxy/\n‚îÇ       ‚îî‚îÄ‚îÄ main.go          # Entry point\n‚îú‚îÄ‚îÄ pkg/\n‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ handler/             # Request handlers\n‚îÇ   ‚îú‚îÄ‚îÄ log/                 # Structured logging\n‚îÇ   ‚îú‚îÄ‚îÄ metrics/             # Prometheus metrics\n‚îÇ   ‚îú‚îÄ‚îÄ pool/                # Connection pooling\n‚îÇ   ‚îî‚îÄ‚îÄ proxy/               # Server implementation\n‚îú‚îÄ‚îÄ test/                    # Tests\n‚îú‚îÄ‚îÄ config.yaml              # Default config\n‚îú‚îÄ‚îÄ Dockerfile               # Multi-stage Docker build\n‚îú‚îÄ‚îÄ Makefile                 # Build automation\n‚îî‚îÄ‚îÄ README.md                # You are here\n```\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Go 1.21+\n-   Make (optional but recommended)\n\n### Build Commands\n\n[](#build-commands)\n\n# Build binary\nmake build\n\n# Run tests\nmake test\n\n# Run with coverage\nmake test-coverage\n\n# Format code\nmake fmt\n\n# Run linter\nmake lint\n\n# Build for all platforms\nmake build-all\n\n# Show all available commands\nmake help\n\n### Running Tests\n\n[](#running-tests)\n\n# All tests\nmake test\n\n# With coverage\nmake test-coverage\n\n# Benchmarks\nmake bench\n\n* * *\n\n## üìä Metrics & Monitoring\n\n[](#-metrics--monitoring)\n\n### Available Metrics\n\n[](#available-metrics)\n\nMetric\n\nType\n\nLabels\n\nDescription\n\n`proxy_requests_total`\n\nCounter\n\nmethod, status, type\n\nTotal requests processed\n\n`proxy_request_duration_seconds`\n\nHistogram\n\nmethod, type\n\nRequest latency\n\n`proxy_active_connections`\n\nGauge\n\n\\-\n\nCurrent active connections\n\n`proxy_tunnel_connections`\n\nGauge\n\n\\-\n\nActive CONNECT tunnels\n\n`proxy_bytes_sent_total`\n\nCounter\n\ntype\n\nBytes sent to clients\n\n`proxy_bytes_received_total`\n\nCounter\n\ntype\n\nBytes received from clients\n\n`proxy_errors_total`\n\nCounter\n\ntype, reason\n\nError counts\n\n### Grafana Dashboard\n\n[](#grafana-dashboard)\n\nImport the provided dashboard or query metrics directly:\n\n```\n# Request rate\nrate(proxy_requests_total[5m])\n\n# P99 latency\nhistogram_quantile(0.99, rate(proxy_request_duration_seconds_bucket[5m]))\n\n# Error rate\nrate(proxy_errors_total[5m]) / rate(proxy_requests_total[5m])\n```\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**Port already in use**\n\nChange port with `PROXY_SERVER_ADDRESS=\":9999\"` or kill existing process\n\n**Connection refused to upstream**\n\nCheck `proxy.dial_timeout` setting and upstream availability\n\n**Too many open files**\n\nIncrease ulimit: `ulimit -n 65535`\n\n**Slow responses**\n\nEnable debug logging to identify bottlenecks: `PROXY_LOGGING_LEVEL=debug`\n\n**Metrics not showing**\n\nEnsure `metrics.enabled: true` and check `:9090/metrics`\n\n* * *\n\n## ü§ù Contributing\n\n[](#-contributing)\n\nContributions are welcome! Please:\n\n1.  Fork the repository\n2.  Create a feature branch (`git checkout -b feature/amazing-feature`)\n3.  Commit your changes (`git commit -m 'Add amazing feature'`)\n4.  Push to the branch (`git push origin feature/amazing-feature`)\n5.  Open a Pull Request\n\n* * *\n\n## üìú License\n\n[](#-license)\n\nThis project is licensed under the MIT License. See [LICENSE](/yigitkonur/go-native-squid-proxy/blob/main/LICENSE) for details.\n\n* * *\n\n## üôè Acknowledgements\n\n[](#-acknowledgements)\n\n-   **[fasthttp](https://github.com/valyala/fasthttp)** - The blazing fast HTTP engine\n-   **[zap](https://github.com/uber-go/zap)** - Structured logging that doesn't suck\n-   **[viper](https://github.com/spf13/viper)** - Configuration management made easy\n-   **[Prometheus](https://prometheus.io/)** - Monitoring that actually works\n\n* * *\n\n**Built with üî• because configuring Squid is a soul-crushing waste of time.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n## About\n\nGoNativeSquidProxy is a high-performance, scalable proxy server fully written in Go, designed to efficiently handle HTTP/HTTPS requests as a modern alternative to Squid.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/go-native-squid-proxy/activity)\n\n### Stars\n\n[**8** stars](/yigitkonur/go-native-squid-proxy/stargazers)\n\n### Watchers\n\n[**2** watching](/yigitkonur/go-native-squid-proxy/watchers)\n\n### Forks\n\n[**3** forks](/yigitkonur/go-native-squid-proxy/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fgo-native-squid-proxy&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/go-native-squid-proxy/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=go-native-squid-proxy)\n\nNo packages published  \n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [Go 82.1%](/yigitkonur/go-native-squid-proxy/search?l=go)\n-   [Makefile 12.7%](/yigitkonur/go-native-squid-proxy/search?l=makefile)\n-   [Dockerfile 5.2%](/yigitkonur/go-native-squid-proxy/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/n8n-community-node-boilerplate\n\nGitHub - yigitkonur/n8n-community-node-boilerplate: an AI-optimized n8n community node boilerplate with comprehensive documentation and examples                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-community-node-boilerplate)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-community-node-boilerplate)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fn8n-community-node-boilerplate)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[n8n-community-node-boilerplate](/yigitkonur/n8n-community-node-boilerplate)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-community-node-boilerplate) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Fn8n-community-node-boilerplate)\n-   [Star 4](/login?return_to=%2Fyigitkonur%2Fn8n-community-node-boilerplate)\n    \n\nan AI-optimized n8n community node boilerplate with comprehensive documentation and examples\n\n### License\n\n[MIT license](/yigitkonur/n8n-community-node-boilerplate/blob/master/LICENSE.md)\n\n[4 stars](/yigitkonur/n8n-community-node-boilerplate/stargazers) [1 fork](/yigitkonur/n8n-community-node-boilerplate/forks) [Branches](/yigitkonur/n8n-community-node-boilerplate/branches) [Tags](/yigitkonur/n8n-community-node-boilerplate/tags) [Activity](/yigitkonur/n8n-community-node-boilerplate/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fn8n-community-node-boilerplate)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-community-node-boilerplate) You must be signed in to change notification settings\n\n# yigitkonur/n8n-community-node-boilerplate\n\n  \n\n¬†master\n\n[Branches](/yigitkonur/n8n-community-node-boilerplate/branches)[Tags](/yigitkonur/n8n-community-node-boilerplate/tags)\n\n[](/yigitkonur/n8n-community-node-boilerplate/branches)[](/yigitkonur/n8n-community-node-boilerplate/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[164 Commits](/yigitkonur/n8n-community-node-boilerplate/commits/master/)\n\n[](/yigitkonur/n8n-community-node-boilerplate/commits/master/)\n\n[.aiassistant/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.aiassistant/rules \"This path skips through empty directories\")\n\n[.aiassistant/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.aiassistant/rules \"This path skips through empty directories\")\n\n[.continue/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.continue/rules \"This path skips through empty directories\")\n\n[.continue/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.continue/rules \"This path skips through empty directories\")\n\n[.cursor/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.cursor/rules \"This path skips through empty directories\")\n\n[.cursor/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.cursor/rules \"This path skips through empty directories\")\n\n[.gemini](/yigitkonur/n8n-community-node-boilerplate/tree/master/.gemini \".gemini\")\n\n[.gemini](/yigitkonur/n8n-community-node-boilerplate/tree/master/.gemini \".gemini\")\n\n[.github](/yigitkonur/n8n-community-node-boilerplate/tree/master/.github \".github\")\n\n[.github](/yigitkonur/n8n-community-node-boilerplate/tree/master/.github \".github\")\n\n[.vscode](/yigitkonur/n8n-community-node-boilerplate/tree/master/.vscode \".vscode\")\n\n[.vscode](/yigitkonur/n8n-community-node-boilerplate/tree/master/.vscode \".vscode\")\n\n[.windsurf/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.windsurf/rules \"This path skips through empty directories\")\n\n[.windsurf/rules](/yigitkonur/n8n-community-node-boilerplate/tree/master/.windsurf/rules \"This path skips through empty directories\")\n\n[credentials](/yigitkonur/n8n-community-node-boilerplate/tree/master/credentials \"credentials\")\n\n[credentials](/yigitkonur/n8n-community-node-boilerplate/tree/master/credentials \"credentials\")\n\n[docs](/yigitkonur/n8n-community-node-boilerplate/tree/master/docs \"docs\")\n\n[docs](/yigitkonur/n8n-community-node-boilerplate/tree/master/docs \"docs\")\n\n[icons](/yigitkonur/n8n-community-node-boilerplate/tree/master/icons \"icons\")\n\n[icons](/yigitkonur/n8n-community-node-boilerplate/tree/master/icons \"icons\")\n\n[nodes](/yigitkonur/n8n-community-node-boilerplate/tree/master/nodes \"nodes\")\n\n[nodes](/yigitkonur/n8n-community-node-boilerplate/tree/master/nodes \"nodes\")\n\n[.aider.conf.yml](/yigitkonur/n8n-community-node-boilerplate/blob/master/.aider.conf.yml \".aider.conf.yml\")\n\n[.aider.conf.yml](/yigitkonur/n8n-community-node-boilerplate/blob/master/.aider.conf.yml \".aider.conf.yml\")\n\n[.clinerules](/yigitkonur/n8n-community-node-boilerplate/blob/master/.clinerules \".clinerules\")\n\n[.clinerules](/yigitkonur/n8n-community-node-boilerplate/blob/master/.clinerules \".clinerules\")\n\n[.gitignore](/yigitkonur/n8n-community-node-boilerplate/blob/master/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/n8n-community-node-boilerplate/blob/master/.gitignore \".gitignore\")\n\n[.prettierrc.js](/yigitkonur/n8n-community-node-boilerplate/blob/master/.prettierrc.js \".prettierrc.js\")\n\n[.prettierrc.js](/yigitkonur/n8n-community-node-boilerplate/blob/master/.prettierrc.js \".prettierrc.js\")\n\n[.roomodes](/yigitkonur/n8n-community-node-boilerplate/blob/master/.roomodes \".roomodes\")\n\n[.roomodes](/yigitkonur/n8n-community-node-boilerplate/blob/master/.roomodes \".roomodes\")\n\n[.rules](/yigitkonur/n8n-community-node-boilerplate/blob/master/.rules \".rules\")\n\n[.rules](/yigitkonur/n8n-community-node-boilerplate/blob/master/.rules \".rules\")\n\n[AGENTS.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/AGENTS.md \"AGENTS.md\")\n\n[AGENTS.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/AGENTS.md \"AGENTS.md\")\n\n[CLAUDE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/CLAUDE.md \"CLAUDE.md\")\n\n[COMPREHENSIVE-FACT-CHECK-REPORT.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/COMPREHENSIVE-FACT-CHECK-REPORT.md \"COMPREHENSIVE-FACT-CHECK-REPORT.md\")\n\n[COMPREHENSIVE-FACT-CHECK-REPORT.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/COMPREHENSIVE-FACT-CHECK-REPORT.md \"COMPREHENSIVE-FACT-CHECK-REPORT.md\")\n\n[FACT-CHECK-REPORT.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/FACT-CHECK-REPORT.md \"FACT-CHECK-REPORT.md\")\n\n[FACT-CHECK-REPORT.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/FACT-CHECK-REPORT.md \"FACT-CHECK-REPORT.md\")\n\n[LICENSE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/LICENSE.md \"LICENSE.md\")\n\n[LICENSE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/LICENSE.md \"LICENSE.md\")\n\n[README.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/README.md \"README.md\")\n\n[README.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/README.md \"README.md\")\n\n[README\\_TEMPLATE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/README_TEMPLATE.md \"README_TEMPLATE.md\")\n\n[README\\_TEMPLATE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/README_TEMPLATE.md \"README_TEMPLATE.md\")\n\n[check-facts.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/check-facts.md \"check-facts.md\")\n\n[check-facts.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/check-facts.md \"check-facts.md\")\n\n[eslint.config.mjs](/yigitkonur/n8n-community-node-boilerplate/blob/master/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/n8n-community-node-boilerplate/blob/master/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/package.json \"package.json\")\n\n[package.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/n8n-community-node-boilerplate/blob/master/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n# üîå n8n Community Node Boilerplate üîå\n\n[](#-n8n-community-node-boilerplate-)\n\n### Stop wrestling with n8n docs. Start shipping nodes that work.\n\n[](#stop-wrestling-with-n8n-docs-start-shipping-nodes-that-work)\n\n**_The ultimate starter kit for building n8n community nodes with AI. Pre-configured rules for 11 coding assistants, 31 documentation files, and production-ready patterns ‚Äî so your AI actually knows what the hell it's doing._**\n\n[![n8n](https://camo.githubusercontent.com/736feee7f767d5fdfa22c2a6361f71cbf2fac9ea0672f9a93da69526594a1caf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e386e2d436f6d6d756e6974792532304e6f64652d4646364435413f7374796c653d666c61742d737175617265266c6f676f3d6e386e266c6f676f436f6c6f723d7768697465)](https://docs.n8n.io/integrations/creating-nodes/) [![typescript](https://camo.githubusercontent.com/d0648ba0932a705fbb157be0f1ad8564c5af1ee62f322e22e8c3b7ed7aceb094/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d352e782d3331373843363f7374796c653d666c61742d737175617265266c6f676f3d74797065736372697074266c6f676f436f6c6f723d7768697465)](https://www.typescriptlang.org/) [![node](https://camo.githubusercontent.com/32102314fdcf5112a4845481024a12c4490513f7bfc1674ee5686d3097743ffc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6f64652e6a732d32322b2d3333393933333f7374796c653d666c61742d737175617265266c6f676f3d6e6f64652e6a73266c6f676f436f6c6f723d7768697465)](https://nodejs.org/) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/6ebf9e030a81e4cf8c3e231c86b3cc4330df8674537f5783261d6136eca22b7b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832353f7374796c653d666c61742d737175617265)](/yigitkonur/n8n-community-node-boilerplate/blob/master/LICENSE.md) [![docs](https://camo.githubusercontent.com/bf1c5658c5d38cd02e14d4edc14e7cd469414c1f827c39ad14a99287ba187d05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63732d333125323046696c65732d3245443537333f7374796c653d666c61742d737175617265)](/yigitkonur/n8n-community-node-boilerplate/blob/master/docs)\n\n[![11 tools](https://camo.githubusercontent.com/b63556b30e8c1f3c309d7575b4e69f9704f4f2fd262d8248102241b2f69f558e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f31315f41495f746f6f6c732d7072652d2d636f6e666967757265645f72756c65732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/b63556b30e8c1f3c309d7575b4e69f9704f4f2fd262d8248102241b2f69f558e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f31315f41495f746f6f6c732d7072652d2d636f6e666967757265645f72756c65732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![production ready](https://camo.githubusercontent.com/47ba1877068135a4ee280da68e8055bbb4f5e3a8061da01d3c7c1a907ef7077b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93a65f70726f64756374696f6e5f72656164792d776f726b696e675f6578616d706c65735f696e636c756465642d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/47ba1877068135a4ee280da68e8055bbb4f5e3a8061da01d3c7c1a907ef7077b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93a65f70726f64756374696f6e5f72656164792d776f726b696e675f6578616d706c65735f696e636c756465642d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-90-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**ü§ñ AI Tool Setup**](#-ai-tool-configuration) ‚Ä¢ [**üìö Documentation**](#-documentation-31-files) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-other-methods)\n\n* * *\n\n**n8n Community Node Boilerplate** is the project manager your AI coding assistant wishes it had. Stop feeding your LLM random n8n docs and praying for working code. This repo acts like a senior n8n developer, providing structured rules, comprehensive documentation, and battle-tested patterns so your AI can actually build nodes that work the first time.\n\n### üß†\n\n[](#)\n\n**AI-Optimized Rules**  \n11 tools pre-configured\n\n### üìö\n\n[](#-1)\n\n**31 Doc Files**  \nEvery n8n pattern covered\n\n### üèóÔ∏è\n\n[](#Ô∏è)\n\n**Working Examples**  \nCopy, modify, ship\n\nHow it slaps:\n\n-   **You:** Clone repo, pick your AI tool, delete the rest.\n-   **AI:** Reads 31 doc files + structured rules.\n-   **You:** \"Build me a Stripe node with customer CRUD\"\n-   **AI:** Generates production-ready code with proper types, error handling, and routing.\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nBuilding n8n nodes without context is pain. This repo makes other approaches look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Boilerplate Way (Glory)**\n\n1.  Read 47 pages of n8n docs.\n2.  Copy random code from community nodes.\n3.  Ask AI to help ‚Äî it hallucinates properties.\n4.  Debug for 3 hours because routing.send is wrong.\n5.  Finally get it working. Realize you forgot credentials.\n\n1.  `git clone` this repo.\n2.  Pick your AI tool, delete others.\n3.  \"Build me a node for X API\"\n4.  AI uses 31 doc files + real examples.\n5.  Ship working code. Go grab a coffee. ‚òï\n\nWe're not just giving your AI random docs. We're providing **structured rules, decision trees, code patterns, and working examples** that turn your AI into an n8n expert.\n\n* * *\n\n## üöÄ Get Started in 90 Seconds\n\n[](#-get-started-in-90-seconds)\n\nStep\n\nCommand\n\nWhat Happens\n\n1Ô∏è‚É£\n\n`git clone https://github.com/yigitkonur/n8n-community-node-boilerplate.git`\n\nGet the goods\n\n2Ô∏è‚É£\n\n`cd n8n-community-node-boilerplate && npm install`\n\nInstall deps\n\n3Ô∏è‚É£\n\nPick your AI tool ‚Üí delete others (see below)\n\nClean setup\n\n4Ô∏è‚É£\n\n`npm run dev`\n\nn8n runs at localhost:5678\n\n### üßπ Pick Your AI Tool\n\n[](#-pick-your-ai-tool)\n\nChoose **one** AI assistant and remove the config files for all others. This prevents conflicts and keeps your AI focused.\n\n**üñ±Ô∏è Click to expand cleanup commands for each tool**\n\nI use...\n\nRun this to clean up\n\n**Cursor**\n\n`rm -rf CLAUDE.md .clinerules .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini .windsurf`\n\n**Windsurf**\n\n`rm -rf CLAUDE.md .clinerules .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini .cursor`\n\n**Claude Code**\n\n`rm -rf .cursor .windsurf .clinerules .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini`\n\n**GitHub Copilot**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .rules .aider.conf.yml .continue .aiassistant .gemini`\n\n**Cline**\n\n`rm -rf CLAUDE.md .cursor .windsurf .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini`\n\n**Roo-Cline**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini`\n\n**Continue.dev**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .rules .aider.conf.yml .github/copilot-instructions.md .aiassistant .gemini`\n\n**Aider**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .rules .continue .github/copilot-instructions.md .aiassistant .gemini`\n\n**Zed**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant .gemini`\n\n**JetBrains AI**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .gemini`\n\n**Gemini CLI**\n\n`rm -rf CLAUDE.md .cursor .windsurf .clinerules .roomodes .rules .aider.conf.yml .continue .github/copilot-instructions.md .aiassistant`\n\n> **üí° Keep `AGENTS.md`** ‚Äî It's the single source of truth referenced by multiple tools.\n\n### üéØ Start Building\n\n[](#-start-building)\n\nnpm run dev  # Opens n8n at http://localhost:5678\n\nNow ask your AI:\n\n> \"Create a new n8n node for the Stripe API with API key authentication and CRUD operations for customers\"\n\nWatch it generate production-ready code. Magic. ‚ú®\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**ü§ñ 11 AI Tools**  \n`pre-configured`\n\nRules for Cursor, Windsurf, Claude Code, Copilot, Cline, Roo-Cline, Continue, Aider, Zed, JetBrains, Gemini\n\nUse your favorite tool, rules already set up\n\n**üìö 31 Doc Files**  \n`comprehensive`\n\nEvery n8n pattern: declarative, programmatic, credentials, routing, pagination, error handling\n\nYour AI has answers before you ask\n\n**üèóÔ∏è Working Examples**  \n`copy & modify`\n\nGitHub Issues node with full CRUD, OAuth2, list search, resource operations\n\nReal patterns, not toy examples\n\n**üß≠ Decision Trees**  \n`built-in`\n\nREST API? ‚Üí Declarative. SDK? ‚Üí Programmatic. Complex auth? ‚Üí Custom execute\n\nAI knows which pattern to use\n\n**üìù AGENTS.md**  \n`single source of truth`\n\nOne master file with all patterns, referenced by all tools\n\nUpdate once, all tools get smarter\n\n**‚ö° Hot Reload**  \n`instant feedback`\n\n`npm run dev` watches your changes\n\nSee results immediately\n\n* * *\n\n## ü§ñ AI Tool Configuration\n\n[](#-ai-tool-configuration)\n\n[![Cursor](https://camo.githubusercontent.com/6e8fddd1753ebb76b59f57c1674defa5081009329a249b495eb59492ece32650/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f437572736f722d3030443946463f7374796c653d666f722d7468652d6261646765266c6f676f3d637572736f72266c6f676f436f6c6f723d7768697465)](#cursor) [![Windsurf](https://camo.githubusercontent.com/05b33c67c06404088ca4f2fe2a9ad638e44bcf58d5d047dfe2d6101415abfdd4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57696e64737572662d3342383246363f7374796c653d666f722d7468652d6261646765)](#windsurf) [![Claude Code](https://camo.githubusercontent.com/a3dc5c2235deec77c9040d6f7bb7a14a9cc6b0aeb896f59fbd778ffb9c348ca6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c61756465253230436f64652d3933333345413f7374796c653d666f722d7468652d6261646765266c6f676f3d616e7468726f706963266c6f676f436f6c6f723d7768697465)](#claude-code) [![GitHub Copilot](https://camo.githubusercontent.com/661205dfc964bceabce0ebb86f7fa3d7e47c1b5305df968a663795470fb33640/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f476974487562253230436f70696c6f742d3138313731373f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465)](#github-copilot)\n\n[![Cline](https://camo.githubusercontent.com/ca84bedf89138e96e6f4a9bdf4b55a1fd0db119630595aaca7bf0ff7425a6df1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c696e652d4646364236423f7374796c653d666f722d7468652d6261646765)](#cline) [![Roo-Cline](https://camo.githubusercontent.com/5b5bb229f44fab6b4ac101265ad0430cec9319fec2db4179d207db453c90cee9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f526f6f2d2d436c696e652d4639373331363f7374796c653d666f722d7468652d6261646765)](#roo-cline) [![Continue.dev](https://camo.githubusercontent.com/4e90aba8ac4f756d8784c3c1ae258337169a56045637537c1ecc61a07b5f41fe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e74696e75652e6465762d3045413545393f7374796c653d666f722d7468652d6261646765)](#continuedev) [![Aider](https://camo.githubusercontent.com/18635b939a747275e686b9c3eae06f7afd886ed50a2fc989ddec63b7c5430461/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41696465722d3235363345423f7374796c653d666f722d7468652d6261646765)](#aider)\n\n[![Zed](https://camo.githubusercontent.com/eb4b2beb9aa6933d428d91b20a1be6a7f939cf019593f4f126ad59ac066747e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5a65642d3038344343463f7374796c653d666f722d7468652d6261646765266c6f676f3d7a6564266c6f676f436f6c6f723d7768697465)](#zed) [![JetBrains AI](https://camo.githubusercontent.com/d2b8b7f950712e96856499df9144a70fd93b46cd2159c9f098db069ee8c9989d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6574427261696e7325323041492d3030303030303f7374796c653d666f722d7468652d6261646765266c6f676f3d6a6574627261696e73266c6f676f436f6c6f723d7768697465)](#jetbrains-ai) [![Gemini CLI](https://camo.githubusercontent.com/144305572a6ccbe4d78c0aa4e4fab6d408d43c88ac5afb2a04644a0c268c274f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47656d696e69253230434c492d3432383546343f7374796c653d666f722d7468652d6261646765266c6f676f3d676f6f676c65266c6f676f436f6c6f723d7768697465)](#gemini-cli)\n\nEach AI assistant has custom-configured rules optimized for n8n node development. Click to expand details.\n\n**üñ•Ô∏è Cursor**\n\n**Config Files:**\n\n```\n.cursor/rules/\n‚îú‚îÄ‚îÄ n8n-node-development.mdc   # Main rules (344 lines)\n‚îú‚îÄ‚îÄ credentials.mdc            # Credential-specific rules\n‚îî‚îÄ‚îÄ documentation.mdc          # Doc navigation helper\n```\n\n**Why these files:** Cursor uses MDC format (Markdown + YAML frontmatter) with glob patterns to auto-attach rules when editing specific file types.\n\n**Key Features:**\n\n-   `globs: [\"nodes/**/*.ts\", \"credentials/**/*.ts\"]` ‚Äî Auto-applies to n8n source files\n-   `alwaysApply: false` ‚Äî Only loads when relevant files are open\n-   Excludes `node_modules/**` and `dist/**`\n\n**üèÑ Windsurf**\n\n**Config Files:**\n\n```\n.windsurf/rules/\n‚îú‚îÄ‚îÄ n8n-node-development.md    # Main rules (180 lines)\n‚îú‚îÄ‚îÄ credentials.md             # Credential-specific rules\n‚îî‚îÄ‚îÄ documentation.md           # Doc navigation helper\n```\n\n**Why these files:** Windsurf supports glob-triggered rules that auto-apply based on file patterns.\n\n**Key Features:**\n\n-   `trigger: glob` with `globs: [\"nodes/**/*.ts\"]` ‚Äî Auto-applies to n8n source files\n-   Separate credential rules for `credentials/**/*.ts`\n-   Doc navigation for `docs/**/*.md`\n\n**üü£ Claude Code**\n\n**Config Files:**\n\n```\nCLAUDE.md                      # Project rules (165 lines)\nAGENTS.md                      # Detailed reference (445 lines)\n```\n\n**Why these files:** Claude Code reads `CLAUDE.md` from the project root as its primary instruction file.\n\n**Key Features:**\n\n-   Key commands section for quick reference\n-   Decision tree in prose format\n-   Pattern examples with inline comments\n-   References to all 31 documentation files\n\n**‚¨õ GitHub Copilot**\n\n**Config Files:**\n\n```\n.github/\n‚îî‚îÄ‚îÄ copilot-instructions.md    # Repository instructions (227 lines)\n```\n\n**Why this file:** GitHub Copilot reads `.github/copilot-instructions.md` for repository-level context.\n\n**Key Features:**\n\n-   Technology stack table\n-   ASCII decision tree for quick parsing\n-   Routing types reference table\n-   Complete 31-file documentation index\n\n**üî¥ Cline**\n\n**Config Files:**\n\n```\n.clinerules                    # Project rules (126 lines)\n```\n\n**Why this file:** Cline (VS Code extension) reads `.clinerules` for project-specific instructions.\n\n**Key Features:**\n\n-   Architecture decision matrix\n-   File structure with annotations\n-   SDK execution pattern template\n-   Common mistakes as bullet points\n\n**üü† Roo-Cline**\n\n**Config Files:**\n\n```\n.roomodes                      # Custom modes (264 lines, YAML)\n```\n\n**Why this file:** Roo-Cline supports custom \"modes\" that define different AI personas with specific file access permissions.\n\n**Key Features:**\n\n-   **n8n-node-developer** ‚Äî Full access to nodes/ and credentials/\n-   **n8n-credential-developer** ‚Äî Focused on credentials/ only\n-   **n8n-docs-navigator** ‚Äî Read-only mode for documentation help\n\n**üîµ Continue.dev**\n\n**Config Files:**\n\n```\n.continue/rules/\n‚îî‚îÄ‚îÄ n8n-rules.md               # Project rules (144 lines)\n```\n\n**Why this file:** Continue.dev uses a `rules/` directory for project-specific markdown instructions.\n\n**Key Features:**\n\n-   Uses `@nodes/GithubIssues/` syntax for file references\n-   Table-based common mistakes guide\n-   Integration with Continue's context system\n\n**üî∑ Aider**\n\n**Config Files:**\n\n```\n.aider.conf.yml                # Aider configuration (33 lines)\nAGENTS.md                      # Read by Aider automatically\n```\n\n**Why these files:** Aider uses `.aider.conf.yml` for project configuration and automatically reads files listed in the `read:` section.\n\n**Key Features:**\n\n-   `read:` section loads AGENTS.md and key docs\n-   `lint-cmd: npm run lint` ‚Äî Auto-lint after changes\n-   `auto-lint: true` ‚Äî Automatically run linter\n\n**‚ö° Zed**\n\n**Config Files:**\n\n```\n.rules                         # Zed rules (58 lines)\n```\n\n**Why this file:** Zed reads `.rules` from the project root for AI assistant context.\n\n**Key Features:**\n\n-   Ultra-compact format (58 lines)\n-   Essential commands and patterns only\n-   Reference file paths for deeper context\n\n**‚¨õ JetBrains AI**\n\n**Config Files:**\n\n```\n.aiassistant/rules/\n‚îî‚îÄ‚îÄ n8n-rules.md               # AI Assistant rules (151 lines)\n```\n\n**Why this file:** JetBrains AI Assistant reads from `.aiassistant/rules/` directory for project-specific instructions.\n\n**Key Features:**\n\n-   Table-formatted technology stack\n-   Decision tree with clear branching\n-   IDE-friendly markdown structure\n\n**üîµ Gemini CLI**\n\n**Config Files:**\n\n```\n.gemini/\n‚îî‚îÄ‚îÄ settings.json              # Gemini configuration (JSON)\n```\n\n**Why this file:** Gemini CLI uses JSON configuration for project context.\n\n**Key Features:**\n\n-   `projectContext` with description and rules reference\n-   `codeStyle` preferences for TypeScript strict mode\n-   `keyReferences` pointing to example files\n\n* * *\n\n## üìö Documentation (31 Files)\n\n[](#-documentation-31-files)\n\nYour AI has access to comprehensive n8n documentation. Every pattern, every gotcha, every best practice.\n\n**üìñ Click to see all 31 documentation files**\n\n#\n\nFile\n\nPurpose\n\n00\n\n`documentation-index.md`\n\nMaster index\n\n01\n\n`project-structure-overview.md`\n\nRepository layout\n\n02\n\n`package-json-configuration.md`\n\nnpm setup\n\n03\n\n`typescript-configuration.md`\n\ntsconfig.json\n\n04\n\n`node-anatomy-and-architecture.md`\n\nNode structure\n\n05\n\n`declarative-vs-programmatic-nodes.md`\n\nArchitecture choice\n\n06\n\n`node-properties-reference.md`\n\nProperty types\n\n07\n\n`credentials-system-overview.md`\n\nAuth overview\n\n08\n\n`api-key-credentials.md`\n\nAPI key auth\n\n09\n\n`oauth2-credentials.md`\n\nOAuth2 auth\n\n10\n\n`creating-your-first-node.md`\n\nTutorial\n\n11\n\n`resources-and-operations.md`\n\nMulti-resource\n\n12\n\n`declarative-routing.md`\n\nrouting.send\n\n13\n\n`custom-execute-methods.md`\n\nexecute()\n\n14\n\n`list-search-methods.md`\n\nDropdowns\n\n15\n\n`resource-locators.md`\n\nMulti-mode inputs\n\n16\n\n`pagination-handling.md`\n\nPagination\n\n17\n\n`error-handling-patterns.md`\n\nErrors\n\n18\n\n`helper-functions-and-utilities.md`\n\nUtilities\n\n19\n\n`external-sdk-integration.md`\n\nSDK patterns\n\n20\n\n`icons-and-branding.md`\n\nSVG icons\n\n21\n\n`node-json-metadata.md`\n\nnode.json\n\n22\n\n`development-workflow.md`\n\nDev workflow\n\n23\n\n`testing-strategies.md`\n\nTesting\n\n24\n\n`linting-and-code-quality.md`\n\nESLint\n\n25\n\n`preparing-for-publication.md`\n\nPre-publish\n\n26\n\n`publishing-to-npm.md`\n\nnpm publish\n\n27\n\n`n8n-cloud-verification.md`\n\nCloud verification\n\n28\n\n`complete-code-examples.md`\n\nExamples\n\n29\n\n`common-patterns-and-recipes.md`\n\nRecipes\n\n30\n\n`troubleshooting-guide.md`\n\nTroubleshooting\n\n* * *\n\n## üèóÔ∏è Project Structure\n\n[](#Ô∏è-project-structure)\n\n```\nn8n-community-node-boilerplate/\n‚îú‚îÄ‚îÄ ü§ñ AI Tool Configs\n‚îÇ   ‚îú‚îÄ‚îÄ .cursor/rules/              # Cursor (MDC format)\n‚îÇ   ‚îú‚îÄ‚îÄ .windsurf/rules/            # Windsurf (glob triggers)\n‚îÇ   ‚îú‚îÄ‚îÄ .continue/rules/            # Continue.dev\n‚îÇ   ‚îú‚îÄ‚îÄ .github/copilot-instructions.md  # GitHub Copilot\n‚îÇ   ‚îú‚îÄ‚îÄ .aiassistant/rules/         # JetBrains AI\n‚îÇ   ‚îú‚îÄ‚îÄ .gemini/settings.json       # Gemini CLI\n‚îÇ   ‚îú‚îÄ‚îÄ CLAUDE.md                   # Claude Code\n‚îÇ   ‚îú‚îÄ‚îÄ .clinerules                 # Cline\n‚îÇ   ‚îú‚îÄ‚îÄ .roomodes                   # Roo-Cline\n‚îÇ   ‚îú‚îÄ‚îÄ .rules                      # Zed\n‚îÇ   ‚îî‚îÄ‚îÄ .aider.conf.yml             # Aider\n‚îÇ\n‚îú‚îÄ‚îÄ üìù AGENTS.md                    # Single source of truth (all tools reference this)\n‚îÇ\n‚îú‚îÄ‚îÄ üîê credentials/                 # Auth implementations\n‚îÇ   ‚îú‚îÄ‚îÄ GithubIssuesApi.credentials.ts      # API key example\n‚îÇ   ‚îî‚îÄ‚îÄ GithubIssuesOAuth2Api.credentials.ts # OAuth2 example\n‚îÇ\n‚îú‚îÄ‚îÄ üîå nodes/                       # Node implementations\n‚îÇ   ‚îú‚îÄ‚îÄ Example/                    # Programmatic pattern\n‚îÇ   ‚îî‚îÄ‚îÄ GithubIssues/               # Declarative pattern (full CRUD)\n‚îÇ       ‚îú‚îÄ‚îÄ resources/              # Per-resource operations\n‚îÇ       ‚îú‚îÄ‚îÄ listSearch/             # Dynamic dropdowns\n‚îÇ       ‚îî‚îÄ‚îÄ shared/                 # Reusable utilities\n‚îÇ\n‚îî‚îÄ‚îÄ üìö docs/                        # 31 documentation files\n```\n\n* * *\n\n## ‚ö° Commands\n\n[](#-commands)\n\nCommand\n\nWhat It Does\n\n`npm install`\n\nInstall dependencies\n\n`npm run dev`\n\nStart n8n with hot reload at localhost:5678\n\n`npm run build`\n\nCompile TypeScript\n\n`npm run lint`\n\nCheck code quality\n\n`npm run lint:fix`\n\nAuto-fix lint issues\n\n`npm run release`\n\nCreate new release\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**AI generates wrong routing.send type**\n\nMake sure your AI tool config is loading. Check `AGENTS.md` is present.\n\n**Node doesn't appear in n8n**\n\nRun `npm run build` then restart `npm run dev`. Check `package.json` registration.\n\n**TypeScript errors on build**\n\nCheck imports are at top of file. Verify `INodeType` implementation.\n\n**Credentials not working**\n\nVerify credential name in node matches credential file export.\n\n**Hot reload not working**\n\nKill all n8n processes and restart `npm run dev`.\n\n* * *\n\n## üì¶ What's Included\n\n[](#-whats-included)\n\nComponent\n\nFiles\n\nDescription\n\n**AI Configs**\n\n11\n\nPre-configured rules for every major AI coding assistant\n\n**Documentation**\n\n31\n\nComprehensive guides from basics to advanced patterns\n\n**Example Nodes**\n\n2\n\nWorking GitHub Issues (declarative) + Example (programmatic)\n\n**Credentials**\n\n2\n\nAPI Key + OAuth2 authentication examples\n\n**TypeScript**\n\n‚úÖ\n\nStrict mode, proper types, ESLint configured\n\n* * *\n\n## üåü Resources\n\n[](#-resources)\n\nResource\n\nLink\n\n**n8n Creating Nodes**\n\n[docs.n8n.io/integrations/creating-nodes](https://docs.n8n.io/integrations/creating-nodes/)\n\n**n8n Community Forum**\n\n[community.n8n.io](https://community.n8n.io/)\n\n**n8n Creator Portal**\n\n[creators.n8n.io/nodes](https://creators.n8n.io/nodes)\n\n* * *\n\n## üìÑ License\n\n[](#-license)\n\nMIT License ‚Äî see [LICENSE.md](/yigitkonur/n8n-community-node-boilerplate/blob/master/LICENSE.md)\n\n* * *\n\n**Built with üî• because manually explaining n8n to AI is a soul-crushing waste of time.**\n\n## About\n\nan AI-optimized n8n community node boilerplate with comprehensive documentation and examples\n\n### Topics\n\n[n8n](/topics/n8n \"Topic: n8n\") [n8n-nodes](/topics/n8n-nodes \"Topic: n8n-nodes\") [n8n-community-node-package](/topics/n8n-community-node-package \"Topic: n8n-community-node-package\") [n8n-template](/topics/n8n-template \"Topic: n8n-template\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/n8n-community-node-boilerplate/activity)\n\n### Stars\n\n[**4** stars](/yigitkonur/n8n-community-node-boilerplate/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/n8n-community-node-boilerplate/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/n8n-community-node-boilerplate/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-community-node-boilerplate&report=yigitkonur+%28user%29)\n\n## [Contributors 19](/yigitkonur/n8n-community-node-boilerplate/graphs/contributors)\n\n-   [![@ivov](https://avatars.githubusercontent.com/u/44588767?s=64&v=4)](https://github.com/ivov)\n-   [![@krynble](https://avatars.githubusercontent.com/u/219272?s=64&v=4)](https://github.com/krynble)\n-   [![@janober](https://avatars.githubusercontent.com/u/6249596?s=64&v=4)](https://github.com/janober)\n-   [![@yigitkonur](https://avatars.githubusercontent.com/u/9989650?s=64&v=4)](https://github.com/yigitkonur)\n-   [![@netroy](https://avatars.githubusercontent.com/u/196144?s=64&v=4)](https://github.com/netroy)\n-   [![@Joffcom](https://avatars.githubusercontent.com/u/4688521?s=64&v=4)](https://github.com/Joffcom)\n-   [![@brianinoa](https://avatars.githubusercontent.com/u/54530642?s=64&v=4)](https://github.com/brianinoa)\n-   [![@csuermann](https://avatars.githubusercontent.com/u/10939809?s=64&v=4)](https://github.com/csuermann)\n-   [![@elsmr](https://avatars.githubusercontent.com/u/8850410?s=64&v=4)](https://github.com/elsmr)\n-   [![@nivbe06](https://avatars.githubusercontent.com/u/99671629?s=64&v=4)](https://github.com/nivbe06)\n-   [![@ariarijp](https://avatars.githubusercontent.com/u/635649?s=64&v=4)](https://github.com/ariarijp)\n-   [![@pemontto](https://avatars.githubusercontent.com/u/939704?s=64&v=4)](https://github.com/pemontto)\n-   [![@thiagopxp](https://avatars.githubusercontent.com/u/1581585?s=64&v=4)](https://github.com/thiagopxp)\n-   [![@ozee31](https://avatars.githubusercontent.com/u/5025797?s=64&v=4)](https://github.com/ozee31)\n\n[\\+ 5 contributors](/yigitkonur/n8n-community-node-boilerplate/graphs/contributors)\n\n## Languages\n\n-   [TypeScript 97.1%](/yigitkonur/n8n-community-node-boilerplate/search?l=typescript)\n-   [JavaScript 2.9%](/yigitkonur/n8n-community-node-boilerplate/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/hdbscan-embeddings-clustering\n\nGitHub - yigitkonur/hdbscan-embeddings-clustering: Python script for automated clustering of embeddings with DBSCAN, using cosine similarity for flexible, size-agnostic grouping (I hate K-means); outputs analysis metrics.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fhdbscan-embeddings-clustering)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fhdbscan-embeddings-clustering)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fhdbscan-embeddings-clustering)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[hdbscan-embeddings-clustering](/yigitkonur/hdbscan-embeddings-clustering)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fhdbscan-embeddings-clustering) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Fhdbscan-embeddings-clustering)\n-   [Star 17](/login?return_to=%2Fyigitkonur%2Fhdbscan-embeddings-clustering)\n    \n\nPython script for automated clustering of embeddings with DBSCAN, using cosine similarity for flexible, size-agnostic grouping (I hate K-means); outputs analysis metrics.\n\n[17 stars](/yigitkonur/hdbscan-embeddings-clustering/stargazers) [1 fork](/yigitkonur/hdbscan-embeddings-clustering/forks) [Branches](/yigitkonur/hdbscan-embeddings-clustering/branches) [Tags](/yigitkonur/hdbscan-embeddings-clustering/tags) [Activity](/yigitkonur/hdbscan-embeddings-clustering/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fhdbscan-embeddings-clustering)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fhdbscan-embeddings-clustering) You must be signed in to change notification settings\n\n# yigitkonur/hdbscan-embeddings-clustering\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/hdbscan-embeddings-clustering/branches)[Tags](/yigitkonur/hdbscan-embeddings-clustering/tags)\n\n[](/yigitkonur/hdbscan-embeddings-clustering/branches)[](/yigitkonur/hdbscan-embeddings-clustering/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[9 Commits](/yigitkonur/hdbscan-embeddings-clustering/commits/main/)\n\n[](/yigitkonur/hdbscan-embeddings-clustering/commits/main/)\n\n[README.md](/yigitkonur/hdbscan-embeddings-clustering/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/hdbscan-embeddings-clustering/blob/main/README.md \"README.md\")\n\n[classify.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/classify.py \"classify.py\")\n\n[classify.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/classify.py \"classify.py\")\n\n[classify\\_hdbscan.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/classify_hdbscan.py \"classify_hdbscan.py\")\n\n[classify\\_hdbscan.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/classify_hdbscan.py \"classify_hdbscan.py\")\n\n[embedding\\_clustering\\_toolkit.ipynb](/yigitkonur/hdbscan-embeddings-clustering/blob/main/embedding_clustering_toolkit.ipynb \"embedding_clustering_toolkit.ipynb\")\n\n[embedding\\_clustering\\_toolkit.ipynb](/yigitkonur/hdbscan-embeddings-clustering/blob/main/embedding_clustering_toolkit.ipynb \"embedding_clustering_toolkit.ipynb\")\n\n[requirements.txt](/yigitkonur/hdbscan-embeddings-clustering/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/hdbscan-embeddings-clustering/blob/main/requirements.txt \"requirements.txt\")\n\n[sample.csv](/yigitkonur/hdbscan-embeddings-clustering/blob/main/sample.csv \"sample.csv\")\n\n[sample.csv](/yigitkonur/hdbscan-embeddings-clustering/blob/main/sample.csv \"sample.csv\")\n\n[sweet\\_spot\\_finder.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/sweet_spot_finder.py \"sweet_spot_finder.py\")\n\n[sweet\\_spot\\_finder.py](/yigitkonur/hdbscan-embeddings-clustering/blob/main/sweet_spot_finder.py \"sweet_spot_finder.py\")\n\nView all files\n\n## Repository files navigation\n\n# üî¨ Embedding Clustering Toolkit üî¨\n\n[](#-embedding-clustering-toolkit-)\n\n### Stop guessing cluster counts. Start discovering natural groupings.\n\n[](#stop-guessing-cluster-counts-start-discovering-natural-groupings)\n\n**_The ultimate clustering toolkit for high-dimensional text embeddings. It finds the natural structure in your data using DBSCAN & HDBSCAN‚Äîno magic numbers required._**\n\n[![python](https://camo.githubusercontent.com/0abf037321fd15baf6784cd7007b739a40828c27c3cd0d823ff75ab8245eaf90/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e392b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) [![embeddings](https://camo.githubusercontent.com/e8ceb0f30389316cf0736756df126e211521f9702246458e47a6b4392597d9ab/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f656d62656464696e67732d33303732642d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![no k](https://camo.githubusercontent.com/6f738037e66ee5111a5e882836840e6f3e04d60ad4ccd8b80aa130c577c6bf8e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f8eaf5f6e6f5f6b5f72657175697265642d6175746f6d617469635f636c75737465725f646973636f766572792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/6f738037e66ee5111a5e882836840e6f3e04d60ad4ccd8b80aa130c577c6bf8e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f8eaf5f6e6f5f6b5f72657175697265642d6175746f6d617469635f636c75737465725f646973636f766572792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![jupyter](https://camo.githubusercontent.com/cd7e0a08bcc4ce7119f5b974492469d64615b9abcd86b9bf94d2a3cde613a866/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93935f6a7570797465725f72656164792d696e7465726163746976655f616e616c797369732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/cd7e0a08bcc4ce7119f5b974492469d64615b9abcd86b9bf94d2a3cde613a866/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93935f6a7570797465725f72656164792d696e7465726163746976655f616e616c797369732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration--customization) ‚Ä¢ [**üÜö Why This Works**](#-why-this-slaps-k-means)\n\n* * *\n\n**Embedding Clustering Toolkit** is the analysis partner your embeddings deserve. Stop arbitrarily picking \"k=5\" and praying your clusters make sense. This toolkit uses density-based algorithms that discover the natural groupings in your data‚Äîautomatically identifying how many clusters exist and which points are just noise.\n\n### üéØ\n\n[](#)\n\n**Auto Cluster Detection**  \nNo predefined k needed\n\n### üîç\n\n[](#-1)\n\n**Parameter Search**  \nFind optimal thresholds\n\n### üìä\n\n[](#-2)\n\n**Quality Metrics**  \nSilhouette scores built-in\n\n### üóëÔ∏è\n\n[](#Ô∏è)\n\n**Noise Handling**  \nOutliers isolated cleanly\n\nHow it slaps:\n\n-   **You:** Load your OpenAI/Cohere/any embeddings CSV\n-   **Toolkit:** Searches parameters, finds natural clusters, isolates noise\n-   **You:** Export to Excel, visualize, analyze\n-   **Result:** Meaningful groupings without arbitrary decisions. Go grab a coffee. ‚òï\n\n* * *\n\n## üí• Why This Slaps K-Means\n\n[](#-why-this-slaps-k-means)\n\nClustering embeddings with K-Means is like forcing your data into boxes it doesn't fit. Density-based clustering finds the boxes that actually exist.\n\n**‚ùå The K-Means Way (Pain)**\n\n**‚úÖ The DBSCAN Way (Glory)**\n\n1.  Guess k=10. Run K-Means.\n2.  Results look weird. Try k=15.\n3.  Still bad. Maybe k=8?\n4.  Elbow method says k=12. Sure, why not.\n5.  Get clusters that mix unrelated items.\n\n1.  Run the notebook.\n2.  Algorithm finds 47 natural clusters.\n3.  Outliers are flagged as noise.\n4.  Silhouette score confirms quality.\n5.  Export and ship. Done. üöÄ\n\nWe're not forcing structure. We're **discovering structure** with cosine similarity, density estimation, and automatic parameter optimization that processes your high-dimensional embeddings the right way.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Python 3.9+\n-   Your embeddings in CSV format\n\n### Installation\n\n[](#installation)\n\n# Clone the repository\ngit clone https://github.com/yigitkonur/embedding-clustering-toolkit.git\ncd embedding-clustering-toolkit\n\n# Install dependencies\npip install -r requirements.txt\n\n### Quick Start with Jupyter Notebook\n\n[](#quick-start-with-jupyter-notebook)\n\nThe **recommended way** to use this toolkit is through the interactive Jupyter notebook:\n\n# Launch Jupyter\njupyter notebook embedding\\_clustering\\_toolkit.ipynb\n\nThe notebook provides:\n\n-   üìã **Configurable parameters** at the top\n-   üìä **Interactive visualizations**\n-   üîç **Parameter search** with visual results\n-   üíæ **One-click export** to Excel\n\n### Quick Start with Scripts\n\n[](#quick-start-with-scripts)\n\nIf you prefer command-line scripts:\n\n# 1. Edit the input path in classify.py\n# 2. Run DBSCAN clustering\npython classify.py\n\n# Or run HDBSCAN with PCA\npython classify\\_hdbscan.py\n\n# Or find optimal parameters first\npython sweet\\_spot\\_finder.py\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\n### Using the Jupyter Notebook (Recommended)\n\n[](#using-the-jupyter-notebook-recommended)\n\n**1\\. Configure Your Analysis**\n\nconfig \\= ClusteringConfig(\n    input\\_csv\\_path\\=\"your\\_embeddings.csv\",\n    vector\\_dimension\\=3072,  \\# Match your embedding model\n    similarity\\_threshold\\=0.78,  \\# Higher = tighter clusters\n    min\\_samples\\=2,  \\# Minimum points for a cluster\n)\n\n**2\\. Run All Cells**\n\nThe notebook walks you through:\n\n1.  Loading and validating your embeddings\n2.  Finding optimal parameters (optional but recommended)\n3.  Running DBSCAN and/or HDBSCAN clustering\n4.  Visualizing results\n5.  Exporting to Excel\n\n**3\\. Analyze Results**\n\n```\nüìä DBSCAN Results:\n   ‚îú‚îÄ Clusters: 47\n   ‚îú‚îÄ Noise points: 23 (4.2%)\n   ‚îú‚îÄ Clustered points: 527 (95.8%)\n   ‚îú‚îÄ Avg cluster size: 11.2\n   ‚îî‚îÄ Silhouette score: 0.634\n```\n\n### CSV Format\n\n[](#csv-format)\n\nYour CSV should have embeddings split across columns or in a single column:\n\n**Split format (default):**\n\n```\nName,1,2,3,4,5,6\n\"Document A\",\"0.001,0.023,...\",\"0.045,0.012,...\",\"...\",...\n```\n\n**Single column format:**\n\n```\nName,embedding\n\"Document A\",\"[0.001, 0.023, 0.045, ...]\"\n```\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üéØ DBSCAN Clustering**  \nCosine similarity\n\nGroups embeddings by semantic similarity without predefined k\n\nNatural clusters that actually make sense\n\n**‚ö° HDBSCAN + PCA**  \nDimensionality reduction\n\nReduces 3072D ‚Üí 30D, then clusters\n\n10x faster on large datasets\n\n**üîç Parameter Search**  \nGrid search optimization\n\nTests hundreds of threshold/min\\_samples combos\n\nFind the \"sweet spot\" automatically\n\n**üìä Quality Metrics**  \nSilhouette scoring\n\nMeasures how well-separated your clusters are\n\nKnow if your clustering is actually good\n\n**üóëÔ∏è Noise Detection**  \nOutlier isolation\n\nFlags points that don't belong anywhere\n\nClean clusters without forced assignments\n\n**üìà Visualizations**  \nPCA projections\n\n2D scatter plots of your clusters\n\nSee the structure in your data\n\n**üíæ Excel Export**  \nOne-click output\n\nSorted results with cluster IDs\n\nReady for downstream analysis\n\n* * *\n\n## ‚öôÔ∏è Configuration & Customization\n\n[](#Ô∏è-configuration--customization)\n\n### Key Parameters\n\n[](#key-parameters)\n\nParameter\n\nDefault\n\nDescription\n\n`similarity_threshold`\n\n`0.78`\n\nCosine similarity cutoff (0-1). Higher = tighter clusters.\n\n`min_samples`\n\n`2`\n\nMinimum points to form a cluster.\n\n`vector_dimension`\n\n`3072`\n\nExpected embedding dimensions.\n\n`n_pca_components`\n\n`30`\n\nPCA dimensions for HDBSCAN.\n\n### Choosing Parameters\n\n[](#choosing-parameters)\n\n**If you get...**\n\n**Try...**\n\nToo many tiny clusters\n\nLower `similarity_threshold` (e.g., 0.70)\n\nEverything in one cluster\n\nRaise `similarity_threshold` (e.g., 0.85)\n\nToo much noise\n\nLower `min_samples` to 1\n\nNoisy clusters\n\nRaise `min_samples` to 3-5\n\n### Embedding Model Dimensions\n\n[](#embedding-model-dimensions)\n\nModel\n\nDimensions\n\nSet `vector_dimension` to\n\nOpenAI text-embedding-3-large\n\n3072\n\n`3072`\n\nOpenAI text-embedding-3-small\n\n1536\n\n`1536`\n\nOpenAI text-embedding-ada-002\n\n1536\n\n`1536`\n\nCohere embed-english-v3.0\n\n1024\n\n`1024`\n\nVoyage voyage-2\n\n1024\n\n`1024`\n\nCustom\n\nvaries\n\nyour dimension\n\n* * *\n\n## üìÅ Repository Structure\n\n[](#-repository-structure)\n\n```\nembedding-clustering-toolkit/\n‚îú‚îÄ‚îÄ üìì embedding_clustering_toolkit.ipynb  # Interactive notebook (START HERE)\n‚îú‚îÄ‚îÄ üìú classify.py                         # DBSCAN clustering script\n‚îú‚îÄ‚îÄ üìú classify_hdbscan.py                 # HDBSCAN + PCA script\n‚îú‚îÄ‚îÄ üìú sweet_spot_finder.py                # Parameter optimization script\n‚îú‚îÄ‚îÄ üìã sample.csv                          # Example embeddings data\n‚îú‚îÄ‚îÄ üìã requirements.txt                    # Python dependencies\n‚îî‚îÄ‚îÄ üìñ README.md                           # You are here\n```\n\n* * *\n\n## üîç Understanding the Output\n\n[](#-understanding-the-output)\n\n### Cluster Labels\n\n[](#cluster-labels)\n\n-   **`cluster >= 0`**: Assigned to a specific cluster\n-   **`cluster = -1`**: Noise/outlier (doesn't fit any cluster)\n\n### Quality Metrics\n\n[](#quality-metrics)\n\nMetric\n\nGood\n\nAcceptable\n\nPoor\n\n**Silhouette Score**\n\n\\> 0.5\n\n0.25 - 0.5\n\n< 0.25\n\n**Noise Ratio**\n\n< 10%\n\n10-30%\n\n\\> 30%\n\n**Avg Cluster Size**\n\n\\> 5\n\n3-5\n\n< 3\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**All points are noise**\n\nLower `similarity_threshold` significantly (try 0.5-0.6)\n\n**One giant cluster**\n\nRaise `similarity_threshold` (try 0.85-0.95)\n\n**Out of memory**\n\nUse HDBSCAN with PCA instead of DBSCAN\n\n**Invalid vector dimensions**\n\nCheck your CSV format matches the expected column structure\n\n**hdbscan import error**\n\nRun `pip install hdbscan` (may need C compiler on some systems)\n\n**Slow clustering**\n\nUse HDBSCAN + PCA or reduce `n_pca_components`\n\n* * *\n\n## üÜö DBSCAN vs HDBSCAN: When to Use Which\n\n[](#-dbscan-vs-hdbscan-when-to-use-which)\n\n**DBSCAN**\n\n**HDBSCAN + PCA**\n\n-   Smaller datasets (< 10K points)\n-   You know a good similarity threshold\n-   Uniform cluster densities\n-   Full dimensional analysis needed\n\n-   Large datasets (10K+ points)\n-   Varying cluster densities\n-   Speed is important\n-   Very high dimensions (3000+)\n\n* * *\n\n## üõ†Ô∏è Advanced: Using as a Library\n\n[](#Ô∏è-advanced-using-as-a-library)\n\nfrom embedding\\_clustering\\_toolkit import (\n    ClusteringConfig,\n    EmbeddingDataLoader,\n    DBSCANClusterer,\n    ParameterSearcher\n)\n\n\\# Configure\nconfig \\= ClusteringConfig(\n    input\\_csv\\_path\\=\"my\\_embeddings.csv\",\n    vector\\_dimension\\=1536\n)\n\n\\# Load data\nloader \\= EmbeddingDataLoader(config)\ndf, valid\\_df \\= loader.load()\n\n\\# Find best parameters\nsearcher \\= ParameterSearcher(loader.get\\_vector\\_matrix())\nresults \\= searcher.search()\nbest \\= searcher.get\\_best\\_params(results)\n\n\\# Cluster\nclusterer \\= DBSCANClusterer(loader.get\\_vector\\_matrix())\nlabels \\= clusterer.fit(\n    similarity\\_threshold\\=best\\['similarity\\_threshold'\\],\n    min\\_samples\\=int(best\\['min\\_samples'\\])\n)\n\n* * *\n\n## üåü Star This Repo\n\n[](#-star-this-repo)\n\nIf this toolkit saved you from K-Means hell, give it a ‚≠ê\n\n**Built with üî• because guessing cluster counts is a soul-crushing waste of time.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n## About\n\nPython script for automated clustering of embeddings with DBSCAN, using cosine similarity for flexible, size-agnostic grouping (I hate K-means); outputs analysis metrics.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/hdbscan-embeddings-clustering/activity)\n\n### Stars\n\n[**17** stars](/yigitkonur/hdbscan-embeddings-clustering/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/hdbscan-embeddings-clustering/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/hdbscan-embeddings-clustering/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fhdbscan-embeddings-clustering&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/hdbscan-embeddings-clustering/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=hdbscan-embeddings-clustering)\n\nNo packages published  \n\n## Languages\n\n-   [Jupyter Notebook 79.0%](/yigitkonur/hdbscan-embeddings-clustering/search?l=jupyter-notebook)\n-   [Python 21.0%](/yigitkonur/hdbscan-embeddings-clustering/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/n8n-nodes-latitude\n\nGitHub - yigitkonur/n8n-nodes-latitude: n8n community node for Latitude.so - Execute AI prompts with dynamic parameters                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-latitude)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-latitude)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fn8n-nodes-latitude)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[n8n-nodes-latitude](/yigitkonur/n8n-nodes-latitude)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-latitude) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-latitude)\n-   [Star 4](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-latitude)\n    \n\nn8n community node for Latitude.so - Execute AI prompts with dynamic parameters\n\n### License\n\n[MIT license](/yigitkonur/n8n-nodes-latitude/blob/main/LICENSE.md)\n\n[4 stars](/yigitkonur/n8n-nodes-latitude/stargazers) [0 forks](/yigitkonur/n8n-nodes-latitude/forks) [Branches](/yigitkonur/n8n-nodes-latitude/branches) [Tags](/yigitkonur/n8n-nodes-latitude/tags) [Activity](/yigitkonur/n8n-nodes-latitude/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-latitude)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-latitude) You must be signed in to change notification settings\n\n# yigitkonur/n8n-nodes-latitude\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/n8n-nodes-latitude/branches)[Tags](/yigitkonur/n8n-nodes-latitude/tags)\n\n[](/yigitkonur/n8n-nodes-latitude/branches)[](/yigitkonur/n8n-nodes-latitude/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[21 Commits](/yigitkonur/n8n-nodes-latitude/commits/main/)\n\n[](/yigitkonur/n8n-nodes-latitude/commits/main/)\n\n[credentials](/yigitkonur/n8n-nodes-latitude/tree/main/credentials \"credentials\")\n\n[credentials](/yigitkonur/n8n-nodes-latitude/tree/main/credentials \"credentials\")\n\n[icons](/yigitkonur/n8n-nodes-latitude/tree/main/icons \"icons\")\n\n[icons](/yigitkonur/n8n-nodes-latitude/tree/main/icons \"icons\")\n\n[nodes/Latitude](/yigitkonur/n8n-nodes-latitude/tree/main/nodes/Latitude \"This path skips through empty directories\")\n\n[nodes/Latitude](/yigitkonur/n8n-nodes-latitude/tree/main/nodes/Latitude \"This path skips through empty directories\")\n\n[.gitignore](/yigitkonur/n8n-nodes-latitude/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/n8n-nodes-latitude/blob/main/.gitignore \".gitignore\")\n\n[.npmignore](/yigitkonur/n8n-nodes-latitude/blob/main/.npmignore \".npmignore\")\n\n[.npmignore](/yigitkonur/n8n-nodes-latitude/blob/main/.npmignore \".npmignore\")\n\n[.prettierrc.js](/yigitkonur/n8n-nodes-latitude/blob/main/.prettierrc.js \".prettierrc.js\")\n\n[.prettierrc.js](/yigitkonur/n8n-nodes-latitude/blob/main/.prettierrc.js \".prettierrc.js\")\n\n[CHANGELOG.md](/yigitkonur/n8n-nodes-latitude/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[CHANGELOG.md](/yigitkonur/n8n-nodes-latitude/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[LICENSE.md](/yigitkonur/n8n-nodes-latitude/blob/main/LICENSE.md \"LICENSE.md\")\n\n[LICENSE.md](/yigitkonur/n8n-nodes-latitude/blob/main/LICENSE.md \"LICENSE.md\")\n\n[README.md](/yigitkonur/n8n-nodes-latitude/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/n8n-nodes-latitude/blob/main/README.md \"README.md\")\n\n[eslint.config.mjs](/yigitkonur/n8n-nodes-latitude/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/n8n-nodes-latitude/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[gulpfile.js](/yigitkonur/n8n-nodes-latitude/blob/main/gulpfile.js \"gulpfile.js\")\n\n[gulpfile.js](/yigitkonur/n8n-nodes-latitude/blob/main/gulpfile.js \"gulpfile.js\")\n\n[package-lock.json](/yigitkonur/n8n-nodes-latitude/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/n8n-nodes-latitude/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/n8n-nodes-latitude/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/n8n-nodes-latitude/blob/main/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/n8n-nodes-latitude/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/n8n-nodes-latitude/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n# üöÄ n8n-nodes-latitude üöÄ\n\n[](#-n8n-nodes-latitude-)\n\n### Stop hardcoding AI prompts. Start shipping smarter workflows.\n\n[](#stop-hardcoding-ai-prompts-start-shipping-smarter-workflows)\n\n**_The ultimate n8n integration for Latitude.so ‚Äî the AI prompt management platform. Execute prompts, continue conversations, and log everything with zero JSON wrestling._**\n\n[![npm](https://camo.githubusercontent.com/9fecdc4797569b5b403edaa90b441569d53e19509a50c25f5b29523e6c457996/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6e386e2d6e6f6465732d6c617469747564652e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](https://www.npmjs.com/package/n8n-nodes-latitude) [![node](https://camo.githubusercontent.com/2eade7e541e11c1c3e84720edced6b4ccff0d367a44d70a26fc392d6b8b54c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6f64652d31382b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](https://opensource.org/licenses/MIT) [![n8n](https://camo.githubusercontent.com/6b22164edadc68e64d1f11e094b15eb7ab33fe67755ba968787209892be2d290/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e386e2d636f6d6d756e6974792532306e6f64652d6666366435612e7376673f7374796c653d666c61742d737175617265)](https://n8n.io)\n\n[![zero config](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![ai agent ready](https://camo.githubusercontent.com/786383b64f6ca506bafc0b4faa9f6f291d01c1a4905f6207602b424f6edac805/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f61695f6167656e745f72656164792d7573655f61735f746f6f6c2d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/786383b64f6ca506bafc0b4faa9f6f291d01c1a4905f6207602b424f6edac805/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4965f61695f6167656e745f72656164792d7573655f61735f746f6f6c2d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Operations**](#-operations-reference) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-credentials-setup) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-hardcoded-prompts)\n\n* * *\n\n**n8n-nodes-latitude** is the prompt manager your n8n workflows wish they had. Stop embedding AI prompts directly in your automation and praying they still work next week. This node connects directly to [Latitude.so](https://latitude.so), letting you manage prompts centrally, hot-reload changes, and execute with auto-detected parameters ‚Äî all without touching your workflow.\n\n### üéØ\n\n[](#)\n\n**Dynamic Parameters**  \nAuto-detects {{ variables }}\n\n### üí¨\n\n[](#-1)\n\n**Conversation Memory**  \nContinue multi-turn chats\n\n### ü§ñ\n\n[](#-2)\n\n**AI Agent Ready**  \nUse as tool in AI workflows\n\n### üìä\n\n[](#-3)\n\n**Full Observability**  \nLogs, costs, token tracking\n\nHow it slaps:\n\n-   **You:** Drag Latitude node into n8n workflow\n-   **Node:** Fetches prompts, shows parameters, handles auth\n-   **You:** Map your data ‚Üí Execute\n-   **Latitude:** Returns response with usage stats, costs, and conversation UUID\n-   **Result:** Ship production AI features. Zero prompt maintenance headaches.\n\n* * *\n\n## üí• Why This Slaps Hardcoded Prompts\n\n[](#-why-this-slaps-hardcoded-prompts)\n\nEmbedding prompts in workflows is a vibe-killer. `n8n-nodes-latitude` makes hardcoding look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Latitude Way (Glory)**\n\n1.  Hardcode prompt in n8n HTTP node.\n2.  Manually build JSON payload.\n3.  Change prompt ‚Üí Redeploy workflow.\n4.  No version history. No rollback.\n5.  Debug blind with no observability.\n\n1.  Select prompt from dropdown.\n2.  Parameters auto-populate.\n3.  Change prompt in Latitude ‚Üí Live instantly.\n4.  Version control + cost tracking.\n5.  Full conversation logs in dashboard. ‚òï\n\nWe're not just calling an API. We're giving you **centralized prompt management** with dynamic parameter extraction, conversation continuity, and automatic SDK integration that handles all the complexity.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\n### Community Nodes (Recommended)\n\n[](#community-nodes-recommended)\n\nThe fastest way to get started. No terminal required.\n\n1.  Go to **Settings** ‚Üí **Community Nodes**\n2.  Click **Install**\n3.  Enter `n8n-nodes-latitude`\n4.  Confirm installation\n\nThat's it. The node appears in your node palette.\n\n### Manual Installation\n\n[](#manual-installation)\n\nFor self-hosted n8n or custom setups:\n\n# Navigate to your n8n installation\ncd ~/.n8n/nodes\n\n# Install the package\nnpm install n8n-nodes-latitude\n\n# Restart n8n\n\n> **‚ú® Zero Config:** After installation, add your Latitude credentials and start building. The node handles SDK initialization, auth, and API versioning automatically.\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üéØ Auto Parameter Detection**  \nNo JSON wrestling\n\nExtracts `{{ variables }}` from prompt content automatically\n\nSelect prompt ‚Üí Parameters appear. Done.\n\n**üí¨ Conversation Continuity**  \nMulti-turn chats\n\nContinue conversations using UUID from previous runs\n\nBuild chatbots and agents with memory\n\n**ü§ñ AI Agent Support**  \n`usableAsTool: true`\n\nWorks as a tool in n8n AI workflows\n\nPlug into AI Agent node directly\n\n**üìä Token & Cost Tracking**  \nFull observability\n\nReturns `usage`, `cost`, and `toolCalls` from each run\n\nKnow exactly what you're spending\n\n**üè† Self-Hosted Support**  \nCustom gateway URL\n\nPoint to your own Latitude instance\n\nEnterprise-ready, air-gapped deployments\n\n**üìù External Logging**  \nCreate log entries\n\nLog conversations from external AI calls to Latitude\n\nUnified analytics across all AI touchpoints\n\n**‚ö° Hot Reload**  \nNo redeploy needed\n\nChange prompts in Latitude, live instantly in n8n\n\nIterate on prompts without workflow changes\n\n**üîí Secure by Default**  \nCredentials encrypted\n\nAPI keys never exposed, errors sanitized\n\nProduction-ready security\n\n* * *\n\n## üéÆ Operations Reference\n\n[](#-operations-reference)\n\n### ‚ñ∂Ô∏è\n\n[](#Ô∏è)\n\n**`Run Prompt`**  \nExecute AI prompts\n\n### üí¨\n\n[](#-4)\n\n**`Chat`**  \nContinue conversations\n\n### üìù\n\n[](#-5)\n\n**`Create Log`**  \nLog external AI calls\n\n### `Run Prompt` ‚ñ∂Ô∏è\n\n[](#run-prompt-Ô∏è)\n\nExecute an AI prompt from your Latitude project with automatic parameter detection.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`Prompt Path`\n\nDropdown\n\nYes\n\nSelect from your Latitude prompts ‚Äî shows required parameters\n\n`Parameters`\n\nKey-Value\n\nNo\n\nMap values to `{{ variables }}`. Supports n8n expressions\n\n`Simplify Output`\n\nBoolean\n\nNo\n\nReturn clean data or full conversation history\n\n`Custom Identifier`\n\nString\n\nNo\n\nTag runs for filtering in Latitude dashboard\n\n`Version UUID`\n\nString\n\nNo\n\nUse specific version instead of live\n\n**Example Configuration:**\n\nPrompt Path: \"marketing/personalized-email\"\nParameters:\n  \\- customer\\_name: {{ $json.name }}\n  \\- product: {{ $json.product }}\n  \\- tone: \"professional\"\n\n**Simplified Output:**\n\n{\n  \"uuid\": \"conv\\_abc123-def456\",\n  \"text\": \"Dear John, thank you for your interest in...\",\n  \"usage\": {\n    \"promptTokens\": 150,\n    \"completionTokens\": 200,\n    \"totalTokens\": 350\n  },\n  \"cost\": 0.0035\n}\n\n* * *\n\n### `Chat` üí¨\n\n[](#chat-)\n\nContinue a conversation using the UUID from a previous prompt run.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`Conversation UUID`\n\nString\n\nYes\n\nUUID from a previous `Run Prompt` response\n\n`Messages`\n\nCollection\n\nYes\n\nNew messages to add to the conversation\n\n`Simplify Output`\n\nBoolean\n\nNo\n\nReturn clean data or full conversation history\n\n**Example ‚Äî Building a Chatbot:**\n\n// First node: Run Prompt (initial)\n// Returns: { uuid: \"conv\\_abc123\", text: \"How can I help?\" }\n\n// Second node: Chat (continue)\nConversation UUID: {{ $('Latitude').item.json.uuid }}\nMessages:\n  \\- Role: user\n    Content: \"What's the weather like?\"\n\n* * *\n\n### `Create Log` üìù\n\n[](#create-log-)\n\nLog conversations from external AI calls (OpenAI, Anthropic, etc.) to your Latitude project for unified analytics.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`Prompt Path`\n\nDropdown\n\nYes\n\nThe prompt to associate this log with\n\n`Messages`\n\nCollection\n\nYes\n\nThe conversation messages to log\n\n`Response`\n\nString\n\nNo\n\nThe AI-generated response text\n\n**Use Case:** You're calling OpenAI directly but want all your AI usage visible in Latitude's dashboard:\n\nPrompt Path: \"support/ticket-classifier\"\nMessages:\n  \\- Role: user\n    Content: {{ $json.ticket\\_body }}\nResponse: {{ $json.openai\\_response }}\n\n* * *\n\n## ‚öôÔ∏è Credentials Setup\n\n[](#Ô∏è-credentials-setup)\n\nYou need a Latitude.so account with API access.\n\nField\n\nDescription\n\nWhere to Find\n\n**API Key**\n\nYour Latitude API key\n\nDashboard ‚Üí Settings ‚Üí API Keys (format: `lat_...`)\n\n**Project ID**\n\nNumeric project identifier\n\nDashboard ‚Üí Project Settings or URL\n\n**Gateway URL**\n\n_Optional_ ‚Äî Custom gateway for self-hosted\n\nYour instance URL (e.g., `https://latitude.yourcompany.com`)\n\n### Setting Up Credentials\n\n[](#setting-up-credentials)\n\n1.  Go to **Credentials** in n8n\n2.  Click **New Credential**\n3.  Search for **Latitude API**\n4.  Fill in your API Key and Project ID\n5.  _(Optional)_ Add Gateway URL for self-hosted instances\n6.  Click **Save** ‚Äî Credentials are automatically validated\n\n> **üîí Security Note:** Your API key is encrypted at rest and never exposed in logs or error messages.\n\n* * *\n\n## üî• Workflow Examples\n\n[](#-workflow-examples)\n\n### Basic: Webhook ‚Üí AI Response\n\n[](#basic-webhook--ai-response)\n\n```\nWebhook ‚Üí Latitude (Run Prompt) ‚Üí Respond to Webhook\n```\n\nPerfect for: Slack bots, API endpoints, form processing.\n\n### Advanced: Multi-Turn Chatbot\n\n[](#advanced-multi-turn-chatbot)\n\n```\nWebhook ‚Üí Latitude (Run) ‚Üí Set UUID ‚Üí Loop ‚Üí Latitude (Chat) ‚Üí Respond\n```\n\nPerfect for: Customer support bots, interactive assistants.\n\n### Hybrid: External AI + Latitude Logging\n\n[](#hybrid-external-ai--latitude-logging)\n\n```\nWebhook ‚Üí OpenAI ‚Üí Latitude (Create Log) ‚Üí Respond\n```\n\nPerfect for: Using other providers but want Latitude's analytics.\n\n### AI Agent Integration\n\n[](#ai-agent-integration)\n\n```\nAI Agent ‚Üí Latitude Tool ‚Üí Agent Output\n```\n\nPerfect for: ReAct agents, function calling, tool use.\n\n* * *\n\n## üîß Compatibility\n\n[](#-compatibility)\n\nRequirement\n\nVersion\n\n**n8n**\n\n1.0.0+\n\n**Node.js**\n\n18.0.0+\n\n**Latitude SDK**\n\n5.2.2+\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**Authentication failed**\n\nVerify API key format (`lat_...`) and Project ID in Latitude dashboard. Keys are project-specific.\n\n**Prompt not found**\n\nRefresh the dropdown. Verify the prompt exists and is published (not draft).\n\n**Parameters not loading**\n\nCheck credentials are valid. Run credential test. Ensure prompt has `{{ variables }}`.\n\n**Empty response**\n\nPrompt might be in draft mode. Check Latitude dashboard for prompt status.\n\n**Chat fails**\n\nVerify `conversationUuid` is from a recent run. UUIDs may expire based on project settings.\n\n**Self-hosted connection issues**\n\nEnsure Gateway URL includes protocol (`https://`). Check firewall rules.\n\n**Rate limiting**\n\nLatitude has usage limits. Check your plan in the dashboard.\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\nWant to contribute or customize?\n\n# Clone the repo\ngit clone https://github.com/yigitkonur/n8n-nodes-latitude.git\ncd n8n-nodes-latitude\n\n# Install dependencies\nnpm install\n\n# Development mode (watch for changes)\nnpm run dev\n\n# Build for production\nnpm run build\n\n# Lint and format\nnpm run lint:fix\nnpm run format\n\n### Project Structure\n\n[](#project-structure)\n\n```\n‚îú‚îÄ‚îÄ credentials/          # Latitude API credential definition\n‚îú‚îÄ‚îÄ nodes/Latitude/\n‚îÇ   ‚îú‚îÄ‚îÄ actions/          # Operation implementations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runPrompt.operation.ts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.operation.ts\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ createLog.operation.ts\n‚îÇ   ‚îú‚îÄ‚îÄ methods/          # Dynamic dropdown loaders\n‚îÇ   ‚îú‚îÄ‚îÄ shared/           # Types, utils, transport\n‚îÇ   ‚îî‚îÄ‚îÄ Latitude.node.ts  # Main node definition\n‚îî‚îÄ‚îÄ icons/                # Light/dark mode icons\n```\n\n* * *\n\n## üìö Resources\n\n[](#-resources)\n\n-   **[Latitude Documentation](https://docs.latitude.so)** ‚Äî Full platform docs\n-   **[Latitude API Reference](https://docs.latitude.so/guides/api/api-access)** ‚Äî API details\n-   **[n8n Community Nodes](https://docs.n8n.io/integrations/community-nodes/)** ‚Äî How community nodes work\n-   **[Changelog](/yigitkonur/n8n-nodes-latitude/blob/main/CHANGELOG.md)** ‚Äî Version history and updates\n\n* * *\n\n## ü§ù Contributing\n\n[](#-contributing)\n\nContributions are welcome! This is how we make it better together.\n\n1.  **Fork** the repository\n2.  **Create** your feature branch (`git checkout -b feature/amazing-feature`)\n3.  **Commit** your changes (`git commit -m 'Add amazing feature'`)\n4.  **Push** to the branch (`git push origin feature/amazing-feature`)\n5.  **Open** a Pull Request\n\n* * *\n\n**Built with üî• because hardcoding AI prompts in workflows is a soul-crushing waste of time.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n[GitHub](https://github.com/yigitkonur/n8n-nodes-latitude) ‚Ä¢ [npm](https://www.npmjs.com/package/n8n-nodes-latitude) ‚Ä¢ [Latitude.so](https://latitude.so) ‚Ä¢ [n8n.io](https://n8n.io)\n\n## About\n\nn8n community node for Latitude.so - Execute AI prompts with dynamic parameters\n\n### Topics\n\n[ai](/topics/ai \"Topic: ai\") [latitude](/topics/latitude \"Topic: latitude\") [n8n](/topics/n8n \"Topic: n8n\") [llm](/topics/llm \"Topic: llm\") [n8n-community-node](/topics/n8n-community-node \"Topic: n8n-community-node\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/n8n-nodes-latitude/activity)\n\n### Stars\n\n[**4** stars](/yigitkonur/n8n-nodes-latitude/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/n8n-nodes-latitude/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/n8n-nodes-latitude/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-latitude&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/n8n-nodes-latitude/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=n8n-nodes-latitude)\n\nNo packages published  \n\n## Languages\n\n-   [TypeScript 92.0%](/yigitkonur/n8n-nodes-latitude/search?l=typescript)\n-   [JavaScript 8.0%](/yigitkonur/n8n-nodes-latitude/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/n8n-nodes-craft-daily-notes\n\nGitHub - yigitkonur/n8n-nodes-craft-daily-notes: n8n community node for Craft Daily Notes API - manage blocks, tasks, collections and search                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fn8n-nodes-craft-daily-notes)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[n8n-nodes-craft-daily-notes](/yigitkonur/n8n-nodes-craft-daily-notes)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes)\n-   [Star 0](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes)\n    \n\nn8n community node for Craft Daily Notes API - manage blocks, tasks, collections and search\n\n[www.craft.do/s/hLrMZpKFfYRWPT](https://www.craft.do/s/hLrMZpKFfYRWPT \"https://www.craft.do/s/hLrMZpKFfYRWPT\")\n\n### License\n\n[MIT license](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/LICENSE.md)\n\n[0 stars](/yigitkonur/n8n-nodes-craft-daily-notes/stargazers) [0 forks](/yigitkonur/n8n-nodes-craft-daily-notes/forks) [Branches](/yigitkonur/n8n-nodes-craft-daily-notes/branches) [Tags](/yigitkonur/n8n-nodes-craft-daily-notes/tags) [Activity](/yigitkonur/n8n-nodes-craft-daily-notes/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes) You must be signed in to change notification settings\n\n# yigitkonur/n8n-nodes-craft-daily-notes\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/n8n-nodes-craft-daily-notes/branches)[Tags](/yigitkonur/n8n-nodes-craft-daily-notes/tags)\n\n[](/yigitkonur/n8n-nodes-craft-daily-notes/branches)[](/yigitkonur/n8n-nodes-craft-daily-notes/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[2 Commits](/yigitkonur/n8n-nodes-craft-daily-notes/commits/main/)\n\n[](/yigitkonur/n8n-nodes-craft-daily-notes/commits/main/)\n\n[.github/workflows](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.vscode](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.vscode \".vscode\")\n\n[.vscode](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.vscode \".vscode\")\n\n[.windsurf/rules](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.windsurf/rules \"This path skips through empty directories\")\n\n[.windsurf/rules](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/.windsurf/rules \"This path skips through empty directories\")\n\n[credentials](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/credentials \"credentials\")\n\n[credentials](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/credentials \"credentials\")\n\n[icons](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/icons \"icons\")\n\n[icons](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/icons \"icons\")\n\n[nodes](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/nodes \"nodes\")\n\n[nodes](/yigitkonur/n8n-nodes-craft-daily-notes/tree/main/nodes \"nodes\")\n\n[.gitignore](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/.gitignore \".gitignore\")\n\n[.prettierrc.js](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/.prettierrc.js \".prettierrc.js\")\n\n[.prettierrc.js](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/.prettierrc.js \".prettierrc.js\")\n\n[CHANGELOG.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[CHANGELOG.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[LICENSE.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/LICENSE.md \"LICENSE.md\")\n\n[LICENSE.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/LICENSE.md \"LICENSE.md\")\n\n[README.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/README.md \"README.md\")\n\n[eslint.config.mjs](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n# üìù n8n-nodes-craft üìù\n\n[](#-n8n-nodes-craft-)\n\n### Automate your Craft docs. Stop manual copy-pasting.\n\n[](#automate-your-craft-docs-stop-manual-copy-pasting)\n\n**_The ultimate n8n community nodes for Craft. Manage daily notes, documents, blocks, tasks, and collections ‚Äî all from your workflows._**\n\n[![npm](https://camo.githubusercontent.com/250778312a5b33be42fa8573fcab12ecb60a783051822155749effad726f3f92/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6e386e2d6e6f6465732d63726166742d6461696c792d6e6f7465732e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](https://www.npmjs.com/package/n8n-nodes-craft-daily-notes) [![node](https://camo.githubusercontent.com/93b013205ec89866855d61545712a5894a6145bf788b4723da421b368f95405d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6f64652d25334525334432302e31352e302d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/LICENSE.md) [![n8n](https://camo.githubusercontent.com/108e4365789a47fcd7918f3f3a10baabab0a16306276a5270712d5a1334ebe81/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e386e2d436f6d6d756e6974792532304e6f64652d4646364435412e7376673f7374796c653d666c61742d737175617265)](https://docs.n8n.io/integrations/community-nodes/)\n\n[![daily notes](https://camo.githubusercontent.com/8ee829ec5146d29a8b4e1e6ce8e386a1de0b106b6ddda3449d4f5850e374f71d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93855f4461696c795f4e6f7465732d626c6f636b732c5f7461736b732c5f7365617263682d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/8ee829ec5146d29a8b4e1e6ce8e386a1de0b106b6ddda3449d4f5850e374f71d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93855f4461696c795f4e6f7465732d626c6f636b732c5f7461736b732c5f7365617263682d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![documents](https://camo.githubusercontent.com/c61ca6d85a85c469d7ededb76180e25a5fe1bbbce67610bc4ee5b68be4401735/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93845f446f63756d656e74732d6d756c74692d2d646f635f6d616e6167656d656e742d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/c61ca6d85a85c469d7ededb76180e25a5fe1bbbce67610bc4ee5b68be4401735/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93845f446f63756d656e74732d6d756c74692d2d646f635f6d616e6167656d656e742d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Install**](#-installation) ‚Ä¢ [**üîë Setup**](#-setup) ‚Ä¢ [**‚ú® Features**](#-features) ‚Ä¢ [**üéÆ Examples**](#-examples) ‚Ä¢ [**üõ†Ô∏è Development**](#%EF%B8%8F-development)\n\n* * *\n\n**Two powerful nodes in one package.** Whether you're automating daily journaling, syncing tasks to external systems, or building AI-powered document workflows ‚Äî this package has you covered.\n\n### üìÖ\n\n[](#)\n\n**Craft Daily Notes**  \nBlocks, tasks, collections, search\n\n### üìÑ\n\n[](#-1)\n\n**Craft Documents**  \nMulti-document management\n\n### ü§ñ\n\n[](#-2)\n\n**AI Agent Ready**  \nWorks with n8n AI tools\n\n* * *\n\n## üí• Why This Exists\n\n[](#-why-this-exists)\n\nCraft is beautiful for writing. But getting data in and out? Pain. This package fixes that.\n\n**‚ùå Without This Node**\n\n**‚úÖ With This Node**\n\n1.  Open Craft. Copy block IDs manually.\n2.  Write custom API scripts.\n3.  Debug authentication issues.\n4.  Pray your automation works.\n\n1.  Install the node.\n2.  Paste your Connect URL.\n3.  Build workflows visually.\n4.  Ship it. ‚òï\n\n* * *\n\n## üöÄ Installation\n\n[](#-installation)\n\n### Community Nodes (Recommended)\n\n[](#community-nodes-recommended)\n\n1.  Go to **Settings ‚Üí Community Nodes** in n8n\n2.  Click **Install**\n3.  Enter `n8n-nodes-craft-daily-notes`\n4.  Click **Install**\n\n### Manual Installation\n\n[](#manual-installation)\n\ncd ~/.n8n/nodes\nnpm install n8n-nodes-craft-daily-notes\n\n* * *\n\n## üîë Setup\n\n[](#-setup)\n\n### Daily Notes API\n\n[](#daily-notes-api)\n\n1.  In Craft: **Settings ‚Üí Connect ‚Üí Daily Notes & Tasks**\n2.  Copy your Connect API URL\n3.  In n8n: Create **Craft Daily Notes API** credentials\n4.  Paste the URL\n\n### Documents API\n\n[](#documents-api)\n\n1.  In Craft: **Settings ‚Üí Connect ‚Üí Your Connection**\n2.  Copy your Connect API URL\n3.  In n8n: Create **Craft Documents API** credentials\n4.  Paste the URL\n\n> **üîê Security Note:** The API URL contains your auth token. Keep it private.\n\n* * *\n\n## ‚ú® Features\n\n[](#-features)\n\n### Craft Daily Notes\n\n[](#craft-daily-notes)\n\nResource\n\nOperations\n\n**üì¶ Block**\n\nGet, Insert, Update, Delete, Move, Search\n\n**‚úÖ Task**\n\nGet, Add, Update, Delete\n\n**üóÇÔ∏è Collection**\n\nList, Get Schema, Get/Add/Update/Delete Items\n\n**üîç Search**\n\nSearch Across All Daily Notes\n\n### Craft Documents\n\n[](#craft-documents)\n\nResource\n\nOperations\n\n**üìÑ Document**\n\nList All Documents\n\n**üì¶ Block**\n\nGet, Insert, Update, Delete, Move, Search\n\n**üóÇÔ∏è Collection**\n\nList, Get Schema, Get/Add/Update/Delete Items\n\n**üîç Search**\n\nSearch Across All Documents\n\n### üéØ Key Capabilities\n\n[](#-key-capabilities)\n\n-   **üìÖ Relative Dates** ‚Äî Use `today`, `tomorrow`, `yesterday` or `YYYY-MM-DD`\n-   **ü§ñ AI Agent Support** ‚Äî Works as a tool in AI-powered workflows\n-   **üìã Smart Dropdowns** ‚Äî Collections load dynamically from your data\n-   **‚ö° Declarative Routing** ‚Äî Clean, maintainable node architecture\n\n* * *\n\n## üéÆ Examples\n\n[](#-examples)\n\n### Get Today's Daily Note\n\n[](#get-todays-daily-note)\n\n```\nNode: Craft Daily Notes\nResource: Block ‚Üí Get\nDate: today\n```\n\n### Add Task to Inbox\n\n[](#add-task-to-inbox)\n\n```\nNode: Craft Daily Notes\nResource: Task ‚Üí Add\nContent: \"Review pull requests\"\nLocation: Inbox\n```\n\n### Insert Content into Document\n\n[](#insert-content-into-document)\n\n```\nNode: Craft Documents\nResource: Block ‚Üí Insert\nDocument ID: (select from dropdown)\nContent: \"## Meeting Notes\\n\\n- Point 1\\n- Point 2\"\n```\n\n### Search Across Everything\n\n[](#search-across-everything)\n\n```\nNode: Craft Daily Notes\nResource: Search ‚Üí Search Across Daily Notes\nTerms: \"project alpha\"\n```\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\n# Install dependencies\nnpm install\n\n# Development mode (hot reload)\nnpm run dev\n\n# Build for production\nnpm run build\n\n# Lint code\nnpm run lint\n\n### Project Structure\n\n[](#project-structure)\n\n```\n‚îú‚îÄ‚îÄ credentials/          # API credential definitions\n‚îú‚îÄ‚îÄ nodes/\n‚îÇ   ‚îú‚îÄ‚îÄ CraftDailyNotes/  # Daily Notes node\n‚îÇ   ‚îî‚îÄ‚îÄ CraftDocuments/   # Documents node\n‚îú‚îÄ‚îÄ icons/                # Node icons (light/dark)\n‚îî‚îÄ‚îÄ dist/                 # Compiled output\n```\n\n* * *\n\n## üìö Resources\n\n[](#-resources)\n\n-   [Craft Connect Documentation](https://www.craft.do/s/hLrMZpKFfYRWPT)\n-   [n8n Community Nodes Guide](https://docs.n8n.io/integrations/community-nodes/)\n-   [Changelog](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/CHANGELOG.md)\n\n* * *\n\n## üë§ Author\n\n[](#-author)\n\n**Yigit Konur**\n\n[![GitHub](https://camo.githubusercontent.com/efb42ce2543f512e6b06ac5927c26c072d189d5809bd7aee13973fba95f4d7a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4769744875622d79696769746b6f6e75722d3138313731373f7374796c653d666c61742d737175617265266c6f676f3d676974687562)](https://github.com/yigitkonur) [![Email](https://camo.githubusercontent.com/092be96ec329d54c1498d098812339b8ca39a22030d3dd9110e9031c3e85c29c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f456d61696c2d7969676974406b6f6e75722e6465762d4541343333353f7374796c653d666c61742d737175617265266c6f676f3d676d61696c266c6f676f436f6c6f723d7768697465)](mailto:yigit@konur.dev)\n\n* * *\n\n**[MIT License](/yigitkonur/n8n-nodes-craft-daily-notes/blob/main/LICENSE.md)** ‚Äî Built with üî• for the Craft + n8n community.\n\n## About\n\nn8n community node for Craft Daily Notes API - manage blocks, tasks, collections and search\n\n[www.craft.do/s/hLrMZpKFfYRWPT](https://www.craft.do/s/hLrMZpKFfYRWPT \"https://www.craft.do/s/hLrMZpKFfYRWPT\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/n8n-nodes-craft-daily-notes/activity)\n\n### Stars\n\n[**0** stars](/yigitkonur/n8n-nodes-craft-daily-notes/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/n8n-nodes-craft-daily-notes/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/n8n-nodes-craft-daily-notes/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-nodes-craft-daily-notes&report=yigitkonur+%28user%29)\n\n## [Releases 2](/yigitkonur/n8n-nodes-craft-daily-notes/releases)\n\n[\n\nv1.0.27 Latest\n\nNov 29, 2025\n\n](/yigitkonur/n8n-nodes-craft-daily-notes/releases/tag/v1.0.27)\n\n[\\+ 1 release](/yigitkonur/n8n-nodes-craft-daily-notes/releases)\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=n8n-nodes-craft-daily-notes)\n\nNo packages published  \n\n## [Contributors 20](/yigitkonur/n8n-nodes-craft-daily-notes/graphs/contributors)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[\\+ 6 contributors](/yigitkonur/n8n-nodes-craft-daily-notes/graphs/contributors)\n\n## Languages\n\n-   [TypeScript 98.9%](/yigitkonur/n8n-nodes-craft-daily-notes/search?l=typescript)\n-   [JavaScript 1.1%](/yigitkonur/n8n-nodes-craft-daily-notes/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/research-powerpack-mcp\n\nGitHub - yigitkonur/research-powerpack-mcp: MCP for deep-dive bug research to use by Claude Code / Cursor / Codex                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fresearch-powerpack-mcp)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fresearch-powerpack-mcp)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fresearch-powerpack-mcp)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[research-powerpack-mcp](/yigitkonur/research-powerpack-mcp)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fresearch-powerpack-mcp) You must be signed in to change notification settings\n-   [Fork 2](/login?return_to=%2Fyigitkonur%2Fresearch-powerpack-mcp)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2Fresearch-powerpack-mcp)\n    \n\nMCP for deep-dive bug research to use by Claude Code / Cursor / Codex\n\n[1 star](/yigitkonur/research-powerpack-mcp/stargazers) [2 forks](/yigitkonur/research-powerpack-mcp/forks) [Branches](/yigitkonur/research-powerpack-mcp/branches) [Tags](/yigitkonur/research-powerpack-mcp/tags) [Activity](/yigitkonur/research-powerpack-mcp/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fresearch-powerpack-mcp)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fresearch-powerpack-mcp) You must be signed in to change notification settings\n\n# yigitkonur/research-powerpack-mcp\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/research-powerpack-mcp/branches)[Tags](/yigitkonur/research-powerpack-mcp/tags)\n\n[](/yigitkonur/research-powerpack-mcp/branches)[](/yigitkonur/research-powerpack-mcp/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[47 Commits](/yigitkonur/research-powerpack-mcp/commits/main/)\n\n[](/yigitkonur/research-powerpack-mcp/commits/main/)\n\n[.github/workflows](/yigitkonur/research-powerpack-mcp/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/research-powerpack-mcp/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[src](/yigitkonur/research-powerpack-mcp/tree/main/src \"src\")\n\n[src](/yigitkonur/research-powerpack-mcp/tree/main/src \"src\")\n\n[tests](/yigitkonur/research-powerpack-mcp/tree/main/tests \"tests\")\n\n[tests](/yigitkonur/research-powerpack-mcp/tree/main/tests \"tests\")\n\n[.env.example](/yigitkonur/research-powerpack-mcp/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/research-powerpack-mcp/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/research-powerpack-mcp/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/research-powerpack-mcp/blob/main/.gitignore \".gitignore\")\n\n[README.md](/yigitkonur/research-powerpack-mcp/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/research-powerpack-mcp/blob/main/README.md \"README.md\")\n\n[package-lock.json](/yigitkonur/research-powerpack-mcp/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/research-powerpack-mcp/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/research-powerpack-mcp/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/research-powerpack-mcp/blob/main/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/research-powerpack-mcp/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/research-powerpack-mcp/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n# üî¨ Research Powerpack MCP üî¨\n\n[](#-research-powerpack-mcp-)\n\n### Stop tab-hopping for research. Start getting god-tier context.\n\n[](#stop-tab-hopping-for-research-start-getting-god-tier-context)\n\n**_The ultimate research toolkit for your AI coding assistant. It searches the web, mines Reddit, scrapes any URL, and synthesizes everything into perfectly structured context your LLM actually understands._**\n\n[![npm](https://camo.githubusercontent.com/4dd9af5d8948d76556e20f6c65d3eabdf89cc9bba553fc9dec3d1d0611e9d530/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f72657365617263682d706f7765727061636b2d6d63702e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](https://www.npmjs.com/package/research-powerpack-mcp) [![node](https://camo.githubusercontent.com/2eade7e541e11c1c3e84720edced6b4ccff0d367a44d70a26fc392d6b8b54c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6f64652d31382b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](https://opensource.org/licenses/MIT) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![modular](https://camo.githubusercontent.com/9aa79112f6770e42dabd53305bef67ce8102ada38a989f471711815e089d8b34/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a95f6d6f64756c61722d7573655f315f746f6f6c5f6f725f616c6c5f352d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/9aa79112f6770e42dabd53305bef67ce8102ada38a989f471711815e089d8b34/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a95f6d6f64756c61722d7573655f315f746f6f6c5f6f725f616c6c5f352d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![zero crash](https://camo.githubusercontent.com/6aa63444c5bf6a686b7d72963841690079b919f1ebaa674f1cf1c9d4c066be00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92aa5f7a65726f5f63726173682d6d697373696e675f6b6579735f3d5f68656c7066756c5f6572726f72732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/6aa63444c5bf6a686b7d72963841690079b919f1ebaa674f1cf1c9d4c066be00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92aa5f7a65726f5f63726173682d6d697373696e675f6b6579735f3d5f68656c7066756c5f6572726f72732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-tool-reference) ‚Ä¢ [**‚öôÔ∏è API Key Setup**](#-api-key-setup-guides) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-other-methods)\n\n* * *\n\n**`research-powerpack-mcp`** is the research assistant your AI wishes it had. Stop asking your LLM to guess about things it doesn't know. This MCP server acts like a senior researcher, searching the web, mining Reddit discussions, scraping documentation, and synthesizing everything into perfectly structured context so your AI can actually give you answers worth a damn.\n\n### üîç\n\n[](#)\n\n**Batch Web Search**  \n100 keywords in parallel\n\n### üí¨\n\n[](#-1)\n\n**Reddit Mining**  \nReal opinions, not marketing\n\n### üåê\n\n[](#-2)\n\n**Universal Scraping**  \nJS rendering + geo-targeting\n\n### üß†\n\n[](#-3)\n\n**Deep Research**  \nAI synthesis with citations\n\nHow it slaps:\n\n-   **You:** \"What's the best database for my use case?\"\n-   **AI + Powerpack:** Searches Google, mines Reddit threads, scrapes docs, synthesizes findings.\n-   **You:** Get an actually informed answer with real community opinions and citations.\n-   **Result:** Ship better decisions. Skip the 47 browser tabs.\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nManually researching is a vibe-killer. `research-powerpack-mcp` makes other methods look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Powerpack Way (Glory)**\n\n1.  Open 15 browser tabs.\n2.  Skim Stack Overflow answers from 2019.\n3.  Search Reddit, get distracted by drama.\n4.  Copy-paste random snippets to your AI.\n5.  Get a mediocre answer from confused context.\n\n1.  Ask your AI to research it.\n2.  AI searches, scrapes, mines Reddit automatically.\n3.  Receive synthesized insights with sources.\n4.  Make an informed decision.\n5.  Go grab a coffee. ‚òï\n\nWe're not just fetching random pages. We're building **high-signal, low-noise context** with CTR-weighted ranking, smart comment allocation, and intelligent token distribution that prevents massive responses from breaking your LLM's context window.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\n### 1\\. Install\n\n[](#1-install)\n\nnpm install research-powerpack-mcp\n\n### 2\\. Configure Your MCP Client\n\n[](#2-configure-your-mcp-client)\n\nClient\n\nConfig File\n\nDocs\n\nüñ•Ô∏è **Claude Desktop**\n\n`claude_desktop_config.json`\n\n[Setup](#claude-desktop)\n\n‚å®Ô∏è **Claude Code**\n\n`~/.claude.json` or CLI\n\n[Setup](#claude-code-cli)\n\nüéØ **Cursor**\n\n`.cursor/mcp.json`\n\n[Setup](#cursorwindsurf)\n\nüèÑ **Windsurf**\n\nMCP settings\n\n[Setup](#cursorwindsurf)\n\n#### Claude Desktop\n\n[](#claude-desktop)\n\nAdd to your `claude_desktop_config.json`:\n\n{\n  \"mcpServers\": {\n    \"research-powerpack\": {\n      \"command\": \"npx\",\n      \"args\": \\[\"research-powerpack-mcp\"\\],\n      \"env\": {\n        \"SERPER\\_API\\_KEY\": \"your\\_key\",\n        \"REDDIT\\_CLIENT\\_ID\": \"your\\_id\",\n        \"REDDIT\\_CLIENT\\_SECRET\": \"your\\_secret\",\n        \"SCRAPEDO\\_API\\_KEY\": \"your\\_key\",\n        \"OPENROUTER\\_API\\_KEY\": \"your\\_key\"\n      }\n    }\n  }\n}\n\n#### Claude Code (CLI)\n\n[](#claude-code-cli)\n\nOne command to rule them all:\n\nclaude mcp add research-powerpack npx \\\\\n  --scope user \\\\\n  --env SERPER\\_API\\_KEY=your\\_key \\\\\n  --env REDDIT\\_CLIENT\\_ID=your\\_id \\\\\n  --env REDDIT\\_CLIENT\\_SECRET=your\\_secret \\\\\n  --env OPENROUTER\\_API\\_KEY=your\\_key \\\\\n  --env OPENROUTER\\_BASE\\_URL=https://openrouter.ai/api/v1 \\\\\n  --env RESEARCH\\_MODEL=x-ai/grok-4.1-fast \\\\\n  -- research-powerpack-mcp\n\nOr manually add to `~/.claude.json`:\n\n{\n  \"mcpServers\": {\n    \"research-powerpack\": {\n      \"command\": \"npx\",\n      \"args\": \\[\"research-powerpack-mcp\"\\],\n      \"env\": {\n        \"SERPER\\_API\\_KEY\": \"your\\_key\",\n        \"REDDIT\\_CLIENT\\_ID\": \"your\\_id\",\n        \"REDDIT\\_CLIENT\\_SECRET\": \"your\\_secret\",\n        \"OPENROUTER\\_API\\_KEY\": \"your\\_key\",\n        \"OPENROUTER\\_BASE\\_URL\": \"https://openrouter.ai/api/v1\",\n        \"RESEARCH\\_MODEL\": \"x-ai/grok-4.1-fast\"\n      }\n    }\n  }\n}\n\n#### Cursor/Windsurf\n\n[](#cursorwindsurf)\n\nAdd to `.cursor/mcp.json` or equivalent:\n\n{\n  \"mcpServers\": {\n    \"research-powerpack\": {\n      \"command\": \"npx\",\n      \"args\": \\[\"research-powerpack-mcp\"\\],\n      \"env\": {\n        \"SERPER\\_API\\_KEY\": \"your\\_key\"\n      }\n    }\n  }\n}\n\n> **‚ú® Zero Crash Promise:** Missing API keys? No problem. The server always starts. Tools just return helpful setup instructions instead of exploding.\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üîç Batch Search**  \n`100 keywords parallel`\n\nSearch Google for up to 100 queries simultaneously\n\nCover every angle of a topic in one shot\n\n**üìä CTR Ranking**  \n`Smart URL scoring`\n\nIdentifies URLs that appear across multiple searches\n\nSurfaces high-consensus authoritative sources\n\n**üí¨ Reddit Mining**  \n`Real human opinions`\n\nGoogle-powered Reddit search + native API fetching\n\nGet actual user experiences, not marketing fluff\n\n**üéØ Smart Allocation**  \n`Token-aware budgets`\n\n1,000 comment budget distributed across posts\n\nDeep dive on 2 posts or quick scan on 50\n\n**üåê Universal Scraping**  \n`Works on everything`\n\nAuto-fallback: basic ‚Üí JS render ‚Üí geo-targeting\n\nHandles SPAs, paywalls, and geo-restricted content\n\n**üß† Deep Research**  \n`AI-powered synthesis`\n\nBatch research with web search and citations\n\nGet comprehensive answers to complex questions\n\n**üß© Modular Design**  \n`Use what you need`\n\nEach tool works independently\n\nPay only for the APIs you actually use\n\n* * *\n\n## üéÆ Tool Reference\n\n[](#-tool-reference)\n\n### üîç\n\n[](#-4)\n\n**`web_search`**  \nBatch Google search\n\n### üí¨\n\n[](#-5)\n\n**`search_reddit`**  \nFind Reddit discussions\n\n### üìñ\n\n[](#-6)\n\n**`get_reddit_post`**  \nFetch posts + comments\n\n### üåê\n\n[](#-7)\n\n**`scrape_links`**  \nExtract any URL\n\n### üß†\n\n[](#-8)\n\n**`deep_research`**  \nAI synthesis\n\n### `web_search`\n\n[](#web_search)\n\n**Batch web search** using Google via Serper API. Search up to 100 keywords in parallel.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`keywords`\n\n`string[]`\n\nYes\n\nSearch queries (1-100). Use distinct keywords for maximum coverage.\n\n**Supports Google operators:** `site:`, `-exclusion`, `\"exact phrase\"`, `filetype:`\n\n{\n  \"keywords\": \\[\n    \"best IDE 2025\",\n    \"VS Code alternatives\",\n    \"Cursor vs Windsurf comparison\"\n  \\]\n}\n\n* * *\n\n### `search_reddit`\n\n[](#search_reddit)\n\n**Search Reddit** via Google with automatic `site:reddit.com` filtering.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`queries`\n\n`string[]`\n\nYes\n\nSearch queries (max 10)\n\n`date_after`\n\n`string`\n\nNo\n\nFilter results after date (YYYY-MM-DD)\n\n**Search operators:** `intitle:keyword`, `\"exact phrase\"`, `OR`, `-exclude`\n\n{\n  \"queries\": \\[\n    \"best mechanical keyboard 2025\",\n    \"intitle:keyboard recommendation\"\n  \\],\n  \"date\\_after\": \"2024-01-01\"\n}\n\n* * *\n\n### `get_reddit_post`\n\n[](#get_reddit_post)\n\n**Fetch Reddit posts** with smart comment allocation (1,000 comment budget distributed automatically).\n\nParameter\n\nType\n\nRequired\n\nDefault\n\nDescription\n\n`urls`\n\n`string[]`\n\nYes\n\n‚Äî\n\nReddit post URLs (2-50)\n\n`fetch_comments`\n\n`boolean`\n\nNo\n\n`true`\n\nWhether to fetch comments\n\n`max_comments`\n\n`number`\n\nNo\n\nauto\n\nOverride comment allocation\n\n**Smart Allocation:**\n\n-   2 posts ‚Üí ~500 comments/post (deep dive)\n-   10 posts ‚Üí ~100 comments/post\n-   50 posts ‚Üí ~20 comments/post (quick scan)\n\n{\n  \"urls\": \\[\n    \"https://reddit.com/r/programming/comments/abc123/post\\_title\",\n    \"https://reddit.com/r/webdev/comments/def456/another\\_post\"\n  \\]\n}\n\n* * *\n\n### `scrape_links`\n\n[](#scrape_links)\n\n**Universal URL content extraction** with automatic fallback modes.\n\nParameter\n\nType\n\nRequired\n\nDefault\n\nDescription\n\n`urls`\n\n`string[]`\n\nYes\n\n‚Äî\n\nURLs to scrape (3-50)\n\n`timeout`\n\n`number`\n\nNo\n\n`30`\n\nTimeout per URL (seconds)\n\n`use_llm`\n\n`boolean`\n\nNo\n\n`false`\n\nEnable AI extraction\n\n`what_to_extract`\n\n`string`\n\nNo\n\n‚Äî\n\nExtraction instructions for AI\n\n**Automatic Fallback:** Basic ‚Üí JS rendering ‚Üí JS + US geo-targeting\n\n{\n  \"urls\": \\[\"https://example.com/article1\", \"https://example.com/article2\"\\],\n  \"use\\_llm\": true,\n  \"what\\_to\\_extract\": \"Extract the main arguments and key statistics\"\n}\n\n* * *\n\n### `deep_research`\n\n[](#deep_research)\n\n**AI-powered batch research** with web search and citations.\n\nParameter\n\nType\n\nRequired\n\nDescription\n\n`questions`\n\n`object[]`\n\nYes\n\nResearch questions (2-10)\n\n`questions[].question`\n\n`string`\n\nYes\n\nThe research question\n\n`questions[].file_attachments`\n\n`object[]`\n\nNo\n\nFiles to include as context\n\n**Token Allocation:** 32,000 tokens distributed across questions:\n\n-   2 questions ‚Üí 16,000 tokens/question (deep dive)\n-   10 questions ‚Üí 3,200 tokens/question (rapid multi-topic)\n\n{\n  \"questions\": \\[\n    { \"question\": \"What are the current best practices for React Server Components in 2025?\" },\n    { \"question\": \"Compare Bun vs Node.js for production workloads with benchmarks.\" }\n  \\]\n}\n\n* * *\n\n## ‚öôÔ∏è Environment Variables & Tool Availability\n\n[](#Ô∏è-environment-variables--tool-availability)\n\nResearch Powerpack uses a **modular architecture**. Tools are automatically enabled based on which API keys you provide:\n\nENV Variable\n\nTools Enabled\n\nFree Tier\n\n`SERPER_API_KEY`\n\n`web_search`, `search_reddit`\n\n2,500 queries/mo\n\n`REDDIT_CLIENT_ID` + `SECRET`\n\n`get_reddit_post`\n\nUnlimited\n\n`SCRAPEDO_API_KEY`\n\n`scrape_links`\n\n1,000 credits/mo\n\n`OPENROUTER_API_KEY`\n\n`deep_research` + AI in `scrape_links`\n\nPay-as-you-go\n\n`RESEARCH_MODEL`\n\nModel for `deep_research`\n\nDefault: `perplexity/sonar-deep-research`\n\n`LLM_EXTRACTION_MODEL`\n\nModel for AI extraction in `scrape_links`\n\nDefault: `openrouter/gpt-oss-120b:nitro`\n\n### Configuration Examples\n\n[](#configuration-examples)\n\n# Search-only mode (just web\\_search and search\\_reddit)\nSERPER\\_API\\_KEY=xxx\n\n# Reddit research mode (search + fetch posts)\nSERPER\\_API\\_KEY=xxx\nREDDIT\\_CLIENT\\_ID=xxx\nREDDIT\\_CLIENT\\_SECRET=xxx\n\n# Full research mode (all 5 tools)\nSERPER\\_API\\_KEY=xxx\nREDDIT\\_CLIENT\\_ID=xxx\nREDDIT\\_CLIENT\\_SECRET=xxx\nSCRAPEDO\\_API\\_KEY=xxx\nOPENROUTER\\_API\\_KEY=xxx\n\n* * *\n\n## üîë API Key Setup Guides\n\n[](#-api-key-setup-guides)\n\n**üîç Serper API (Google Search) ‚Äî FREE: 2,500 queries/month**\n\n### What you get\n\n[](#what-you-get)\n\n-   Fast Google search results via API\n-   Enables `web_search` and `search_reddit` tools\n\n### Setup Steps\n\n[](#setup-steps)\n\n1.  Go to [serper.dev](https://serper.dev)\n2.  Click **\"Get API Key\"** (top right)\n3.  Sign up with email or Google\n4.  Copy your API key from the dashboard\n5.  Add to your config:\n    \n    ```\n    SERPER_API_KEY=your_key_here\n    ```\n    \n\n### Pricing\n\n[](#pricing)\n\n-   **Free**: 2,500 queries/month\n-   **Paid**: $50/month for 50,000 queries\n\n**ü§ñ Reddit OAuth ‚Äî FREE: Unlimited access**\n\n### What you get\n\n[](#what-you-get-1)\n\n-   Full Reddit API access\n-   Fetch posts and comments with upvote sorting\n-   Enables `get_reddit_post` tool\n\n### Setup Steps\n\n[](#setup-steps-1)\n\n1.  Go to [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)\n2.  Scroll down and click **\"create another app...\"**\n3.  Fill in:\n    -   **Name**: `research-powerpack` (or any name)\n    -   **App type**: Select **\"script\"** (important!)\n    -   **Redirect URI**: `http://localhost:8080`\n4.  Click **\"create app\"**\n5.  Copy your credentials:\n    -   **Client ID**: The string under your app name\n    -   **Client Secret**: The \"secret\" field\n6.  Add to your config:\n    \n    ```\n    REDDIT_CLIENT_ID=your_client_id\n    REDDIT_CLIENT_SECRET=your_client_secret\n    ```\n**üåê Scrape.do (Web Scraping) ‚Äî FREE: 1,000 credits/month**\n\n### What you get\n\n[](#what-you-get-2)\n\n-   JavaScript rendering support\n-   Geo-targeting and CAPTCHA handling\n-   Enables `scrape_links` tool\n\n### Setup Steps\n\n[](#setup-steps-2)\n\n1.  Go to [scrape.do](https://scrape.do)\n2.  Click **\"Start Free\"**\n3.  Sign up with email\n4.  Copy your API key from the dashboard\n5.  Add to your config:\n    \n    ```\n    SCRAPEDO_API_KEY=your_key_here\n    ```\n    \n\n### Credit Usage\n\n[](#credit-usage)\n\n-   **Basic scrape**: 1 credit\n-   **JavaScript rendering**: 5 credits\n-   **Geo-targeting**: +25 credits\n\n**üß† OpenRouter (AI Models) ‚Äî Pay-as-you-go**\n\n### What you get\n\n[](#what-you-get-3)\n\n-   Access to 100+ AI models via one API\n-   Enables `deep_research` tool\n-   Enables AI extraction in `scrape_links`\n\n### Setup Steps\n\n[](#setup-steps-3)\n\n1.  Go to [openrouter.ai](https://openrouter.ai)\n2.  Sign up with Google/GitHub/email\n3.  Go to [openrouter.ai/keys](https://openrouter.ai/keys)\n4.  Click **\"Create Key\"**\n5.  Copy the key (starts with `sk-or-...`)\n6.  Add to your config:\n    \n    ```\n    OPENROUTER_API_KEY=sk-or-v1-xxxxx\n    ```\n    \n\n### Recommended Models for Deep Research\n\n[](#recommended-models-for-deep-research)\n\n# Default (optimized for research)\nRESEARCH\\_MODEL=perplexity/sonar-deep-research\n\n# Fast and capable\nRESEARCH\\_MODEL=x-ai/grok-4.1-fast\n\n# High quality\nRESEARCH\\_MODEL=anthropic/claude-3.5-sonnet\n\n# Budget-friendly\nRESEARCH\\_MODEL=openai/gpt-4o-mini\n\n### Recommended Models for AI Extraction (`use_llm` in `scrape_links`)\n\n[](#recommended-models-for-ai-extraction-use_llm-in-scrape_links)\n\n# Default (fast and cost-effective for extraction)\nLLM\\_EXTRACTION\\_MODEL=openrouter/gpt-oss-120b:nitro\n\n# High quality extraction\nLLM\\_EXTRACTION\\_MODEL=anthropic/claude-3.5-sonnet\n\n# Budget-friendly\nLLM\\_EXTRACTION\\_MODEL=openai/gpt-4o-mini\n\n> **Note:** `RESEARCH_MODEL` and `LLM_EXTRACTION_MODEL` are independent. You can use a powerful model for deep research and a faster/cheaper model for content extraction, or vice versa.\n\n* * *\n\n## üî• Recommended Workflows\n\n[](#-recommended-workflows)\n\n### Research a Technology Decision\n\n[](#research-a-technology-decision)\n\n```\n1. web_search ‚Üí [\"React vs Vue 2025\", \"Next.js vs Nuxt comparison\"]\n2. search_reddit ‚Üí [\"best frontend framework 2025\", \"Next.js production experience\"]\n3. get_reddit_post ‚Üí [URLs from step 2]\n4. scrape_links ‚Üí [Documentation and blog URLs from step 1]\n5. deep_research ‚Üí [Synthesize findings into specific questions]\n```\n\n### Competitive Analysis\n\n[](#competitive-analysis)\n\n```\n1. web_search ‚Üí [\"competitor name review\", \"competitor vs alternatives\"]\n2. scrape_links ‚Üí [Competitor websites, review sites]\n3. search_reddit ‚Üí [\"competitor name experience\", \"switching from competitor\"]\n4. get_reddit_post ‚Üí [URLs from step 3]\n```\n\n### Debug an Obscure Error\n\n[](#debug-an-obscure-error)\n\n```\n1. web_search ‚Üí [\"exact error message\", \"error + framework name\"]\n2. search_reddit ‚Üí [\"error message\", \"framework + error type\"]\n3. get_reddit_post ‚Üí [URLs with solutions]\n4. scrape_links ‚Üí [Stack Overflow answers, GitHub issues]\n```\n\n* * *\n\n## üî• Enable Full Power Mode\n\n[](#-enable-full-power-mode)\n\nFor the best research experience, configure all four API keys:\n\nSERPER\\_API\\_KEY=your\\_serper\\_key       # Free: 2,500 queries/month\nREDDIT\\_CLIENT\\_ID=your\\_reddit\\_id       # Free: Unlimited\nREDDIT\\_CLIENT\\_SECRET=your\\_reddit\\_secret\nSCRAPEDO\\_API\\_KEY=your\\_scrapedo\\_key   # Free: 1,000 credits/month\nOPENROUTER\\_API\\_KEY=your\\_openrouter\\_key # Pay-as-you-go\n\nThis unlocks:\n\n-   **5 research tools** working together\n-   **AI-powered content extraction** in scrape\\_links\n-   **Deep research with web search** and citations\n-   **Complete Reddit mining** (search ‚Üí fetch ‚Üí analyze)\n\n**Total setup time:** ~10 minutes. **Total free tier value:** ~$50/month equivalent.\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\n# Clone\ngit clone https://github.com/yigitkonur/research-powerpack-mcp.git\ncd research-powerpack-mcp\n\n# Install\nnpm install\n\n# Development\nnpm run dev\n\n# Build\nnpm run build\n\n# Type check\nnpm run typecheck\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**Tool returns \"API key not configured\"**\n\nAdd the required ENV variable to your MCP config. The error message tells you exactly which key is missing.\n\n**Reddit posts returning empty**\n\nCheck your `REDDIT_CLIENT_ID` and `REDDIT_CLIENT_SECRET`. Make sure you created a \"script\" type app.\n\n**Scraping fails on JavaScript sites**\n\nThis is expected for first attempt. The tool auto-retries with JS rendering. If still failing, the site may be blocking scrapers.\n\n**Deep research taking too long**\n\nUse a faster model like `x-ai/grok-4.1-fast` instead of `perplexity/sonar-deep-research`.\n\n**Token limit errors**\n\nReduce the number of URLs/questions per request. The tool distributes a fixed token budget.\n\n* * *\n\n**Built with üî• because manually researching for your AI is a soul-crushing waste of time.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n## About\n\nMCP for deep-dive bug research to use by Claude Code / Cursor / Codex\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/research-powerpack-mcp/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/research-powerpack-mcp/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/research-powerpack-mcp/watchers)\n\n### Forks\n\n[**2** forks](/yigitkonur/research-powerpack-mcp/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fresearch-powerpack-mcp&report=yigitkonur+%28user%29)\n\n## [Contributors 3](/yigitkonur/research-powerpack-mcp/graphs/contributors)\n\n¬†¬†¬†### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [TypeScript 95.3%](/yigitkonur/research-powerpack-mcp/search?l=typescript)\n-   [JavaScript 4.7%](/yigitkonur/research-powerpack-mcp/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/code-to-clipboard-for-llms\n\nGitHub - yigitkonur/code-to-clipboard-for-llms: LLM-optimized repo copy: visual tree w/ stats (loc/%) + smart filtering (+.gitignore) + LLM markdown output (hints) + clipboard copy + token saving exclusions + easy installers + custom flags (incl/excl/size/types) + prioritizes root .md + logical folder traversal (keeps context grouped) w/ file size sort + file/stdout opts + set as alias easily                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcode-to-clipboard-for-llms)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcode-to-clipboard-for-llms)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fcode-to-clipboard-for-llms)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[code-to-clipboard-for-llms](/yigitkonur/code-to-clipboard-for-llms)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fcode-to-clipboard-for-llms) You must be signed in to change notification settings\n-   [Fork 3](/login?return_to=%2Fyigitkonur%2Fcode-to-clipboard-for-llms)\n-   [Star 45](/login?return_to=%2Fyigitkonur%2Fcode-to-clipboard-for-llms)\n    \n\nLLM-optimized repo copy: visual tree w/ stats (loc/%) + smart filtering (+.gitignore) + LLM markdown output (hints) + clipboard copy + token saving exclusions + easy installers + custom flags (incl/excl/size/types) + prioritizes root .md + logical folder traversal (keeps context grouped) w/ file size sort + file/stdout opts + set as alias easily\n\n[thinkbuddy.ai](https://thinkbuddy.ai \"https://thinkbuddy.ai\")\n\n[45 stars](/yigitkonur/code-to-clipboard-for-llms/stargazers) [3 forks](/yigitkonur/code-to-clipboard-for-llms/forks) [Branches](/yigitkonur/code-to-clipboard-for-llms/branches) [Tags](/yigitkonur/code-to-clipboard-for-llms/tags) [Activity](/yigitkonur/code-to-clipboard-for-llms/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fcode-to-clipboard-for-llms)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fcode-to-clipboard-for-llms) You must be signed in to change notification settings\n\n# yigitkonur/code-to-clipboard-for-llms\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/code-to-clipboard-for-llms/branches)[Tags](/yigitkonur/code-to-clipboard-for-llms/tags)\n\n[](/yigitkonur/code-to-clipboard-for-llms/branches)[](/yigitkonur/code-to-clipboard-for-llms/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[37 Commits](/yigitkonur/code-to-clipboard-for-llms/commits/main/)\n\n[](/yigitkonur/code-to-clipboard-for-llms/commits/main/)\n\n[.github/workflows](/yigitkonur/code-to-clipboard-for-llms/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/code-to-clipboard-for-llms/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.gitignore](/yigitkonur/code-to-clipboard-for-llms/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/code-to-clipboard-for-llms/blob/main/.gitignore \".gitignore\")\n\n[README.md](/yigitkonur/code-to-clipboard-for-llms/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/code-to-clipboard-for-llms/blob/main/README.md \"README.md\")\n\n[llmcontext.py](/yigitkonur/code-to-clipboard-for-llms/blob/main/llmcontext.py \"llmcontext.py\")\n\n[llmcontext.py](/yigitkonur/code-to-clipboard-for-llms/blob/main/llmcontext.py \"llmcontext.py\")\n\n[pyproject.toml](/yigitkonur/code-to-clipboard-for-llms/blob/main/pyproject.toml \"pyproject.toml\")\n\n[pyproject.toml](/yigitkonur/code-to-clipboard-for-llms/blob/main/pyproject.toml \"pyproject.toml\")\n\nView all files\n\n## Repository files navigation\n\n# üì¶ repo-to-llm-context üì¶\n\n[](#-repo-to-llm-context-)\n\n### Stop copy-pasting code. Start shipping smarter prompts.\n\n[](#stop-copy-pasting-code-start-shipping-smarter-prompts)\n\n**_the ultimate context packer for your AI coding assistant. it scans your repo, ditches the junk, and bundles the good stuff into one perfect, clipboard-ready prompt._**\n\n[![pypi](https://camo.githubusercontent.com/588e1fce43cde509399510448d5468c1cde1aec1cba9cd9579503afdab28bc59/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7265706f2d746f2d6c6c6d2d636f6e746578742e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](#) [![python](https://camo.githubusercontent.com/9a9a98bc7718c0cf7a156a094578d426cbc14ba6efbd7e4624bc37c3c1bd5b24/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![zero config](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a847dd4a8e4a6b9dd33b1d3c2878cb864756ed3170cf7a18090d79e571e4e65b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f6f75745f6f665f7468655f626f782d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![clipboard ready](https://camo.githubusercontent.com/fc48909f2e7d59e4100c0fd9b22605fcd9745bd196dcb1b51da3f7f66116aca5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938b5f636c6970626f6172645f72656164792d6f6e655f636f6d6d616e645f746f5f636f70792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/fc48909f2e7d59e4100c0fd9b22605fcd9745bd196dcb1b51da3f7f66116aca5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938b5f636c6970626f6172645f72656164792d6f6e655f636f6d6d616e645f746f5f636f70792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Advanced Flags**](#%EF%B8%8F-advanced-usage--customization) ‚Ä¢ [**üÜö Comparison**](#-why-this-slaps-other-methods)\n\n* * *\n\n**`repo-to-llm-context`** is the project manager your AI assistant wishes it had. Stop feeding your LLM random files and praying for a good answer. This tool acts like a pro developer, reading your entire project, intelligently selecting the most relevant files, and packaging them into a perfectly structured prompt so your AI can actually understand what the hell is going on.\n\n### üß†\n\n[](#)\n\n**Smart Filtering**  \nDitches node\\_modules & junk\n\n### üéØ\n\n[](#-1)\n\n**Relevance Scoring**  \nPuts the important code first\n\n### üìã\n\n[](#-2)\n\n**Clipboard Ready**  \nOne command, ready to paste\n\nHow it slaps:\n\n-   **You:** `cd my-project && context`\n-   **`context`:** Scans, filters, scores, formats, and copies.\n-   **You:** `Cmd+V` into Claude/ChatGPT/Gemini.\n-   **LLM:** \"Ah, I see. A well-structured project. Here is your god-tier answer.\"\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nManually prepping context is a vibe-killer. `repo-to-llm-context` makes other methods look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The \\`context\\` Way (Glory)**\n\n1.  Open 15 files in VS Code.\n2.  Frantically copy-paste into a text file.\n3.  Realize you forgot the Dockerfile.\n4.  Curse as you hit the token limit.\n5.  Get a mediocre answer from a confused LLM.\n\n1.  `cd my-project`\n2.  `context`\n3.  Paste.\n4.  Receive genius-level insights.\n5.  Go grab a coffee. ‚òï\n\nWe're not just concatenating files. We're building a **high-signal, low-noise prompt** with intelligent depth-first traversal that processes directories systematically, prioritizes README files, and prevents massive files from breaking your LLM's context window.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\nThe `context` command will be available in your terminal after installation.\n\nPlatform\n\nRecommended Method\n\nOne-liner\n\nüçé **macOS**\n\nHomebrew\n\n`brew install yigitkonur/context/context`\n\nü™ü **Windows**\n\nScoop\n\n`scoop bucket add context https://github.com/yigitkonur/scoop-context && scoop install context`\n\nüêß **Linux**\n\nHomebrew\n\n`brew install yigitkonur/context/context`\n\n### üçé macOS: Homebrew (Recommended)\n\n[](#-macos-homebrew-recommended)\n\nThe cleanest, most native experience for Mac users.\n\n# Add the tap and install (one-time setup)\nbrew tap yigitkonur/context\nbrew install yigitkonur/context/context\n\n### ü™ü Windows: Scoop (Recommended)\n\n[](#-windows-scoop-recommended)\n\n[Scoop](https://scoop.sh/) is the developer's choice for Windows package management. No admin rights needed!\n\n# First, install Scoop if you don't have it (one-time)\nSet-ExecutionPolicy \\-ExecutionPolicy RemoteSigned \\-Scope CurrentUser\nInvoke-RestMethod \\-Uri https://get.scoop.sh | Invoke-Expression\n\n# Add the bucket and install\nscoop bucket add context https://github.com/yigitkonur/scoop\\-context\nscoop install context\n\n### ü™ü Windows: pipx (Alternative)\n\n[](#-windows-pipx-alternative)\n\nIf you already have Python installed and prefer `pipx`:\n\n# 1. Install pipx if you don't have it\npython \\-m pip install \\--user pipx\npython \\-m pipx ensurepath\n\n# 2. Install the tool\npipx install repo\\-to\\-llm\\-context\n\n### üêß Linux: Homebrew (Recommended)\n\n[](#-linux-homebrew-recommended)\n\nHomebrew works great on Linux too!\n\n# Install Homebrew for Linux (if not installed)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Add the tap and install\nbrew tap yigitkonur/context\nbrew install yigitkonur/context/context\n\n### üêß Linux: pipx (Alternative)\n\n[](#-linux-pipx-alternative)\n\nFor Linux users who prefer `pipx` over Homebrew:\n\n# 1. Install pipx if you don't have it\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\n\n# 2. Install the tool\npipx install repo-to-llm-context\n\n> **‚ú® Zero Manual Setup:** After installation, the `context` command should be ready to go. If not, just restart your terminal!\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\nThe workflow is dead simple.\n\n**1\\. Navigate to Your Project**\n\ncd /path/to/your/killer-app\n\n**2\\. Run the Command**\n\ncontext\n\nYour clipboard is now loaded with perfectly formatted Markdown.\n\n**3\\. Paste & Prompt** Go to your favorite LLM and paste the context. Now you can ask the real questions.\n\n### ÔøΩ GitHub Repository Support (NEW!)\n\n[](#-github-repository-support-new)\n\nYou can now clone and analyze any public GitHub repository directly without downloading it locally!\n\n**Supported Formats:**\n\n-   `owner/repo` - Shorthand format\n-   `owner/repo@branch` - Specific branch\n-   `https://github.com/owner/repo` - Full URL\n-   `github.com/owner/repo` - Short URL\n\n**Examples:**\n\n# Clone and scan a repository\ncontext sindresorhus/is-online\n\n# Scan specific branch\ncontext facebook/react@main\n\n# Full GitHub URL\ncontext https://github.com/torvalds/linux\n\n# With filtering options\ncontext owner/repo --include \"\\*.py\" --preview\n\n# Output to file instead of clipboard\ncontext owner/repo --output analysis.md\n\nThe tool automatically:\n\n-   Performs shallow clones for speed\n-   Manages temporary directories (auto-cleanup on exit)\n-   Shows GitHub source links in output\n-   Applies the same intelligent filtering to remote repositories\n\n### ÔøΩüìè Large File Control (NEW!)\n\n[](#-large-file-control-new)\n\nGot massive JSON files or generated code breaking your LLM context? We've got you covered.\n\n-   **Skip large files entirely:**\n    \n    context --skip-large-files --max-file-chars 5000\n    \n-   **Truncate large files with smart preview:**\n    \n    context --truncate-large-files --max-file-chars 8000\n    \n-   **Custom limits for different projects:**\n    \n    context --max-file-chars 15000 --truncate-large-files\n    \n\nThe tool shows exactly what it's doing:\n\n```\nINFO: Truncated response.json: 1,487,897 ‚Üí 10,075 chars\nSuccess: 530,745 chars copied to clipboard\n```\n\n### Output Control üïπÔ∏è\n\n[](#output-control-Ô∏è)\n\nDon't want it on your clipboard? No problem.\n\n-   **Save to a file:**\n    \n    context --output project\\_context.md\n    \n-   **Print directly to your terminal (for piping or peeking):**\n    \n    context --stdout\n    \n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üß† Smart Filtering**  \n`No junk allowed`\n\nAuto-excludes `node_modules`, `venv`, `builds`, `.git`, logs & more\n\nStops you from wasting tokens on garbage\n\n**üéØ Depth-First Sorting**  \n`Perfect file order`\n\nTraverses directories systematically, README.md files first\n\nYour LLM gets context in logical, hierarchical order\n\n**üìè Large File Control**  \n`Token-aware sizing`\n\nSkip or truncate files over configurable limits (default: 50K chars)\n\nNever blow your LLM's context window again\n\n**üèóÔ∏è Project Tree**  \n`Visual context`\n\nIncludes a `tree`\\-style view of what's included\n\nThe AI (and you) can see the project structure\n\n**‚öôÔ∏è Git-Aware**  \n`Respects your repo`\n\nCan read your `.gitignore` and check tracking status\n\nContext matches your actual source code\n\n**üìã Clipboard Ready**  \n`Cmd+C on steroids`\n\nCopies the entire formatted output in one go\n\nZero manual work between terminal and AI\n\n**üîß Hyper-Configurable**  \n`You're the boss`\n\nFlags to include/exclude anything you want\n\nFine-tune the context for any weird project\n\n**üîí Privacy First**  \n`No path leaks`\n\nMasks your local home directory path in the summary\n\nShare your code, not your user folder\n\n**üéØ Interactive Mode**  \n`Guided setup`\n\nRun `--interactive` for a step-by-step config wizard\n\nPerfect for first-time users or complex setups\n\n* * *\n\n## ‚öôÔ∏è Advanced Usage & Customization\n\n[](#Ô∏è-advanced-usage--customization)\n\nThe defaults are great, but you can dial it in just right.\n\n**Expand for the full list of command-line flags**\n\n#### Filtering and Inclusion Control\n\n[](#filtering-and-inclusion-control)\n\n-   `--include PATTERN`: Glob pattern to force inclusion of files/directories that might be excluded (e.g., `--include \"config/**.yaml\"`).\n-   `--exclude PATTERN`: Glob pattern to add custom exclusions beyond the defaults (e.g., `--exclude \"*.log\"`).\n-   `--include-only`: A powerful mode that includes _only_ files matching `--include` patterns, excluding everything else.\n-   `--exclude-extension EXT`: Exclude all files with a specific extension (e.g., `--exclude-extension .tmp`).\n-   `--include-extension EXT`: Force include files with an extension that is normally excluded by default.\n\n#### Override Default File Type Exclusions\n\n[](#override-default-file-type-exclusions)\n\n-   `--include-json`: Include `.json` / `.jsonc` files.\n-   `--include-yaml`: Include `.yaml` / `.yml` files.\n-   `--include-xml`: Include `.xml` files.\n-   `--include-html`: Include `.html` / `.htm` files.\n-   `--include-css`: Include `.css` files.\n-   `--include-sql`: Include `.sql` files.\n-   `--include-csv`: Include `.csv` / `.tsv` files.\n-   `--include-markdown`: Include all Markdown files, not just the root `README.md`.\n\n#### Size and Content Control\n\n[](#size-and-content-control)\n\n-   `--max-size SIZE`: Exclude files larger than the specified size (e.g., `500k`, `10M`). Default is `2M`.\n-   `--max-file-chars N`: Set maximum characters per file (default: 50,000). Works with skip/truncate options.\n-   `--skip-large-files`: Skip files that exceed the `--max-file-chars` limit entirely.\n-   `--truncate-large-files`: Keep large files but show only the first N characters with a truncation notice.\n-   `--include-binary`: Attempt to include files detected as binary (default is to exclude them).\n-   `--max-depth N`: Limit scanning to a maximum directory depth.\n\n#### Git Integration Behavior\n\n[](#git-integration-behavior)\n\n-   `--no-gitignore`: Ignore `.gitignore` rules and Git tracking status entirely.\n-   `--gitignore-only`: (Default) Use `.gitignore` rules for exclusion but _don't_ filter based on Git tracking status.\n-   `--use-git`: Use both `.gitignore` rules and only include files that are tracked by Git.\n\n#### Output and Execution Behavior\n\n[](#output-and-execution-behavior)\n\n-   `--output FILE`: Write output to a file instead of the clipboard.\n-   `--stdout`: Print the full output to the terminal.\n-   `--no-clipboard`: Disable automatic copying to the clipboard (useful when using `--stdout` or `--output`).\n-   `--preview`: Show a summary of what would be included without processing files or generating output.\n-   `--dry-run`: Run the entire process but do not write any output to the clipboard, file, or stdout.\n-   `--sort-alpha`: Override the relevance-based sorting and sort files alphabetically instead.\n\n#### Information\n\n[](#information)\n\n-   `--version`: Display the current version and exit.\n-   `--check-updates`: Check for available updates.\n\n* * *\n\n## üÜö Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods-1)\n\nMethod\n\nThe Pain üò©\n\nThe `context` Way üòé\n\n**Manual Copy/Paste**\n\nYou'll miss a file. You'll include junk. You'll hate your life.\n\nOne command. Perfect context. Every time.\n\n**`cat file1 file2 > out.txt`**\n\nZero structure. No filtering. Still manual. Basically useless.\n\nAuto-filters, adds a file tree, and formats beautifully.\n\n**Sharing a GitHub Link**\n\nLLM can't see local changes. Can't access private repos.\n\nWorks offline. Works on your latest, unpushed code.\n\n**Simple `tree` command**\n\nShows structure but includes zero code content.\n\nGives you the full package: structure AND content.\n\n* * *\n\n## üõ†Ô∏è For Developers & Tinkerers\n\n[](#Ô∏è-for-developers--tinkerers)\n\n### Running from Source\n\n[](#running-from-source)\n\nWant to hack on the code? Easy.\n\n1.  **Clone the repo:**\n    \n    git clone https://github.com/yigitkonur/code-to-clipboard-for-llms.git\n    cd code-to-clipboard-for-llms\n    \n2.  **Set up a virtual environment and install in editable mode:**\n    \n    python3 -m venv venv\n    source venv/bin/activate\n    pip install -e .\n    \n    Now, any change you make to `llmcontext.py` will be live on your `context` command.\n    \n\n### Fork & Customize\n\n[](#fork--customize)\n\nIf you fork the repo, you can permanently change the default filters by editing the constants at the top of `llmcontext.py`.\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**`context: command not found`**\n\n**Restart your terminal.** 99% of the time, this is the fix. If not, run `pipx ensurepath` (for pipx) or check your `PATH` environment variable.\n\n**Clipboard isn't working**\n\n**Linux users:** You might need a clipboard utility. Run `sudo apt install xclip` or `sudo pacman -S xclip`. For any OS, you can always use `--stdout` or `--output my_context.md` to bypass the clipboard.\n\n**`.gitignore` is ignored**\n\nMake sure you have `gitignore-parser` installed. The tool uses `.gitignore` by default. Use `--no-gitignore` to disable.\n\n**Script errors out**\n\nMake sure you're on Python 3.8 or newer (`python3 --version`).\n\n**Windows-specific issues:**\n\nProblem\n\nSolution\n\n**Scoop install fails**\n\nMake sure you've enabled script execution: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`\n\n**`context` not recognized in PowerShell**\n\nClose and reopen PowerShell, or run `scoop reset context` to refresh the shim.\n\n**Permission denied errors**\n\nScoop installs to user directory, so admin rights shouldn't be needed. If using pipx, run PowerShell as Administrator.\n\n**Python not found (pipx method)**\n\nDownload Python from [python.org](https://www.python.org/downloads/) and ensure \"Add Python to PATH\" is checked during installation.\n\n* * *\n\n**Built with üî• because manually crafting LLM prompts is a soul-crushing waste of time.**\n\n## About\n\nLLM-optimized repo copy: visual tree w/ stats (loc/%) + smart filtering (+.gitignore) + LLM markdown output (hints) + clipboard copy + token saving exclusions + easy installers + custom flags (incl/excl/size/types) + prioritizes root .md + logical folder traversal (keeps context grouped) w/ file size sort + file/stdout opts + set as alias easily\n\n[thinkbuddy.ai](https://thinkbuddy.ai \"https://thinkbuddy.ai\")\n\n### Topics\n\n[markdown](/topics/markdown \"Topic: markdown\") [cli](/topics/cli \"Topic: cli\") [productivity](/topics/productivity \"Topic: productivity\") [ai](/topics/ai \"Topic: ai\") [cross-platform](/topics/cross-platform \"Topic: cross-platform\") [scripting](/topics/scripting \"Topic: scripting\") [developer-tools](/topics/developer-tools \"Topic: developer-tools\") [developer-productivity](/topics/developer-productivity \"Topic: developer-productivity\") [llm](/topics/llm \"Topic: llm\") [prompt-engineering](/topics/prompt-engineering \"Topic: prompt-engineering\") [llms](/topics/llms \"Topic: llms\") [chatgpt](/topics/chatgpt \"Topic: chatgpt\") [context-management](/topics/context-management \"Topic: context-management\") [clipboard-automation](/topics/clipboard-automation \"Topic: clipboard-automation\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/code-to-clipboard-for-llms/activity)\n\n### Stars\n\n[**45** stars](/yigitkonur/code-to-clipboard-for-llms/stargazers)\n\n### Watchers\n\n[**2** watching](/yigitkonur/code-to-clipboard-for-llms/watchers)\n\n### Forks\n\n[**3** forks](/yigitkonur/code-to-clipboard-for-llms/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcode-to-clipboard-for-llms&report=yigitkonur+%28user%29)\n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/code-to-clipboard-for-llms/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/claude-cto\n\nGitHub - yigitkonur/claude-cto: Fire-and-forget task execution system for Claude Code SDK with CLI and MCP interfaces                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fclaude-cto)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fclaude-cto)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fclaude-cto)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[claude-cto](/yigitkonur/claude-cto)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fclaude-cto) You must be signed in to change notification settings\n-   [Fork 2](/login?return_to=%2Fyigitkonur%2Fclaude-cto)\n-   [Star 9](/login?return_to=%2Fyigitkonur%2Fclaude-cto)\n    \n\nFire-and-forget task execution system for Claude Code SDK with CLI and MCP interfaces\n\n### License\n\n[MIT license](/yigitkonur/claude-cto/blob/main/LICENSE)\n\n[9 stars](/yigitkonur/claude-cto/stargazers) [2 forks](/yigitkonur/claude-cto/forks) [Branches](/yigitkonur/claude-cto/branches) [Tags](/yigitkonur/claude-cto/tags) [Activity](/yigitkonur/claude-cto/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fclaude-cto)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fclaude-cto) You must be signed in to change notification settings\n\n# yigitkonur/claude-cto\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/claude-cto/branches)[Tags](/yigitkonur/claude-cto/tags)\n\n[](/yigitkonur/claude-cto/branches)[](/yigitkonur/claude-cto/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[151 Commits](/yigitkonur/claude-cto/commits/main/)\n\n[](/yigitkonur/claude-cto/commits/main/)\n\n[.github/workflows](/yigitkonur/claude-cto/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/claude-cto/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[claude\\_cto](/yigitkonur/claude-cto/tree/main/claude_cto \"claude_cto\")\n\n[claude\\_cto](/yigitkonur/claude-cto/tree/main/claude_cto \"claude_cto\")\n\n[docs](/yigitkonur/claude-cto/tree/main/docs \"docs\")\n\n[docs](/yigitkonur/claude-cto/tree/main/docs \"docs\")\n\n[examples](/yigitkonur/claude-cto/tree/main/examples \"examples\")\n\n[examples](/yigitkonur/claude-cto/tree/main/examples \"examples\")\n\n[scripts](/yigitkonur/claude-cto/tree/main/scripts \"scripts\")\n\n[scripts](/yigitkonur/claude-cto/tree/main/scripts \"scripts\")\n\n[.dockerignore](/yigitkonur/claude-cto/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/claude-cto/blob/main/.dockerignore \".dockerignore\")\n\n[.gitignore](/yigitkonur/claude-cto/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/claude-cto/blob/main/.gitignore \".gitignore\")\n\n[CHANGELOG.md](/yigitkonur/claude-cto/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[CHANGELOG.md](/yigitkonur/claude-cto/blob/main/CHANGELOG.md \"CHANGELOG.md\")\n\n[CLAUDE.md](/yigitkonur/claude-cto/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/claude-cto/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[Dockerfile](/yigitkonur/claude-cto/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/claude-cto/blob/main/Dockerfile \"Dockerfile\")\n\n[LICENSE](/yigitkonur/claude-cto/blob/main/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/claude-cto/blob/main/LICENSE \"LICENSE\")\n\n[MANIFEST.in](/yigitkonur/claude-cto/blob/main/MANIFEST.in \"MANIFEST.in\")\n\n[MANIFEST.in](/yigitkonur/claude-cto/blob/main/MANIFEST.in \"MANIFEST.in\")\n\n[README.md](/yigitkonur/claude-cto/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/claude-cto/blob/main/README.md \"README.md\")\n\n[VERSION\\_MANAGEMENT.md](/yigitkonur/claude-cto/blob/main/VERSION_MANAGEMENT.md \"VERSION_MANAGEMENT.md\")\n\n[VERSION\\_MANAGEMENT.md](/yigitkonur/claude-cto/blob/main/VERSION_MANAGEMENT.md \"VERSION_MANAGEMENT.md\")\n\n[config.example.json](/yigitkonur/claude-cto/blob/main/config.example.json \"config.example.json\")\n\n[config.example.json](/yigitkonur/claude-cto/blob/main/config.example.json \"config.example.json\")\n\n[docker-compose.yml](/yigitkonur/claude-cto/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/claude-cto/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-entrypoint.sh](/yigitkonur/claude-cto/blob/main/docker-entrypoint.sh \"docker-entrypoint.sh\")\n\n[docker-entrypoint.sh](/yigitkonur/claude-cto/blob/main/docker-entrypoint.sh \"docker-entrypoint.sh\")\n\n[poetry.lock](/yigitkonur/claude-cto/blob/main/poetry.lock \"poetry.lock\")\n\n[poetry.lock](/yigitkonur/claude-cto/blob/main/poetry.lock \"poetry.lock\")\n\n[pyproject.toml](/yigitkonur/claude-cto/blob/main/pyproject.toml \"pyproject.toml\")\n\n[pyproject.toml](/yigitkonur/claude-cto/blob/main/pyproject.toml \"pyproject.toml\")\n\n[smithery.yaml](/yigitkonur/claude-cto/blob/main/smithery.yaml \"smithery.yaml\")\n\n[smithery.yaml](/yigitkonur/claude-cto/blob/main/smithery.yaml \"smithery.yaml\")\n\n[uv.toml](/yigitkonur/claude-cto/blob/main/uv.toml \"uv.toml\")\n\n[uv.toml](/yigitkonur/claude-cto/blob/main/uv.toml \"uv.toml\")\n\nView all files\n\n## Repository files navigation\n\n### üóø claude-cto üóø your ai coding agents' cto that gets shit done 10x faster\n\n[](#-claude-cto--your-ai-coding-agents-cto-that-gets-shit-done-10x-faster)\n\n**_your AI squad on crack: tasks run in parallel, smart waits handle dependent tasks, workflows fly on autopilot. ship code at ludicrous speed while making the vibe coding more enjoyable_**\n\n> wanna get some sleep but still not used your Opus limit? just queue more tasks before the 5 AM reset üêâ\n\n[![pypi](https://camo.githubusercontent.com/11a3faec66ef73ca07a4d2f8af95a4a642647d11dd71f6d34256e1fe849db006/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f636c617564652d63746f2e7376673f7374796c653d666c61742d73717561726526636f6c6f723d344438374536)](#) [![python](https://camo.githubusercontent.com/4808218212ece46251343f2523af0fdfb557a5bef0edc616a82c5480e8cd2da2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![mcp compatible](https://camo.githubusercontent.com/68d12595054e63a3777151d2e218f910b3ab5045feff032eeb85ee118734eef5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d43502d436f6d70617469626c652d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![no api key needed](https://camo.githubusercontent.com/a658d32902b2132efd85cfe7796c23a6a1c06ea6decd4c82c656e00a4f5909f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29c855f6e6f5f6170695f6b65795f6e65656465642d757365735f796f75725f636c617564655f7375622d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a658d32902b2132efd85cfe7796c23a6a1c06ea6decd4c82c656e00a4f5909f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29c855f6e6f5f6170695f6b65795f6e65656465642d757365735f796f75725f636c617564655f7375622d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![zero config](https://camo.githubusercontent.com/7f979b7e805e3bb0dee366b4413074bf07ca4d67e1119b22a67067bfcac94155/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d706c75675f265f706c61792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/7f979b7e805e3bb0dee366b4413074bf07ca4d67e1119b22a67067bfcac94155/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d706c75675f265f706c61792d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-cooking-in-60-seconds) ‚Ä¢ [**üéØ Real Example**](#-real-case-building-a-calorie-counter-in-minutes-wclaude-cto) ‚Ä¢ [**üíª CLI Dashboard**](#-your-mission-control-the-cli-dashboard) ‚Ä¢ [**üõ†Ô∏è REST API**](#%EF%B8%8F-rest-api-your-integration-playground) ‚Ä¢ [**üê≥ Docker**](#-deployment-options) ‚Ä¢ [**‚ú® Features**](#-feature-breakdown-the-tech-sauce)\n\n* * *\n\n**`claude-cto`** is your AI dream team on steroids. stop doing that one-task-at-a-time workflow ‚Äî now you've got a whole squad of ai devs (opus for the heavy stuff, sonnet for mid-tier, haiku for the easy wins) cracking code simultaneously while you sip your coffee ‚òï\n\n### ‚ö°\n\n[](#)\n\n**10x Faster**  \nParallel execution\n\n### üß†\n\n[](#-1)\n\n**Smart AF**  \nDependency resolution\n\n### üî•\n\n[](#-2)\n\n**Never Fails**  \nAuto-retries & circuit breakers\n\nhow it slaps:\n\n-   you're the big-picture boss\n-   claude-cto handles the boring pm work\n-   subtle notifications keep you in the loop without killing your vibe\n\n* * *\n\n## üí• why claude-cto claps traditional workflows\n\n[](#-why-claude-cto-claps-traditional-workflows)\n\nwe've hacked the claude code sdk with that spicy `--dangerously-skip-permissions` flag to make your ai go brrrrr:\n\n1.  cooks up a bulletproof game plan\n2.  delegates like a machine\n3.  handles task dependencies so smooth you'll think it's cheating\n\n**‚ö° Performance Comparison: See the Difference**\n\n### claude-cto: advantage of parallel execution\n\n[](#claude-cto-advantage-of-parallel-execution)\n\ngraph LR\n    Start\\[üéØ kickoff\\]:::start\n    \n    subgraph SG1\\[\"STAGE 1<br/>‚è∞ 4m (auth\\_system)\"\\]\n        S1A\\[setup\\_database<br/>3m\\]:::stage1\n        S1B\\[auth\\_system<br/>4m\\]:::stage1\n    end\n    \n    subgraph SG2\\[\"STAGE 2<br/>‚è∞ 5m (api\\_endpoints)\"\\]\n        S2A\\[api\\_endpoints<br/>5m\\]:::stage2\n        S2B\\[payment\\_integration<br/>3m\\]:::stage2\n        S2C\\[frontend\\_app<br/>4m\\]:::stage2\n    end\n    \n    subgraph SG3\\[\"STAGE 3<br/>‚è∞ 2m (deploy\\_production)\"\\]\n        S3A\\[deploy\\_production<br/>2m\\]:::stage3\n    end\n    \n    Final\\[üéâ launched\\]:::final\n    Total\\[‚è±Ô∏è 11m total<br/>4m + 5m + 2m\\]:::timeFast\n    \n    Start ==> S1A\n    Start ==> S1B\n    S1A ==> S2A\n    S1B ==> S2A\n    S1A ==> S2B\n    S1B ==> S2B\n    S1A ==> S2C\n    S1B ==> S2C\n    S2A ==> S3A\n    S2B ==> S3A\n    S2C ==> S3A\n    S3A ==> Final\n    \n    classDef start fill:#7950f2,stroke:#6741d9,stroke-width:3px,color:#fff\n    classDef stage1 fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#000\n    classDef stage2 fill:#4dabf7,stroke:#339af0,stroke-width:2px,color:#fff\n    classDef stage3 fill:#51cf66,stroke:#2f9e44,stroke-width:2px,color:#fff\n    classDef final fill:#ff6b6b,stroke:#f03e3e,stroke-width:3px,color:#fff\n    classDef timeFast fill:#20c997,stroke:#12b886,stroke-width:2px,color:#fff\n    \n    linkStyle default stroke:#51cf66,stroke-width:3px\n\nLoading\n\n### classic claude code approach: sequential execution\n\n[](#classic-claude-code-approach-sequential-execution)\n\ngraph LR\n    Start\\[üéØ kickoff\\]:::start\n    S1\\[setup\\_database<br/>3m\\]:::stage1\n    S2\\[auth\\_system<br/>4m\\]:::stage1\n    S3\\[api\\_endpoints<br/>5m\\]:::stage2\n    S4\\[payment\\_integration<br/>3m\\]:::stage2\n    S5\\[frontend\\_app<br/>4m\\]:::stage2\n    S6\\[deploy\\_production<br/>2m\\]:::stage3\n    Final\\[üéâ launched\\]:::final\n    \n    Start ==> S1 ==> S2 ==> S3 ==> S4 ==> S5 ==> S6 ==> Final\n    \n    Total\\[‚è±Ô∏è 21m total<br/>Sequential execution\\]:::timeSlow\n    \n    classDef start fill:#7950f2,stroke:#6741d9,stroke-width:3px,color:#fff\n    classDef stage1 fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#000\n    classDef stage2 fill:#4dabf7,stroke:#339af0,stroke-width:2px,color:#fff\n    classDef stage3 fill:#51cf66,stroke:#2f9e44,stroke-width:2px,color:#fff\n    classDef final fill:#ff6b6b,stroke:#f03e3e,stroke-width:3px,color:#fff\n    classDef timeSlow fill:#ffa94d,stroke:#fd7e14,stroke-width:2px,color:#000\n    \n    linkStyle default stroke:#51cf66,stroke-width:3px\n\nLoading\n\n**‚ùå Old Way**  \nSequential = Slow\n\n**‚úÖ With claude-cto**  \nParallel = Fast AF\n\n```\n21 minutes of pain\ntask1 ‚Üí task2 ‚Üí task3 ‚Üí task4\n```\n\n```\n11 minutes of glory\ntask1 ‚ü∂\ntask2 ‚ü∂ } ‚Üí task4\ntask3 ‚ü∂\n```\n\n* * *\n\n## üß† let your AI be the CTO - the real power move\n\n[](#-let-your-ai-be-the-cto---the-real-power-move)\n\nthis is where shit gets wild. instead of micromanaging like some middle manager, you let claude wear the cto hat. drop a high-level goal and watch it use `claude-cto`'s toolkit to plan, delegate, and execute like a boss.\n\n### the mcp tool belt üß∞\n\n[](#the-mcp-tool-belt-)\n\n**üìö Available MCP Tools (click to expand)**\n\nonce installed, claude gets these shiny new toys to whip your ai team into shape:\n\ntool\n\nwhat it does\n\n**`create_task`**  \nüöÄ\n\n**delegate like a pro**: assigns jobs to ai workers with optional dependencies\n\n**`submit_orchestration`**  \nüèÅ\n\n**the big red button**: launches entire task groups in one go\n\n**`get_task_status`**  \nüìä\n\n**spy mode**: checks up on a single worker's progress\n\n**`list_tasks`**  \nüìã\n\n**mission control**: shows what the whole squad's cooking\n\n**`clear_tasks`**  \nüßπ\n\n**spring cleaning**: wipes all completed & failed tasks in one sweep\n\n**`delete_task`**  \nüóëÔ∏è\n\n**surgical removal**: deletes a single non-running task by id\n\n**`check_api_health`**  \n‚ù§Ô∏è\n\n**pulse check**: makes sure the engine's purring\n\n* * *\n\n# üì∏ real case: building a calorie counter in minutes w/claude-cto\n\n[](#-real-case-building-a-calorie-counter-in-minutes-wclaude-cto)\n\nwatch how claude architects a photo-to-calories app like a 10x engineer on red bull.\n\n## you drop the mic:\n\n[](#you-drop-the-mic)\n\n> \"yo, need a mobile app that identifies food from photos and tracks calories. full AI integration, slick UI, the works. make it happen.\"\n\n## claude's game plan (internal monologue):\n\n[](#claudes-game-plan-internal-monologue)\n\n**üß† Claude's Master Plan (click to see the strategy)**\n\n> \"aight bet. this is a classic waterfall-but-make-it-parallel situation:\n> \n> 1.  **phase 1 (laying groundwork)**: app scaffold + database + AI research all go brrr at once\n> 2.  **phase 2 (the magic)**: camera + food recognition + nutrition math run parallel where deps allow\n> 3.  **phase 3 (make it pretty)**: UI + manual entry + analytics dashboard spawn when ready\n> 4.  **phase 4 (ship it)**: tests then optimization - can't polish what ain't built\n> \n> tagging this whole circus as `calorie_counter` crew.\"\n\ngraph TD\n    subgraph \"üå± Phase 1: Foundation (parallel)\"\n        A\\[üì± setup\\_mobile\\_app\\]:::phase1\n        B\\[üóÑÔ∏è design\\_database\\]:::phase1\n        C\\[üîç research\\_ai\\_apis\\]:::phase1\n    end\n    subgraph \"üèóÔ∏è Phase 2: Core Features (parallel)\"\n        D\\[üì∑ camera\\_module\\]:::phase2\n        E\\[ü§ñ food\\_recognition\\]:::phase2\n        F\\[üßÆ nutrition\\_engine\\]:::phase2\n    end\n    subgraph \"‚ú® Phase 3: User Experience (parallel)\"\n        G\\[üé® ui\\_screens\\]:::phase3\n        H\\[‚úèÔ∏è manual\\_entry\\]:::phase3\n        I\\[üìä analytics\\_dashboard\\]:::phase3\n    end\n    subgraph \"üöÄ Phase 4: Launch Ready\"\n        J\\[üß™ testing\\_suite\\]:::phase4\n        K\\[‚ö° app\\_optimization\\]:::phase4\n    end\n    A --> D\n    A --> G\n    B --> F\n    B --> I\n    C --> E\n    D --> E\n    E --> F\n    F --> G\n    F --> H\n    G --> J\n    H --> J\n    I --> J\n    J --> K\n    classDef phase1 fill:#FF6B6B,stroke:#C92A2A,color:#fff\n    classDef phase2 fill:#4ECDC4,stroke:#15AAA0,color:#fff\n    classDef phase3 fill:#45B7D1,stroke:#2196F3,color:#fff\n    classDef phase4 fill:#96CEB4,stroke:#4CAF50,color:#fff\n\nLoading\n\n## claude starts delegating (`create_task` calls):\n\n[](#claude-starts-delegating-create_task-calls)\n\n**üì± Phase 1: Foundation Setup (parallel ops)**\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"foundation\\_setup\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task A: React Native TypeScript Initialize ‚Üí \\[1\\]Run npx react-native init SnapCalories --template react-native-template-typescript, create /src with subdirs: /screens, /components, /services, /db, /utils, /store, /navigation, /types ‚Üí \\[2\\]Install core: yarn add @react-navigation/native @react-navigation/bottom-tabs @reduxjs/toolkit react-redux redux-persist @react-native-async-storage/async-storage react-native-screens react-native-safe-area-context ‚Üí \\[3\\]Configure tsconfig.json with \"strict\": true, paths: {\"@screens/\\*\": \\[\"src/screens/\\*\"\\], \"@components/\\*\": \\[\"src/components/\\*\"\\]}, setup .prettierrc with singleQuote, no semicolons ‚Üí (Review: yarn start launches Metro, TypeScript compiles without errors|Retest: Import @screens/Home works|Fail‚Üí\\[2\\])\n    Task B: Navigation Redux Store Setup ‚Üí \\[1\\]Create /src/navigation/AppNavigator.tsx with createBottomTabNavigator containing 5 tabs: HomeScreen, SearchScreen, CameraScreen, HistoryScreen, ProfileScreen with icons from react-native-vector-icons/Ionicons ‚Üí \\[2\\]Setup Redux in /src/store/index.ts: configureStore with userSlice (name, goals, preferences), mealsSlice (recent, favorites), persistConfig whitelist: \\['user', 'preferences'\\] ‚Üí \\[3\\]Wrap App.tsx with Provider and PersistGate, create placeholder screens that display their name, verify tab navigation works ‚Üí (Review: All 5 tabs navigate correctly, Redux DevTools shows state|Retest: Kill app, reopen, user preferences persist|Fail‚Üí\\[2\\]) \\[Req: Task A\\]\n    \"\"\",\n    model\\=\"sonnet\"\n)\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"database\\_schema\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task C: SQLite Database Schema Implementation ‚Üí \\[1\\]Install react-native-sqlite-storage, create /src/db/schema.ts with tables: users(id TEXT PRIMARY KEY, email TEXT UNIQUE, goals\\_json TEXT, created\\_at INTEGER), meals(id TEXT, user\\_id TEXT, type TEXT, logged\\_at INTEGER, total\\_calories REAL, photo\\_path TEXT) ‚Üí \\[2\\]Add food\\_items(id TEXT, meal\\_id TEXT, name TEXT, quantity REAL, unit TEXT, calories REAL, protein REAL, carbs REAL, fat REAL), food\\_database(id TEXT, name TEXT, brand TEXT, barcode TEXT UNIQUE, calories REAL, protein REAL, carbs REAL, fat REAL) ‚Üí \\[3\\]Create DatabaseService class with init(), executeSql(), methods for createTables(), dropTables(), verify tables exist with SELECT name FROM sqlite\\_master ‚Üí (Review: All 4 tables created successfully|Retest: Insert and retrieve test meal|Fail‚Üí\\[1\\]) \\[Req: Task A\\]\n    Task D: Database Seed Indexes Migrations ‚Üí \\[1\\]Create /src/db/seeds/foods.json with 5000 USDA foods: each having name, calories, protein, carbs, fat, serving\\_size, import with transaction INSERT OR IGNORE ‚Üí \\[2\\]Add indexes: CREATE INDEX idx\\_meals\\_date ON meals(user\\_id, logged\\_at DESC); CREATE INDEX idx\\_food\\_name ON food\\_database(name); CREATE INDEX idx\\_barcode ON food\\_database(barcode) ‚Üí \\[3\\]Implement migration system: migrations table tracking version, up/down functions, test by adding test column then rolling back ‚Üí (Review: SELECT \\* FROM food\\_database WHERE name LIKE '%chicken%' returns in <50ms|Retest: 1000 meal inserts complete <2s|Fail‚Üí\\[2\\]) \\[Req: Task C\\]\n    Task E: API Keys Service Configuration ‚Üí \\[1\\]Setup Clarifai: create account at clarifai.com, get API key, install @clarifai/nodejs-grpc, create /src/services/ClarifaiService.ts with class containing apiKey from env ‚Üí \\[2\\]Setup OpenFoodFacts: no key needed, install node-fetch, create /src/services/BarcodeService.ts with lookupBarcode(code) method calling https://world.openfoodfacts.org/api/v0/product/{code}.json ‚Üí \\[3\\]Create unified FoodAPIService that wraps both, with methods recognizeImage(base64) and scanBarcode(code), add mock mode for testing without API calls ‚Üí (Review: Mock mode returns fake data, API mode requires keys|Retest: Invalid API key throws clear error|Fail‚Üí\\[1\\]) \\[Req: Task D\\]\n    \"\"\",\n    depends\\_on\\=\\[\"foundation\\_setup\"\\],\n    model\\=\"sonnet\"\n)\n\n**üèóÔ∏è Phase 2: Core Features (needs phase 1 stuff)**\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"camera\\_recognition\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task F: Camera Module Photo Capture ‚Üí \\[1\\]Install react-native-vision-camera, add iOS Info.plist camera usage description, Android manifest CAMERA permission, create /src/components/Camera/CameraView.tsx with useCameraDevices() hook ‚Üí \\[2\\]Implement capture: const photo = await camera.current.takePhoto({qualityPrioritization: 'balanced'}), resize with react-native-image-resizer to max 1024px maintaining aspect, convert to base64 ‚Üí \\[3\\]Add controls: TouchableOpacity capture button 70x70 center bottom, flash toggle iconButton top-left, flip camera icon top-right, show captured photo preview for 2s ‚Üí (Review: Photo captures and shows preview|Retest: Deny permission shows instructions|Fail‚Üí\\[1\\]) \\[Req: Task B\\]\n    Task G: Food Recognition AI Integration ‚Üí \\[1\\]Create /src/services/FoodRecognition.ts with recognizeFood(imageBase64) using Clarifai: stub.PostModelOutputs with model\\_id: 'food-item-recognition', filter outputs.data.concepts where value > 0.7 ‚Üí \\[2\\]Process response: map concepts to {name: string, confidence: number}, enrich each with nutrition from local DB: SELECT calories, protein FROM food\\_database WHERE name LIKE concept ‚Üí \\[3\\]Add caching: MD5 hash image, store results in AsyncStorage for 24h with key food\\_recognition\\_{hash}, check cache before API call ‚Üí (Review: Test burger.jpg returns 'burger' with confidence >0.7|Retest: Same image uses cache, no API call|Fail‚Üí\\[1\\]) \\[Req: Task F, Task G\\]\n    Task H: Nutrition Calculator Engine Service ‚Üí \\[1\\]Create /src/services/NutritionEngine.ts with calculateCalories(protein\\_g, carbs\\_g, fat\\_g): return protein\\*4 + carbs\\*4 + fat\\*9, calculateBMR(weight\\_kg, height\\_cm, age, isMale) using Mifflin-St Jeor ‚Üí \\[2\\]Add meal tracking: saveMeal(userId, foods\\[\\], mealType) inserts into meals table, then food\\_items for each food, updates daily\\_summaries with trigger ‚Üí \\[3\\]Create getDailySummary(userId, date) aggregating: SELECT SUM(calories), SUM(protein) FROM meals JOIN food\\_items WHERE DATE(logged\\_at) = date ‚Üí (Review: Calculate 25g protein, 30g carbs, 10g fat = 350 calories|Retest: Save meal and verify in daily summary|Fail‚Üí\\[2\\]) \\[Req: Task E\\]\n    \"\"\",\n    depends\\_on\\=\\[\"database\\_schema\"\\],\n    model\\=\"opus\"\n)\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"advanced\\_features\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task J: Barcode Scanner Food Lookup ‚Üí \\[1\\]Install react-native-camera-kit for barcode scanning, create BarcodeScannerScreen with <CameraKitCameraScreen scanBarcode={true} onReadCode={(event) => handleBarcode(event.nativeEvent.codeStringValue)} /> ‚Üí \\[2\\]On scan: call OpenFoodFacts API, parse response.product.nutriments for calories\\_100g, proteins\\_100g, carbohydrates\\_100g, fat\\_100g, convert to per serving ‚Üí \\[3\\]If found: show product name, image\\_url, nutrition facts in modal with \"Add to meal\" button, if not found: prompt manual entry with barcode pre-filled ‚Üí (Review: Scan Coke barcode shows 42cal/100ml|Retest: Unknown barcode opens manual entry|Fail‚Üí\\[1\\]) \\[Req: Task F\\]\n    Task K: Manual Food Entry Search ‚Üí \\[1\\]Create SearchFoodScreen with TextInput, implement fuzzy search using Fuse.js on food\\_database with keys: \\['name', 'brand'\\], threshold: 0.3, show results in FlatList ‚Üí \\[2\\]Each result item shows: food name, brand, calories per serving, (+) button to add, implement recent searches in AsyncStorage (last 10), show below search bar ‚Üí \\[3\\]Add filters: meal type (breakfast/lunch/dinner/snack), calorie range slider 0-1000, dietary tags (vegan, gluten-free), sort by: relevance/calories/name ‚Üí (Review: Search \"chken\" returns \"chicken\" results|Retest: Filter vegan excludes meat|Fail‚Üí\\[1\\]) \\[Req: Task E\\]\n    \"\"\",\n    depends\\_on\\=\\[\"camera\\_recognition\"\\],\n    model\\=\"sonnet\"\n)\n\n**‚ú® Phase 3: User Experience (make it sexy)**\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"core\\_screens\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task M: Home Dashboard Screen UI ‚Üí \\[1\\]Create HomeScreen with header showing date picker (default today), circular progress ring using react-native-svg showing calories (current/goal) with animated fill on mount ‚Üí \\[2\\]Add macro bars: horizontal stacked bar with protein(red) carbs(blue) fat(yellow) showing grams and percentages, below add water tracker with 8 glasses icons filling on tap ‚Üí \\[3\\]Recent meals section: FlatList showing last 3 meals with thumbnail, name, calories, time ago using date-fns, swipe left to delete with confirmation ‚Üí (Review: Progress ring animates to 1200/2000 calories|Retest: Delete meal updates totals immediately|Fail‚Üí\\[1\\]) \\[Req: Task I\\]\n    Task N: History Calendar Food Diary ‚Üí \\[1\\]Create HistoryScreen with react-native-calendars Calendar component, mark dates with meals using markedDates prop: green=under goal, red=over goal, yellow=at goal ‚Üí \\[2\\]On date tap: show modal with that day's meals grouped by type (Breakfast/Lunch/Dinner/Snacks), each meal shows foods list, total calories, edit button ‚Üí \\[3\\]Add week view: ScrollView with 7 cards showing daily totals, mini macro pie chart using victory-native, tap to expand day details ‚Üí (Review: Calendar shows last 30 days colored correctly|Retest: Edit past meal updates that day's color|Fail‚Üí\\[2\\]) \\[Req: Task M\\]\n    \"\"\",\n    depends\\_on\\=\\[\"advanced\\_features\"\\],\n    model\\=\"sonnet\"\n)\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"meal\\_flow\\_screens\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task P: Meal Detail Edit Screen ‚Üí \\[1\\]Create MealDetailScreen receiving meal from navigation, show hero image at top with parallax scroll effect, list recognized foods with confidence badges (green >80%, yellow >60%, red <60%) ‚Üí \\[2\\]Each food item: editable name TextInput, quantity with +/- buttons, unit dropdown, calories auto-updating based on quantity, swipe to delete, \"Not right?\" button to search alternatives ‚Üí \\[3\\]Bottom section: add more foods button opening search modal, nutrition totals updating real-time, save button with loading state, share button generating image with meal photo and macros ‚Üí (Review: Changing quantity from 100g to 150g updates calories by 1.5x|Retest: Delete item updates totals|Fail‚Üí\\[2\\]) \\[Req: Task H\\]\n    \"\"\",\n    depends\\_on\\=\\[\"core\\_screens\"\\],\n    model\\=\"sonnet\"\n)\n\n**üöÄ Phase 4: Ship It (final boss)**\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"testing\\_suite\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task S: Unit Integration Test Suite ‚Üí \\[1\\]Setup Jest with React Native Testing Library, create \\_\\_tests\\_\\_ folders, write unit tests: NutritionEngine.test.ts testing calculateCalories(25,30,10)===350, macro validation, BMR calculation ‚Üí \\[2\\]Integration tests: photo-to-meal flow mocking camera and API, test saves correctly to DB, offline queue when no network, sync when reconnected ‚Üí \\[3\\]Component tests: render all screens without crashing, test navigation between tabs, Redux actions update store correctly, AsyncStorage persists ‚Üí (Review: All tests pass with >70% coverage|Retest: Run on CI, all green|Fail‚Üí\\[1\\]) \\[Req: Task R\\]\n    \"\"\",\n    depends\\_on\\=\\[\"meal\\_flow\\_screens\"\\],\n    model\\=\"sonnet\"\n)\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"production\\_deploy\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task V: Security Privacy Compliance Setup ‚Üí \\[1\\]Implement API key encryption using react-native-keychain, add certificate pinning for API calls, obfuscate sensitive strings with react-native-obfuscating-transformer ‚Üí \\[2\\]Privacy: add GDPR consent screen for EU users, data deletion option in settings, export user data as JSON, analytics opt-out toggle, clear privacy policy link ‚Üí \\[3\\]Security audit: no hardcoded secrets in code, all user data encrypted at rest, HTTPS only, SQL injection prevention with parameterized queries, input validation on all forms ‚Üí (Review: Security scanner finds no vulnerabilities|Retest: Man-in-middle attack fails|Fail‚Üí\\[1\\]) \\[Req: Task U\n    Task W: Production Build Release Prep ‚Üí \\[1\\]iOS: generate certificates in Apple Developer, configure Xcode with provisioning profiles, set bundle ID com.snapcalories.app, archive and validate with App Store Connect ‚Üí \\[2\\]Android: generate signed keystore, configure gradle with release signing, enable minification and R8, build AAB format for Play Store, test on multiple devices ‚Üí \\[3\\]Create store listings: write descriptions emphasizing AI food recognition, take 5 screenshots per platform, app icon 1024x1024, privacy policy URL, age rating 4+ ‚Üí (Review: Both builds install and run on real devices|Retest: Upload to TestFlight/Internal testing works|Fail‚Üí\\[2\\]) \\[Req: Task V\\]\n    \"\"\",\n    depends\\_on\\=\\[\"testing\\_suite\"\\],\n    model\\=\"opus\"\n)\n\n\\# Claude's Tool Call:\ncreate\\_task(\n    task\\_identifier\\=\"launch\\_features\",\n    orchestration\\_group\\=\"calorie\\_counter\",\n    execution\\_prompt\\=\"\"\"\n    Task Y: Beta Testing Feedback Loop ‚Üí \\[1\\]Setup TestFlight for 100 iOS beta testers, Google Play Internal Testing for Android, create feedback form in-app with screenshot capability, discord/slack community for testers ‚Üí \\[2\\]Track metrics: daily active users, meal logging rate, photo vs manual entry ratio, feature usage heatmap, crash reports, average session time, user retention day 1/7/30 ‚Üí \\[3\\]Iterate based on feedback: fix top 3 crashes, improve food recognition accuracy on reported failures, add most requested foods to database, optimize slow screens ‚Üí (Review: 50+ beta testers active, <0.5% crash rate|Retest: User feedback form submits successfully|Fail‚Üí\\[1\\]) \\[Req: Task X\\]\n    \"\"\",\n    depends\\_on\\=\\[\"production\\_deploy\", \"launch\\_features\"\\],\n    model\\=\"sonnet\"\n)\n\n## claude hits go (`submit_orchestration`):\n\n[](#claude-hits-go-submit_orchestration)\n\nTip\n\n\"locked and loaded. spinning up the `calorie_counter` squad. grab a coffee, this is gonna be lit.\"\n\nsubmit\\_orchestration(orchestration\\_group\\=\"calorie\\_counter\")\n\n**boom:** claude just architected your entire app while you were doom-scrolling twitter. 11 parallel claude instances working in perfect harmony. `claude-cto` handles all the dependency management and handoffs automagically.\n\n**endgame:** production-ready app with AI food recognition, butter-smooth UI, offline support, and 85% test coverage. shipped before lunch. üöÄ\n\n* * *\n\n## üíª your mission control: the cli dashboard\n\n[](#-your-mission-control-the-cli-dashboard)\n\nwhile your AI cto's running the show, the CLI is your ops center - monitor progress, debug issues, or drop manual tasks like a boss.\n\n[![claude cto cli go brrr](https://camo.githubusercontent.com/9c276d769d6831bb3809673b541e4b016b697acb126f4a3f82d849f13acf7f3c/68747470733a2f2f692e696d6775722e636f6d2f796f75722d636c692d696e2d616374696f6e2e676966)](https://camo.githubusercontent.com/9c276d769d6831bb3809673b541e4b016b697acb126f4a3f82d849f13acf7f3c/68747470733a2f2f692e696d6775722e636f6d2f796f75722d636c692d696e2d616374696f6e2e676966)\n\n### real-time ops: how you'd run this circus\n\n[](#real-time-ops-how-youd-run-this-circus)\n\n**üéÆ CLI Commands Cheat Sheet**\n\nsay your ai just launched that \"ecommerce\\_modernization\" plan. here's how you stay in the loop.\n\n**step 1: get the big picture üó∫Ô∏è**  \nfirst, scope out what plans are cooking:\n\n$ claude-cto list-orchestrations\n\n  id   status     tasks   completed   failed   created\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  1    running    5       2           0        2025-08-21T10:30:00\n\n> \"cool, plan #1's live with 2/5 tasks already crushed.\"\n\n**step 2: watch the magic happen üçø**  \npeek behind the curtain with live updates:\n\n$ claude-cto orchestration-status 1 --watch\n\nnow your terminal's a live feed showing tasks flipping from `waiting` ‚Üí `running` ‚Üí `completed`.\n\n**step 3: stalk a specific worker üìù**  \npayments refactor taking too long? let's investigate:\n\n$ claude-cto list\n\n  id   status     last action                       logs\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  1    completed  wrote security report             task\\_1\\_...\n  2    completed  wrote perf report                 task\\_2\\_...\n  3    running    editing auth/session.js           task\\_3\\_...\n  4    running    analyzing stripe\\_api.py           task\\_4\\_...\n  5    waiting    -                                 task\\_5\\_...\n\nnow tail the payments worker (id 4):\n\n# get the tl;dr\n$ tail -f ~/.claude-cto/tasks/task\\_4\\_\\*\\_summary.log\n\n# or go full detective mode\n$ less ~/.claude-cto/tasks/task\\_4\\_\\*\\_detailed.log\n\n**step 4: drop ad-hoc tasks üïπÔ∏è**  \nforgot something? no sweat - add tasks on the fly:\n\n# fire-and-forget\n$ claude-cto run \"slap a 'modernization in progress' banner in readme.md\"\n\n# watch it live\n$ claude-cto run \"whip up a db seed script\" --watch\n\n**step 5: launch pre-cooked workflows**  \ngot a json blueprint? be the cto:\n\n# deploy a full ci/cd pipeline\n$ claude-cto orchestrate cicd-pipeline.json --wait\n\npro tip: mix these commands like a devops bartender. your ai team's always on call. üç∏\n\n* * *\n\n## üì¶ get cooking in 60 seconds\n\n[](#-get-cooking-in-60-seconds)\n\n### üöÄ Quick Start (Platform-Optimized)\n\n[](#-quick-start-platform-optimized)\n\n#### üçé macOS: Homebrew (Recommended)\n\n[](#-macos-homebrew-recommended)\n\n# Install via Homebrew (includes auto-MCP configuration)\nbrew tap yigitkonur/claude-cto\nbrew install claude-cto\n\n# Start the server (MCP auto-configures on first use)\nclaude-cto server start\n\n#### ü™ü Windows: pip/uv (Recommended)\n\n[](#-windows-pipuv-recommended)\n\n# Install with pip (includes auto-MCP configuration)\npip install \"claude-cto\\[full\\]\"\n\n# Start the server (MCP auto-configures on first use)\nclaude-cto server start\n\n#### üêß Linux: pip/uv or Homebrew\n\n[](#-linux-pipuv-or-homebrew)\n\n# Option A: pip/uv (Universal, works everywhere)\npip install \"claude-cto\\[full\\]\"\n\n# Option B: Homebrew (if you prefer package managers)\nbrew tap yigitkonur/claude-cto && brew install claude-cto\n\n# Start the server (MCP auto-configures on first use)\nclaude-cto server start\n\n> **‚ú® Auto-Configuration:** All installation methods automatically configure claude-cto as an MCP server for Claude Code on first use. No manual setup required!\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   **Python** (v3.10+) & **Node.js** (v16+)\n-   **Claude Code SDK**: `npm install -g @anthropic-ai/claude-code`\n-   **Authentication**: `claude auth login` - no API key needed with Claude subscription\n\n### Installation Methods\n\n[](#installation-methods)\n\n> **üéØ Platform-Specific Recommendations:**\n> \n> -   **üçé macOS:** Homebrew is the easiest (zero-config setup, native integration)\n> -   **ü™ü Windows:** pip/uv recommended (Claude Code works via WSL/native, no Homebrew needed)\n> -   **üêß Linux:** Both pip/uv and Homebrew work great - choose your preference\n> -   **‚ö° Speed demons:** UV is 10-100x faster than pip on all platforms\n> -   **üß† MCP-only:** Use Smithery for Claude Code integration without CLI/server\n\nMethod\n\nCommand\n\nBest For\n\n**üç∫ Homebrew**  \nRecommended\n\n`brew tap yigitkonur/claude-cto   brew install claude-cto`\n\nZero-config setup + auto-MCP\n\n**‚ö° UV**  \nblazing fast\n\n`uv pip install \"claude-cto[full]\"`\n\nModern Python workflows\n\n**‚ú® Smithery**  \nMCP only\n\n`npx -y @smithery/cli install @yigitkonur/claude-cto`\n\nClaude Desktop/VSCode users\n\n**üëë Full Monty**\n\n`pip install \"claude-cto[full]\"`\n\nCLI + API + MCP + auto-config\n\n**üß† MCP Only**\n\n`pip install \"claude-cto[mcp]\"`\n\nJust the Claude sauce\n\n**üñ•Ô∏è CLI/Server**\n\n`pip install \"claude-cto[server]\"`\n\nTerminal power user\n\n**üê≥ Docker**\n\n`docker run yigitkonur35/claude-cto`\n\nZero dependencies\n\n### 3\\. installation deep dive\n\n[](#3-installation-deep-dive)\n\n**‚ö° UV Users (the fast lane)**\n\n# install uv if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# install with all extras\nuv pip install \"claude-cto\\[full\\]\"\n\n# or for a project setup with lock file\nuv init my-automation\ncd my-automation\nuv add \"claude-cto\\[full\\]\"\nuv sync  # creates deterministic uv.lock\n\nUV is Rust-powered and **10-100x faster** than pip. It also handles virtual environments automatically and creates lock files for reproducible installs.\n\n### 4\\. Installation Notes & Cross-Platform Compatibility\n\n[](#4-installation-notes--cross-platform-compatibility)\n\n**üçé macOS Users:**\n\n-   **Homebrew (Recommended):** Includes wrapper script with zero-config MCP setup\n-   **pip/uv:** Auto-configures on first command, works with any Python environment\n-   Both methods play nicely together (first-run-wins, no conflicts)\n\n**ü™ü Windows Users:**\n\n-   **pip/uv (Recommended):** Full native support with auto-MCP configuration\n-   Claude Code works via WSL or native Windows installation\n-   Auto-configures using correct Python interpreter (virtualenv, conda, pyenv compatible)\n-   Shows friendly setup message: \"üóø Setting up claude-cto MCP server...\"\n\n**üêß Linux Users:**\n\n-   **Both pip/uv and Homebrew:** Equally reliable options\n-   Choose based on your preference (package manager vs Python ecosystem)\n-   Auto-configures MCP server using system Claude CLI\n\n**‚ö° UV Users (All Platforms):**\n\n-   Rust-powered speed (10-100x faster than pip)\n-   Automatic virtual environment handling\n-   Lock file generation for reproducible builds\n-   Same auto-MCP configuration as pip\n\n**üîí Security & Safety:**\n\n-   Cross-platform subprocess handling with proper timeouts\n-   Graceful fallback if Claude CLI not available\n-   No conflicts between installation methods\n-   Safe path handling for Windows/Linux/macOS\n\n> **‚ú® Zero Manual Setup:** All installation methods automatically detect Claude Code and configure the MCP server using your correct Python environment. No `.claude.json` editing required!\n\n* * *\n\n## üõ†Ô∏è rest api: your integration playground\n\n[](#Ô∏è-rest-api-your-integration-playground)\n\nthe real magic happens through a slick local rest api - hook it into anything from ci/cd to custom tooling. this can be a thing where you can manage your claude code execution remotely, just built on top of this API.\n\n**üêç Python ETL Pipeline Example**\n\nimport httpx, time, json\n\nSERVER \\= \"http://localhost:8000\"\n\n\\# 1. define the whole shebang\netl\\_blueprint \\= {\n    \"tasks\": \\[\n        {\"id\": \"grab\\_sales\", \"prompt\": \"yoink sales data from postgres\"},\n        {\"id\": \"snag\\_inventory\", \"prompt\": \"pull inventory from mongodb\"},\n        {\n            \"id\": \"transform\", \n            \"prompt\": \"cleanup & merge datasets\",\n            \"needs\": \\[\"grab\\_sales\", \"snag\\_inventory\"\\],\n            \"delay\": 2.0  \\# chill for 2 secs\n        },\n        {\"id\": \"dump\\_to\\_warehouse\", \"prompt\": \"shove clean data into snowflake\", \"needs\": \\[\"transform\"\\]},\n    \\]\n}\n\n\\# 2. fire it off\nprint(\"üöÄ launching etl pipeline...\")\nr \\= httpx.post(f\"{SERVER}/api/v1/orchestrations\", json\\=etl\\_blueprint)\norch\\_id \\= r.json()\\[\"orchestration\\_id\"\\]\nprint(f\"‚úÖ pipeline #{orch\\_id} live!\")\n\n\\# 3. watch like a hawk\nwhile True:\n    status \\= httpx.get(f\"{SERVER}/api/v1/orchestrations/{orch\\_id}\").json()\n    \n    print(f\"status: {status\\['status'\\]} | progress: {status\\['completed\\_tasks'\\]}/{status\\['total\\_tasks'\\]}\")\n    if status\\['status'\\] in \\[\"done\", \"failed\", \"cancelled\"\\]:\n        print(\"\\\\nüéâ all done! final report:\")\n        print(json.dumps(status, indent\\=2))\n        break\n    time.sleep(5)  \\# don't spam the api\n\nthis api's your golden ticket - automate all the things without touching the cli. ü§ñ\n\n* * *\n\n## ‚ú® feature breakdown: the tech sauce\n\n[](#-feature-breakdown-the-tech-sauce)\n\nFeature\n\nWhat it does\n\nWhy you care\n\n**üöÄ Parallel**  \n`10x speed`\n\nRuns multiple AI agents simultaneously\n\n10 tasks finish in 5 mins, not 50\n\n**üîó Dependencies**  \n`DAG support`\n\nHandles task dependencies automatically\n\nComplex workflows just work\n\n**üîÑ Auto-retries**  \n`exponential backoff`\n\nSmart retries with circuit breakers\n\nNo manual restarts for flaky wifi\n\n**üß† Model picker**  \n`opus/sonnet/haiku`\n\nAssign models per task complexity\n\nSave $$$ using heavy models wisely\n\n**üìú Full logs**  \n`summary + detailed`\n\nTwo-tier logging for every task\n\nDebug like a pro with context\n\n**üìä Resource guard**  \n`CPU/memory/disk`\n\nMonitors system resources\n\nPrevents machine meltdown\n\n**üíæ Crash-proof**  \n`disk persistence`\n\nEverything saved to SQLite\n\nPower outage? Pick up where you left\n\n**üõ°Ô∏è Circuit breaker**  \n`failure protection`\n\nStops retrying broken components\n\nNo infinite failure loops\n\n* * *\n\n## üö¢ deployment options\n\n[](#-deployment-options)\n\n**üê≥ Docker (set it & forget it)**\n\n# quick start with docker\n# an api key isn't required, but you must manually run claude and complete auth.  \n# use \\`brew\\` or \\`pip install\\` for an easier setup.\ndocker run -d \\\\\n  --name claude-cto \\\\\n  -p 8000:8000 \\\\\n  -e ANTHROPIC\\_API\\_KEY=$ANTHROPIC\\_API\\_KEY \\\\\n  yigitkonur35/claude-cto:latest\n\n# or run CLI commands directly\ndocker run --rm \\\\\n  -e ANTHROPIC\\_API\\_KEY=$ANTHROPIC\\_API\\_KEY \\\\\n  yigitkonur35/claude-cto run \"analyze this codebase\"\n\n# or use docker-compose for full setup\ndocker-compose up -d\n\nfor detailed docker setup, multi-arch builds, and advanced configs, check out [DOCKER.md](/yigitkonur/claude-cto/blob/main/DOCKER.md).\n\n### docker-compose snippet\n\n[](#docker-compose-snippet)\n\ndrop this in `docker-compose.yml`:\n\nversion: '3.8'\nservices:\n  claude-cto:\n    image: yigitkonur35/claude-cto:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ANTHROPIC\\_API\\_KEY=${API\\_KEY}\n    volumes:\n      - ./claude\\_data:/root/.claude-cto # keeps your data safe\n    restart: unless-stopped\n\nfire it up: `docker-compose up -d`\n\n**üêß Systemd (for linux servers)**\n\n1.  create `/etc/systemd/system/claude-cto.service`\n2.  paste config (ask your friendly neighborhood sysadmin)\n3.  run: `sudo systemctl enable --now claude-cto`\n\n* * *\n\n## üîß config & fixes\n\n[](#-config--fixes)\n\n**‚öôÔ∏è Environment Variables**\n\nvariable\n\npurpose\n\ndefault\n\n`ANTHROPIC_API_KEY`\n\nyour claude api key\n\n**no need for claude max sub users** - optional for key-based usage\n\n`CLAUDE_CTO_SERVER_URL`\n\nwhere the cli connects\n\n`http://localhost:8000`\n\n`CLAUDE_CTO_DB`\n\ntask database location\n\n`~/.claude-cto/tasks.db`\n\n`CLAUDE_CTO_ENABLE_SOUNDS`\n\nping when tasks complete\n\n`true`\n\n`CLAUDE_CTO_MODE`\n\n`standalone` or `proxy`\n\n`auto`\n\n**üî• Common Issues Quickfix**\n\nproblem\n\nsolution\n\n**tasks stuck**\n\nrun `claude --version` to check auth\n\n**database locked**\n\n`pkill -f claude-cto && rm ~/.claude-cto/tasks.db-journal`\n\n**port 8000 taken**\n\n`lsof -i :8000` or let it auto-find ports\n\n**permission denied**\n\n`sudo chown -R $(whoami) ~/.claude-cto`\n\n* * *\n\n**Built with üî• by devs who got tired of watching AI work sequentially**\n\n## About\n\nFire-and-forget task execution system for Claude Code SDK with CLI and MCP interfaces\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/claude-cto/activity)\n\n### Stars\n\n[**9** stars](/yigitkonur/claude-cto/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/claude-cto/watchers)\n\n### Forks\n\n[**2** forks](/yigitkonur/claude-cto/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fclaude-cto&report=yigitkonur+%28user%29)\n\n## [Releases 36](/yigitkonur/claude-cto/releases)\n\n[\n\nRelease v0.20.0 Latest\n\nSep 3, 2025\n\n](/yigitkonur/claude-cto/releases/tag/v0.20.0)\n\n[\\+ 35 releases](/yigitkonur/claude-cto/releases)\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=claude-cto)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## [Contributors 3](/yigitkonur/claude-cto/graphs/contributors)\n\n-    [![@yigitkonur](https://avatars.githubusercontent.com/u/9989650?s=64&v=4)](https://github.com/yigitkonur)[**yigitkonur** Yigit Konur](https://github.com/yigitkonur)\n-    [![@github-actions[bot]](https://avatars.githubusercontent.com/in/15368?s=64&v=4)](https://github.com/apps/github-actions)[**github-actions\\[bot\\]**](https://github.com/apps/github-actions)\n-    [![@actions-user](https://avatars.githubusercontent.com/u/65916846?s=64&v=4)](https://github.com/actions-user)[**actions-user**](https://github.com/actions-user)\n\n## Languages\n\n-   [Python 98.2%](/yigitkonur/claude-cto/search?l=python)\n-   [Shell 1.3%](/yigitkonur/claude-cto/search?l=shell)\n-   [Dockerfile 0.5%](/yigitkonur/claude-cto/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/cctrace-js\n\nGitHub - yigitkonur/cctrace-js: A TypeScript/Node.js version of cctrace - Export Claude Code chat sessions with conversation history, internal reasoning blocks, tool usage, and comprehensive metadata                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcctrace-js)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcctrace-js)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fcctrace-js)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[cctrace-js](/yigitkonur/cctrace-js)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fcctrace-js) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fcctrace-js)\n-   [Star 2](/login?return_to=%2Fyigitkonur%2Fcctrace-js)\n    \n\nA TypeScript/Node.js version of cctrace - Export Claude Code chat sessions with conversation history, internal reasoning blocks, tool usage, and comprehensive metadata\n\n### License\n\n[MIT license](/yigitkonur/cctrace-js/blob/master/LICENSE)\n\n[2 stars](/yigitkonur/cctrace-js/stargazers) [0 forks](/yigitkonur/cctrace-js/forks) [Branches](/yigitkonur/cctrace-js/branches) [Tags](/yigitkonur/cctrace-js/tags) [Activity](/yigitkonur/cctrace-js/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fcctrace-js)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fcctrace-js) You must be signed in to change notification settings\n\n# yigitkonur/cctrace-js\n\n  \n\n¬†master\n\n[Branches](/yigitkonur/cctrace-js/branches)[Tags](/yigitkonur/cctrace-js/tags)\n\n[](/yigitkonur/cctrace-js/branches)[](/yigitkonur/cctrace-js/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[26 Commits](/yigitkonur/cctrace-js/commits/master/)\n\n[](/yigitkonur/cctrace-js/commits/master/)\n\n[.claude](/yigitkonur/cctrace-js/tree/master/.claude \".claude\")\n\n[.claude](/yigitkonur/cctrace-js/tree/master/.claude \".claude\")\n\n[.github/workflows](/yigitkonur/cctrace-js/tree/master/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/cctrace-js/tree/master/.github/workflows \"This path skips through empty directories\")\n\n[dist](/yigitkonur/cctrace-js/tree/master/dist \"dist\")\n\n[dist](/yigitkonur/cctrace-js/tree/master/dist \"dist\")\n\n[docs](/yigitkonur/cctrace-js/tree/master/docs \"docs\")\n\n[docs](/yigitkonur/cctrace-js/tree/master/docs \"docs\")\n\n[src](/yigitkonur/cctrace-js/tree/master/src \"src\")\n\n[src](/yigitkonur/cctrace-js/tree/master/src \"src\")\n\n[.gitignore](/yigitkonur/cctrace-js/blob/master/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/cctrace-js/blob/master/.gitignore \".gitignore\")\n\n[CHANGELOG.md](/yigitkonur/cctrace-js/blob/master/CHANGELOG.md \"CHANGELOG.md\")\n\n[CHANGELOG.md](/yigitkonur/cctrace-js/blob/master/CHANGELOG.md \"CHANGELOG.md\")\n\n[LICENSE](/yigitkonur/cctrace-js/blob/master/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/cctrace-js/blob/master/LICENSE \"LICENSE\")\n\n[NPM\\_PUBLISH\\_CHECKLIST.md](/yigitkonur/cctrace-js/blob/master/NPM_PUBLISH_CHECKLIST.md \"NPM_PUBLISH_CHECKLIST.md\")\n\n[NPM\\_PUBLISH\\_CHECKLIST.md](/yigitkonur/cctrace-js/blob/master/NPM_PUBLISH_CHECKLIST.md \"NPM_PUBLISH_CHECKLIST.md\")\n\n[README.md](/yigitkonur/cctrace-js/blob/master/README.md \"README.md\")\n\n[README.md](/yigitkonur/cctrace-js/blob/master/README.md \"README.md\")\n\n[package-lock.json](/yigitkonur/cctrace-js/blob/master/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/cctrace-js/blob/master/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/cctrace-js/blob/master/package.json \"package.json\")\n\n[package.json](/yigitkonur/cctrace-js/blob/master/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/cctrace-js/blob/master/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/cctrace-js/blob/master/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n# cctrace-js\n\n[](#cctrace-js)\n\n[![npm version](https://camo.githubusercontent.com/a2d7e78bd41f24e2e4c0abe2bdd3c941afa06e4818a288f2a04668aabe53638d/68747470733a2f2f62616467652e667572792e696f2f6a732f636374726163652d6a732e737667)](https://badge.fury.io/js/cctrace-js) [![TypeScript](https://camo.githubusercontent.com/4e0187e6b676f265f374875b52dd0b6ebffbe84e21172b03095772876af26b9c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d52656164792d626c75652e737667)](https://www.typescriptlang.org/) [![License: MIT](https://camo.githubusercontent.com/fdf2982b9f5d7489dcf44570e714e3a15fce6253e0cc6b5aa61a075aac2ff71b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667)](https://opensource.org/licenses/MIT)\n\nA command-line tool that exports Claude Code chat sessions with conversation history, internal reasoning blocks, tool usage, and comprehensive metadata in XML and Markdown formats.\n\n**This is a TypeScript/Node.js port of the excellent [cctrace](https://github.com/jimmc414/cctrace) tool originally created by [@jimmc414](https://github.com/jimmc414).**\n\n## üôè Acknowledgments & Motivation\n\n[](#-acknowledgments--motivation)\n\n**Huge respect and gratitude to [@jimmc414](https://github.com/jimmc414)** for creating the original [cctrace](https://github.com/jimmc414/cctrace) tool. Your work provided the foundation and inspiration for this TypeScript version.\n\n### Why this TypeScript version exists\n\n[](#why-this-typescript-version-exists)\n\nAs someone who builds extensively with Claude Code, I found myself constantly needing to combine conversation context effectively for:\n\n-   **LLM-friendly context building**: Creating comprehensive conversation exports that can be easily fed back into LLMs for continued development\n-   **Project documentation**: Maintaining detailed records of development decisions and reasoning\n-   **Context preservation**: Saving the complete thought process, including internal reasoning blocks, for future reference\n-   **Collaborative development**: Sharing detailed conversation history with team members and the Claude Code community\n\nThe original Python tool was fantastic, but I needed:\n\n-   **Library integration**: Programmatic access from Node.js/TypeScript projects\n-   **Enhanced performance**: Faster startup and processing for frequent use\n-   **Extended features**: Message truncation, better formatting options, and modern tooling integration\n-   **NPM ecosystem**: Easy installation and distribution through npm\n\nThis TypeScript version maintains 100% compatibility with the original while adding modern JavaScript ecosystem benefits.\n\n## ‚ú® Features\n\n[](#-features)\n\n-   **üöÄ Fast & Reliable**: Built with TypeScript for better performance and type safety\n-   **üéØ Automatic Session Detection**: Intelligently identifies your current Claude Code session, even with multiple concurrent sessions\n-   **üì¶ Complete Export**: Captures all messages, thinking blocks, tool uses, and responses\n-   **üîç PID Cross Reference Validation**: Cross-references process ID to ensure the correct session is exported\n-   **üìÑ Multiple Output Formats**: Generates Markdown, JSON, XML, and raw JSONL files\n-   **üìä Session Statistics**: Provides detailed metrics about your conversation\n-   **üåê Rich Programmatic API**: Comprehensive library with 10+ functions for session management\n-   **üß© Easy Integration**: Embed in React, Express.js, Webpack, Vite, Jest, and CI/CD pipelines\n-   **üìè Smart Message Truncation**: Control message length with `--max-message-length` for better LLM context management\n-   **üìÅ Auto-Copy to Working Directory**: Automatically copies export to your current directory (configurable)\n-   **‚ö° CLI & Library**: Use as a command-line tool or import as a library\n-   **üõ°Ô∏è Type-Safe**: Full TypeScript support with custom error classes and comprehensive type definitions\n-   **üöø Stdout Support**: Export directly to stdout for piping to other tools with `--stdout`\n\n[![Demo](https://private-user-images.githubusercontent.com/6346529/462078035-b316bd46-94f0-44ef-8030-e73b393cb119.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii82MzQ2NTI5LzQ2MjA3ODAzNS1iMzE2YmQ0Ni05NGYwLTQ0ZWYtODAzMC1lNzNiMzkzY2IxMTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjllYTMzMWRiZTBmZGUyYTc4YTQxOTkyYjkyMzRhODUzZjg3YzczMDU1MjQxOGUxNTRhMWEzNjY2ZTZlYzczNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Djs9vgB9oc7IQxjM0xzTTTDshSJiA3SiQUstZBnUSP0)](https://private-user-images.githubusercontent.com/6346529/462078035-b316bd46-94f0-44ef-8030-e73b393cb119.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii82MzQ2NTI5LzQ2MjA3ODAzNS1iMzE2YmQ0Ni05NGYwLTQ0ZWYtODAzMC1lNzNiMzkzY2IxMTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjllYTMzMWRiZTBmZGUyYTc4YTQxOTkyYjkyMzRhODUzZjg3YzczMDU1MjQxOGUxNTRhMWEzNjY2ZTZlYzczNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Djs9vgB9oc7IQxjM0xzTTTDshSJiA3SiQUstZBnUSP0)\n\n## üìã Requirements\n\n[](#-requirements)\n\n-   **Node.js** >= 16.0.0\n-   **Claude Code** running on macOS, Linux, or WSL\n-   Access to `~/.claude/projects/` directory\n\n## üöÄ Quick Start\n\n[](#-quick-start)\n\n### Installation\n\n[](#installation)\n\n# Install globally for CLI usage\nnpm install -g cctrace-js\n\n# Or install locally for your project\nnpm install cctrace-js\n\n# TypeScript projects (types included)\nnpm install cctrace-js\n# No need for @types/cctrace-js - types are built-in!\n\n### Command Line Usage\n\n[](#command-line-usage)\n\n# Export current active session\ncctrace\n\n# Export with specific format\ncctrace --format md\n\n# Export specific session by ID\ncctrace --session-id f33cdb42-0a41-40d4-91eb-c89c109af38a\n\n# Export with custom settings\ncctrace --max-age 600 --output-dir ./exports --no-copy-to-cwd\n\n# Export to stdout for piping\ncctrace --stdout --format md | less\ncctrace --stdout --format xml | xmllint --format -\ncctrace --stdout --format md \\> session.md\n\n### Programmatic Usage\n\n[](#programmatic-usage)\n\nimport { \n  exportCurrentSession, \n  findProjectSessions, \n  getSessionStats,\n  sessionToMarkdown \n} from 'cctrace-js';\n\n// Export current session\nconst result \\= await exportCurrentSession({\n  format: 'all',\n  copyToCwd: true,\n  maxMessageLength: 5000\n});\n\nconsole.log(\\`Exported ${result.metadata.totalMessages} messages\\`);\n\n// Get session statistics without full export\nconst stats \\= await getSessionStats();\nconsole.log(\\`Found ${stats.totalSessions} sessions, latest has ${stats.latestSession?.messageCount} messages\\`);\n\n// Convert session to markdown in-memory\nconst markdown \\= await sessionToMarkdown('/path/to/session.jsonl', {\n  maxMessageLength: 2000\n});\n\n// Find all sessions for a project\nconst sessions \\= findProjectSessions('/path/to/project');\nconsole.log(\\`Found ${sessions.length} sessions\\`);\n\n## üìñ CLI Options\n\n[](#-cli-options)\n\nOption\n\nDescription\n\nDefault\n\n`-s, --session-id <uuid>`\n\nExport specific session by ID\n\nauto-detect\n\n`-o, --output-dir <path>`\n\nCustom output directory\n\n`~/claude_sessions/exports`\n\n`-f, --format <format>`\n\nOutput format: `md`, `xml`, or `all`\n\n`all`\n\n`-m, --max-age <seconds>`\n\nMax age for active session detection\n\n`300`\n\n`--max-message-length <chars>`\n\nTruncate messages longer than N characters\n\nunlimited\n\n`--no-copy-to-cwd`\n\nDon't copy export to current directory\n\n`false`\n\n`--stdout`\n\nOutput to stdout instead of files (requires single format)\n\n`false`\n\n`-V, --version`\n\nShow version number\n\n`-h, --help`\n\nShow help message\n\n## üìÅ Export Contents\n\n[](#-export-contents)\n\nEach export creates a timestamped directory containing:\n\n```\n~/claude_sessions/exports/2025-08-31_14-12-44_fe7084a3/\n‚îú‚îÄ‚îÄ session_info.json      # Complete session metadata\n‚îú‚îÄ‚îÄ conversation_full.md   # Human-readable conversation\n‚îú‚îÄ‚îÄ conversation_full.xml  # Fully structured XML with metadata\n‚îú‚îÄ‚îÄ raw_messages.jsonl     # Original JSONL data\n‚îú‚îÄ‚îÄ schema.xsd            # XML schema definition\n‚îú‚îÄ‚îÄ summary.md            # Markdown summary with analytics\n‚îî‚îÄ‚îÄ summary.txt           # Plain text overview\n```\n\n### File Descriptions\n\n[](#file-descriptions)\n\n#### **session\\_info.json** - Complete Metadata\n\n[](#session_infojson---complete-metadata)\n\n{\n  \"sessionId\": \"fe7084a3-01ef-4a6e-bbae-43d3cf7a696c\",\n  \"projectDir\": \"/Users/username/myproject\",\n  \"startTime\": \"2025-08-31T14:02:49.129Z\",\n  \"endTime\": \"2025-08-31T14:12:42.753Z\",\n  \"totalMessages\": 124,\n  \"userMessages\": 56,\n  \"assistantMessages\": 68,\n  \"toolUses\": 56,\n  \"modelsUsed\": \\[\"claude-sonnet-4-20250514\"\\]\n}\n\n#### **conversation\\_full.xml** - Structured Data\n\n[](#conversation_fullxml---structured-data)\n\nComplete XML export with:\n\n-   Session-level metadata (ID, timestamps, working directory)\n-   Message relationships (UUID, parent-UUID)\n-   Content preservation (text, thinking blocks, tool usage)\n-   Token usage statistics per message\n-   Tool execution metadata (response codes, duration, etc.)\n\n#### **conversation\\_full.md** - Human-Readable\n\n[](#conversation_fullmd---human-readable)\n\n-   Clean, readable conversation format\n-   Collapsible thinking/reasoning blocks\n-   Tool usage with inputs and outputs\n-   Timestamps for each interaction\n\n## üîß Environment Variables\n\n[](#-environment-variables)\n\nVariable\n\nDescription\n\nDefault\n\n`CLAUDE_EXPORT_COPY_TO_CWD`\n\nAuto-copy to current directory\n\n`true`\n\n## üß© Library Integration & Embedding\n\n[](#-library-integration--embedding)\n\ncctrace-js is designed to be easily embedded in JavaScript/TypeScript projects as a library. See our comprehensive guides:\n\n-   **üìñ [Complete API Reference](/yigitkonur/cctrace-js/blob/master/docs/api/core-functions.md)** - Detailed API documentation\n-   **üîß [TypeScript Examples](/yigitkonur/cctrace-js/blob/master/docs/examples/typescript.md)** - React hooks, Express.js integration\n-   **üì¶ [JavaScript Examples](/yigitkonur/cctrace-js/blob/master/docs/examples/javascript.md)** - CommonJS and ESM usage\n-   **üèóÔ∏è [Embedding Guide](/yigitkonur/cctrace-js/blob/master/docs/guides/embedding.md)** - Webpack, Vite, Jest, CI/CD integration\n\n### Quick Embedding Examples\n\n[](#quick-embedding-examples)\n\n#### React Hook for Session Management\n\n[](#react-hook-for-session-management)\n\nimport { useCallback, useEffect, useState } from 'react';\nimport { getSessionStats, exportCurrentSession } from 'cctrace-js';\n\nexport function useClaudeSession() {\n  const \\[stats, setStats\\] \\= useState(null);\n  \n  const refreshStats \\= useCallback(async () \\=> {\n    try {\n      const sessionStats \\= await getSessionStats();\n      setStats(sessionStats);\n    } catch (error) {\n      console.warn('No Claude sessions found');\n    }\n  }, \\[\\]);\n\n  const exportSession \\= useCallback(async (options \\= {}) \\=> {\n    return await exportCurrentSession({\n      format: 'md',\n      maxMessageLength: 5000,\n      ...options\n    });\n  }, \\[\\]);\n\n  return { stats, refreshStats, exportSession };\n}\n\n#### Express.js Middleware\n\n[](#expressjs-middleware)\n\nconst { getSessionStats, exportCurrentSession } \\= require('cctrace-js');\n\nfunction cctraceMiddleware() {\n  return async (req, res, next) \\=> {\n    // Add export endpoint\n    if (req.path \\=== '/api/export-session' && req.method \\=== 'POST') {\n      try {\n        const result \\= await exportCurrentSession({\n          format: req.body.format || 'md',\n          maxMessageLength: req.body.maxMessageLength\n        });\n        \n        res.json({\n          success: true,\n          exportPath: result.exportPath,\n          messageCount: result.metadata.totalMessages\n        });\n      } catch (error) {\n        res.status(500).json({ error: error.message });\n      }\n      return;\n    }\n    next();\n  };\n}\n\n#### Webpack Plugin Integration\n\n[](#webpack-plugin-integration)\n\nconst { exportCurrentSession } \\= require('cctrace-js');\n\nclass CCTracePlugin {\n  apply(compiler) {\n    compiler.hooks.done.tapAsync('CCTracePlugin', async (stats, callback) \\=> {\n      try {\n        await exportCurrentSession({ format: 'all', copyToCwd: false });\n        console.log('‚úÖ Claude session exported after build');\n      } catch (error) {\n        console.warn('‚ö†Ô∏è Session export failed:', error.message);\n      }\n      callback();\n    });\n  }\n}\n\n## üìö Enhanced API Reference\n\n[](#-enhanced-api-reference)\n\n### Core Functions\n\n[](#core-functions)\n\n#### `exportCurrentSession(options?)`\n\n[](#exportcurrentsessionoptions)\n\nExport the current active session with full control.\n\nconst result \\= await exportCurrentSession({\n  sessionId?: string,           // Specific session ID\n  outputDir?: string,           // Custom output directory\n  format?: 'md' | 'xml' | 'all', // Export format(s)\n  maxAge?: number,              // Max session age in seconds\n  copyToCwd?: boolean,          // Copy to current directory\n  maxMessageLength?: number     // Truncate long messages\n});\n\n#### `getSessionStats(projectPath?)`\n\n[](#getsessionstatsprojectpath)\n\nGet comprehensive session statistics without full export.\n\nconst stats \\= await getSessionStats();\n// Returns: { totalSessions, latestSession, sessions\\[\\], ... }\n\n#### `sessionToMarkdown(sessionPath, options?)`\n\n[](#sessiontomarkdownsessionpath-options)\n\nConvert session to markdown string in-memory.\n\nconst markdown \\= await sessionToMarkdown('/path/to/session.jsonl', {\n  maxMessageLength: 2000\n});\n\n#### `sessionToXml(sessionPath)`\n\n[](#sessiontoxmlsessionpath)\n\nConvert session to XML string in-memory.\n\nconst xml \\= await sessionToXml('/path/to/session.jsonl');\n\n#### `extractMessages(sessionPath)`\n\n[](#extractmessagessessionpath)\n\nExtract conversation messages only (lightweight).\n\nconst messages \\= extractMessages('/path/to/session.jsonl');\nmessages.forEach(msg \\=> {\n  console.log(\\`${msg.role}: ${msg.content}\\`);\n});\n\n#### `findActiveSessions(projectPath?, maxAgeSeconds?)`\n\n[](#findactivesessionsprojectpath-maxageseconds)\n\nFind recently active sessions.\n\nconst activeSessions \\= findActiveSessions(process.cwd(), 300);\nconsole.log(\\`Found ${activeSessions.length} active sessions\\`);\n\n### Error Handling\n\n[](#error-handling)\n\nimport { \n  CCTraceError, \n  SessionNotFoundError, \n  InvalidSessionError \n} from 'cctrace-js';\n\ntry {\n  await exportCurrentSession();\n} catch (error) {\n  if (error instanceof SessionNotFoundError) {\n    console.log('No Claude sessions found');\n  } else if (error instanceof InvalidSessionError) {\n    console.warn('Session file corrupted');\n  }\n}\n\n### Classes\n\n[](#classes)\n\n#### `SessionFinder`\n\n[](#sessionfinder)\n\n-   `findProjectSessions(projectPath)` - Find all sessions for a project\n-   `getBestSessionToExport(projectPath, sessionId?, maxAge?)` - Get best session to export\n-   `identifyCurrentSession(sessions, projectDir)` - Identify current Claude instance session\n\n#### `SessionExporter`\n\n[](#sessionexporter)\n\n-   `exportSession(sessionInfo, options)` - Export a session with options\n-   `validateExportOptions(options)` - Validate export options\n-   `getExportStats(exportPath)` - Get statistics about an export\n\n#### `SessionParser`\n\n[](#sessionparser)\n\n-   `parseJsonlFile(filePath)` - Parse JSONL session file\n-   `extractTextContent(content)` - Extract text from message content\n-   `extractThinkingContent(content)` - Extract thinking blocks\n-   `extractToolUses(content)` - Extract tool usage information\n\n#### Formatters\n\n[](#formatters)\n\n-   `MarkdownFormatter` - Format sessions as Markdown\n-   `XmlFormatter` - Format sessions as XML with schema\n\n## üèóÔ∏è Development\n\n[](#Ô∏è-development)\n\n### Setup\n\n[](#setup)\n\n# Clone the repository\ngit clone https://github.com/yourusername/cctrace-js.git\ncd cctrace-js\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Run tests\nnpm test\n\n### Project Structure\n\n[](#project-structure)\n\n```\nsrc/\n‚îú‚îÄ‚îÄ cli.ts                    # Command-line interface\n‚îú‚îÄ‚îÄ index.ts                  # Main library exports\n‚îú‚îÄ‚îÄ types.ts                  # TypeScript type definitions\n‚îú‚îÄ‚îÄ utils.ts                  # Utility functions\n‚îú‚îÄ‚îÄ sessionFinder.ts          # Session discovery logic\n‚îú‚îÄ‚îÄ sessionParser.ts          # JSONL parsing logic\n‚îú‚îÄ‚îÄ exporter.ts              # Main export orchestration\n‚îî‚îÄ‚îÄ formatters/\n    ‚îú‚îÄ‚îÄ markdownFormatter.ts  # Markdown output formatter\n    ‚îî‚îÄ‚îÄ xmlFormatter.ts       # XML output formatter\n```\n\n## üö¢ Publishing Checklist\n\n[](#-publishing-checklist)\n\n-    Update version in `package.json`\n-    Run `npm run build` to compile TypeScript\n-    Run `npm test` to ensure all tests pass\n-    Run `npm run lint` to check code style\n-    Update `CHANGELOG.md` with new features/fixes\n-    Update `README.md` if needed\n-    Commit and tag the release: `git tag v1.0.0`\n-    Push to repository: `git push origin main --tags`\n-    Run `npm publish` to publish to NPM\n-    Create GitHub release with release notes\n\n## ü§ù Differences from Python Version\n\n[](#-differences-from-python-version)\n\nFeature\n\nPython cctrace\n\ncctrace-js\n\n**Runtime**\n\nPython 3.6+\n\nNode.js 16+\n\n**Installation**\n\nManual setup\n\n`npm install -g`\n\n**Dependencies**\n\nStandard library only\n\nTypeScript ecosystem\n\n**Type Safety**\n\nRuntime checking\n\nCompile-time TypeScript\n\n**Performance**\n\nGood\n\nExcellent (V8 engine)\n\n**API**\n\nCLI only\n\nCLI + Programmatic API\n\n**Message Control**\n\nFixed output\n\nConfigurable truncation\n\n**Packaging**\n\nCopy script files\n\nNPM package\n\n**Cross-platform**\n\nLinux/WSL\n\nmacOS/Linux/Windows\n\n## üêõ Troubleshooting\n\n[](#-troubleshooting)\n\n### \"No Claude Code sessions found\"\n\n[](#no-claude-code-sessions-found)\n\n-   Ensure you're running from a directory with active Claude Code sessions\n-   Check that `~/.claude/projects/` exists and contains your project\n-   Verify Claude Code has been used in this directory\n\n### \"Could not identify specific session\"\n\n[](#could-not-identify-specific-session)\n\n-   The tool will default to the most recently active session\n-   Use `--session-id` to manually specify a session\n-   Ensure the session file has been recently modified\n\n### Permission Errors\n\n[](#permission-errors)\n\n-   Verify you have read access to `~/.claude/projects/`\n-   Ensure write permissions for the export directory\n-   Check Node.js permissions\n\n### TypeScript Compilation Issues\n\n[](#typescript-compilation-issues)\n\n-   Ensure Node.js version >= 16.0.0\n-   Run `npm install` to install all dependencies\n-   Check TypeScript version compatibility\n\n## üìÑ License\n\n[](#-license)\n\nMIT License - see [LICENSE](/yigitkonur/cctrace-js/blob/master/LICENSE) file for details.\n\n## üôè Acknowledgments\n\n[](#-acknowledgments)\n\n-   Original Python [cctrace](https://github.com/jimmc414/cctrace) by jimmc414\n-   Claude Code team at Anthropic for the excellent development environment\n-   TypeScript and Node.js communities\n\n## üîó Related Projects\n\n[](#-related-projects)\n\n-   [cctrace](https://github.com/jimmc414/cctrace) - Original Python version\n-   [Claude Code](https://docs.anthropic.com/en/docs/claude-code) - Official documentation\n\n* * *\n\n**Made with ‚ù§Ô∏è for the Claude Code community**\n\n## About\n\nA TypeScript/Node.js version of cctrace - Export Claude Code chat sessions with conversation history, internal reasoning blocks, tool usage, and comprehensive metadata\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/cctrace-js/activity)\n\n### Stars\n\n[**2** stars](/yigitkonur/cctrace-js/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/cctrace-js/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/cctrace-js/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcctrace-js&report=yigitkonur+%28user%29)\n\n## [Releases 2](/yigitkonur/cctrace-js/releases)\n\n[\n\nv1.1.0 - Enhanced API with Comprehensive Embedding Support Latest\n\nAug 31, 2025\n\n](/yigitkonur/cctrace-js/releases/tag/v1.1.0)\n\n[\\+ 1 release](/yigitkonur/cctrace-js/releases)\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=cctrace-js)\n\nNo packages published  \n\n## [Contributors 2](/yigitkonur/cctrace-js/graphs/contributors)\n\n¬†¬†### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [TypeScript 87.1%](/yigitkonur/cctrace-js/search?l=typescript)\n-   [JavaScript 12.9%](/yigitkonur/cctrace-js/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/codex-worker\n\nGitHub - yigitkonur/codex-worker: Safe parallel execution of AI coding agents on your codebase                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcodex-worker)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcodex-worker)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fcodex-worker)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[codex-worker](/yigitkonur/codex-worker)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fcodex-worker) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fcodex-worker)\n-   [Star 0](/login?return_to=%2Fyigitkonur%2Fcodex-worker)\n    \n\nSafe parallel execution of AI coding agents on your codebase\n\n### License\n\n[MIT license](/yigitkonur/codex-worker/blob/main/LICENSE)\n\n[0 stars](/yigitkonur/codex-worker/stargazers) [0 forks](/yigitkonur/codex-worker/forks) [Branches](/yigitkonur/codex-worker/branches) [Tags](/yigitkonur/codex-worker/tags) [Activity](/yigitkonur/codex-worker/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fcodex-worker)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fcodex-worker) You must be signed in to change notification settings\n\n# yigitkonur/codex-worker\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/codex-worker/branches)[Tags](/yigitkonur/codex-worker/tags)\n\n[](/yigitkonur/codex-worker/branches)[](/yigitkonur/codex-worker/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[7 Commits](/yigitkonur/codex-worker/commits/main/)\n\n[](/yigitkonur/codex-worker/commits/main/)\n\n[src](/yigitkonur/codex-worker/tree/main/src \"src\")\n\n[src](/yigitkonur/codex-worker/tree/main/src \"src\")\n\n[.gitignore](/yigitkonur/codex-worker/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/codex-worker/blob/main/.gitignore \".gitignore\")\n\n[ARCHITECTURE.md](/yigitkonur/codex-worker/blob/main/ARCHITECTURE.md \"ARCHITECTURE.md\")\n\n[ARCHITECTURE.md](/yigitkonur/codex-worker/blob/main/ARCHITECTURE.md \"ARCHITECTURE.md\")\n\n[LICENSE](/yigitkonur/codex-worker/blob/main/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/codex-worker/blob/main/LICENSE \"LICENSE\")\n\n[Makefile](/yigitkonur/codex-worker/blob/main/Makefile \"Makefile\")\n\n[Makefile](/yigitkonur/codex-worker/blob/main/Makefile \"Makefile\")\n\n[README.md](/yigitkonur/codex-worker/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/codex-worker/blob/main/README.md \"README.md\")\n\n[demo.py](/yigitkonur/codex-worker/blob/main/demo.py \"demo.py\")\n\n[demo.py](/yigitkonur/codex-worker/blob/main/demo.py \"demo.py\")\n\n[pyproject.toml](/yigitkonur/codex-worker/blob/main/pyproject.toml \"pyproject.toml\")\n\n[pyproject.toml](/yigitkonur/codex-worker/blob/main/pyproject.toml \"pyproject.toml\")\n\n[requirements-dev.txt](/yigitkonur/codex-worker/blob/main/requirements-dev.txt \"requirements-dev.txt\")\n\n[requirements-dev.txt](/yigitkonur/codex-worker/blob/main/requirements-dev.txt \"requirements-dev.txt\")\n\n[requirements.txt](/yigitkonur/codex-worker/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/codex-worker/blob/main/requirements.txt \"requirements.txt\")\n\n[run.py](/yigitkonur/codex-worker/blob/main/run.py \"run.py\")\n\n[run.py](/yigitkonur/codex-worker/blob/main/run.py \"run.py\")\n\n[setup.py](/yigitkonur/codex-worker/blob/main/setup.py \"setup.py\")\n\n[setup.py](/yigitkonur/codex-worker/blob/main/setup.py \"setup.py\")\n\nView all files\n\n## Repository files navigation\n\n# Codex Worker ü§ñ\n\n[](#codex-worker-)\n\n**Production-ready tool for running OpenAI Codex CLI in parallel on your codebase**\n\nCodex Worker enables safe, resumable, parallel execution of OpenAI Codex CLI on multiple files. Perfect for automating repetitive coding tasks that don't require complex reasoning.\n\n## üî• Real-World Use Cases\n\n[](#-real-world-use-cases)\n\n### Save Claude/GPT-4 Tokens on Repetitive Tasks\n\n[](#save-claudegpt-4-tokens-on-repetitive-tasks)\n\n> \"I use Claude Code to analyze npm lint errors, group them by file and error type, then create task files for OpenAI Codex CLI to fix. Since the tasks are clear and mechanical, OpenAI Codex CLI handles them perfectly. This saves my Claude tokens for complex problems and avoids hitting rate limits!\"\n\n### Common Scenarios\n\n[](#common-scenarios)\n\n-   **Bulk Lint Fixes**: Let Claude analyze errors ‚Üí Create fix tasks ‚Üí OpenAI Codex CLI executes in parallel\n-   **Test Generation**: Define test requirements ‚Üí OpenAI Codex CLI writes tests for multiple files\n-   **Code Migrations**: Specify transformation rules ‚Üí OpenAI Codex CLI applies across codebase\n-   **Documentation**: Outline what's needed ‚Üí OpenAI Codex CLI adds to multiple files\n-   **Refactoring**: Define the pattern ‚Üí OpenAI Codex CLI applies consistently\n\nThe key insight: Use expensive AI (Claude/GPT-4) for **planning**, use OpenAI Codex CLI for **execution**.\n\n## üéØ Why Codex Worker?\n\n[](#-why-codex-worker)\n\nWhen working with AI coding agents, you often want to:\n\n-   **Process multiple tasks in parallel** - Run 10 agents fixing different bugs simultaneously\n-   **Resume after interruptions** - Power outage? Network issue? Just run again, completed tasks are skipped\n-   **Track progress visually** - See what's done, in-progress, or failed at a glance\n-   **Maintain safety** - Default read-only mode, explicit permissions for modifications\n-   **Coordinate multiple workers** - Run from multiple terminals without conflicts\n\n## üöÄ Key Features\n\n[](#-key-features)\n\n### üìÅ File Prefix State Management\n\n[](#-file-prefix-state-management)\n\nCodex Worker uses a simple but powerful approach: file prefixes track state.\n\n```\ntasks/\n‚îú‚îÄ‚îÄ fix-auth-bug.md                 # ‚è∏Ô∏è  Pending task\n‚îú‚îÄ‚îÄ in-progress-add-feature.md      # üîÑ Currently being processed\n‚îú‚îÄ‚îÄ done_exec_log-update-api.md.txt # ‚úÖ Completed task (with output log)\n‚îî‚îÄ‚îÄ failed_exec_log-refactor.md.txt # ‚ùå Failed task (with error log)\n```\n\nThis prefix system enables:\n\n-   **Zero-config resumability** - Just run the command again\n-   **Parallel coordination** - Workers see what others are doing\n-   **Clear visual status** - `ls` shows you everything\n-   **No database needed** - Filesystem is the source of truth\n\n### Command Options\n\n[](#command-options)\n\nOption\n\nDescription\n\nDefault\n\n`--pattern`\n\nFile pattern to match\n\n`*.md`\n\n`--model`\n\nModel name\n\n`o4-mini`\n\n`--mode`\n\nSafety level\n\n`read-only`\n\n`--concurrency`\n\nParallel workers\n\n`1`\n\n`--timeout`\n\nTask timeout (seconds)\n\nNone\n\n`--retries`\n\nRetry attempts\n\n`0`\n\n`--approval`\n\nHuman approval mode\n\n`on-request`\n\n### üîÑ Perfect Resumability\n\n[](#-perfect-resumability)\n\nInterrupted? No problem. Codex Worker automatically:\n\n1.  Skips completed tasks (files with `done_exec_log-` prefix)\n2.  Detects in-progress tasks from other workers\n3.  Cleans up stale locks from crashed processes\n4.  Retries failed tasks based on your configuration\n\n## üì¶ Installation\n\n[](#-installation)\n\n# Install with pip\npip install codex-worker\n\n# Or install from source\ngit clone https://github.com/yigitkonur/codex-worker\ncd codex-worker\npip install -e .\n\n### Requirements\n\n[](#requirements)\n\n-   Python 3.8+\n-   OpenAI Codex CLI:\n    \n    npm install -g @openai/codex\n    \n-   Typer, Rich (installed automatically with pip install)\n\n## üéÆ Usage Examples\n\n[](#-usage-examples)\n\n### Basic: Process All Tasks in Current Directory\n\n[](#basic-process-all-tasks-in-current-directory)\n\n# Create task files\necho \"Fix the authentication bug in login.py\" \\> fix-auth.md\necho \"Add rate limiting to API endpoints\" \\> add-ratelimit.md\necho \"Refactor database queries for performance\" \\> optimize-db.md\n\n# Run OpenAI Codex CLI on all .md files (safe read-only mode by default)\ncodex-worker\n\n# Output:\n# ‚úÖ fix-auth.md completed\n# ‚úÖ add-ratelimit.md completed  \n# ‚úÖ optimize-db.md completed\n\n### Parallel Execution: Speed Up with Multiple Workers\n\n[](#parallel-execution-speed-up-with-multiple-workers)\n\n# Run 4 AI agents in parallel\ncodex-worker --concurrency 4\n\n# Each agent works on a different file simultaneously\n# 4x faster than sequential execution!\n\n### Resume After Interruption\n\n[](#resume-after-interruption)\n\n# Start processing 100 files\ncodex-worker large-tasks/ --concurrency 8\n\n# Ctrl+C after 30 files complete\n# ^C Shutting down gracefully...\n\n# Run again - only processes remaining 70 files!\ncodex-worker large-tasks/ --concurrency 8\n# Automatically skips the 30 completed files\n\n### Different Models\n\n[](#different-models)\n\n# Use a specific OpenAI Codex CLI model\ncodex-worker --model o4\n\n# Use different models for different task complexity\ncodex-worker simple-tasks/ --model o4-mini\ncodex-worker complex-tasks/ --model o4\n\n### Safety Modes\n\n[](#safety-modes)\n\n# Dry run - see what would be executed\ncodex-worker --mode dry-run\n\n# Read-only (default) - agent can only read files\ncodex-worker --mode read-only\n\n# Workspace write - agent can modify files in current directory\ncodex-worker --mode workspace-write\n\n# Full access (DANGEROUS) - agent can modify any file\ncodex-worker --mode danger-full-access --yes\n\n### Monitor Progress\n\n[](#monitor-progress)\n\n# Check status of all tasks\ncodex-worker status\n\n# Output:\n# ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n# ‚îÉ Status      ‚îÉ Count ‚îÉ Percentage ‚îÉ\n# ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n# ‚îÇ Pending     ‚îÇ    45 ‚îÇ      45.0% ‚îÇ\n# ‚îÇ In Progress ‚îÇ     5 ‚îÇ       5.0% ‚îÇ\n# ‚îÇ Completed   ‚îÇ    40 ‚îÇ      40.0% ‚îÇ\n# ‚îÇ Failed      ‚îÇ    10 ‚îÇ      10.0% ‚îÇ\n# ‚îÇ Total       ‚îÇ   100 ‚îÇ     100.0% ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n# Detailed file-by-file status\ncodex-worker status --detailed\n\n### Clean Up and Reset\n\n[](#clean-up-and-reset)\n\n# Clean stale in-progress markers (process died)\ncodex-worker clean --max-age 3600\n\n# Reset all states to rerun tasks\ncodex-worker reset --force\n\n## üèóÔ∏è Architecture\n\n[](#Ô∏è-architecture)\n\n### How It Works\n\n[](#how-it-works)\n\n1.  **Task Discovery**: Finds all matching files (e.g., `*.md`)\n2.  **State Check**: Checks file prefixes to determine status\n3.  **Lock Acquisition**: Atomically acquires lock with `in-progress-` prefix\n4.  **Execution**: Runs AI agent with configured settings\n5.  **State Update**: Updates prefix to `done_exec_log-` or `failed_exec_log-`\n6.  **Parallel Coordination**: Multiple workers respect each other's locks\n\n### File Prefix Convention\n\n[](#file-prefix-convention)\n\nPrefix\n\nMeaning\n\nExample\n\n(none)\n\nPending task\n\n`fix-bug.md`\n\n`in-progress-`\n\nCurrently processing\n\n`in-progress-fix-bug.md`\n\n`done_exec_log-`\n\nSuccessfully completed\n\n`done_exec_log-fix-bug.md.txt`\n\n`failed_exec_log-`\n\nFailed execution\n\n`failed_exec_log-fix-bug.md.txt`\n\n`.lock-`\n\nTemporary lock file\n\n`.lock-fix-bug.md`\n\n## üõ°Ô∏è Safety Features\n\n[](#Ô∏è-safety-features)\n\n-   **Read-only by default** - Must explicitly enable write access\n-   **Confirmation prompts** - For dangerous operations\n-   **Atomic operations** - No partial states or race conditions\n-   **Process monitoring** - Detects and cleans up dead processes\n-   **Graceful shutdown** - Ctrl+C handled properly\n-   **Comprehensive validation** - All inputs validated with Typer\n\n## üéØ Real-World Use Cases\n\n[](#-real-world-use-cases-1)\n\n### 1\\. Parallel Bug Fixes\n\n[](#1-parallel-bug-fixes)\n\n# Create bug report files\nfor i in {1..20}; do\n  echo \"Fix bug #$i: Check the error in module$i.py\" \\> bug-$i.md\ndone\n\n# Fix all bugs in parallel with 5 workers\ncodex-worker bug-\\*.md --concurrency 5 --mode workspace-write\n\n### 2\\. Code Review Automation\n\n[](#2-code-review-automation)\n\n# Generate review tasks for each PR file\ngit diff main --name-only | while read file; do\n  echo \"Review and suggest improvements for $file\" \\> review-$(basename $file).md\ndone\n\n# Run reviews in parallel\ncodex-worker review-\\*.md --concurrency 10\n\n### 3\\. Test Generation\n\n[](#3-test-generation)\n\n# Create test generation tasks\nfind src -name \"\\*.py\" | while read file; do\n  echo \"Generate comprehensive tests for $file\" \\> test-$(basename $file).md\ndone\n\n# Generate all tests\ncodex-worker test-\\*.md --concurrency 8 --mode workspace-write\n\n### 4\\. Documentation Updates\n\n[](#4-documentation-updates)\n\n# Create doc tasks\necho \"Update README with new API endpoints\" \\> doc-api.md\necho \"Add examples to authentication guide\" \\> doc-auth.md\necho \"Document the new CLI commands\" \\> doc-cli.md\n\n# Process documentation updates\ncodex-worker doc-\\*.md --concurrency 3\n\n## üîß Configuration\n\n[](#-configuration)\n\n### Environment Variables\n\n[](#environment-variables)\n\nexport CODEX\\_CMD=/path/to/codex     # Custom codex binary\n\n## About\n\nSafe parallel execution of AI coding agents on your codebase\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/codex-worker/activity)\n\n### Stars\n\n[**0** stars](/yigitkonur/codex-worker/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/codex-worker/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/codex-worker/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcodex-worker&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/codex-worker/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=codex-worker)\n\nNo packages published  \n\n## Languages\n\n-   [Python 97.7%](/yigitkonur/codex-worker/search?l=python)\n-   [Makefile 2.3%](/yigitkonur/codex-worker/search?l=makefile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/example-mcp-server-stdio\n\nGitHub - yigitkonur/example-mcp-server-stdio: Simple, educational STDIO-based MCP server example demonstrating best practices for various protocols.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-stdio)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-stdio)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fexample-mcp-server-stdio)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[example-mcp-server-stdio](/yigitkonur/example-mcp-server-stdio)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-stdio) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-stdio)\n-   [Star 0](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-stdio)\n    \n\nSimple, educational STDIO-based MCP server example demonstrating best practices for various protocols.\n\n[0 stars](/yigitkonur/example-mcp-server-stdio/stargazers) [1 fork](/yigitkonur/example-mcp-server-stdio/forks) [Branches](/yigitkonur/example-mcp-server-stdio/branches) [Tags](/yigitkonur/example-mcp-server-stdio/tags) [Activity](/yigitkonur/example-mcp-server-stdio/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-stdio)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-stdio) You must be signed in to change notification settings\n\n# yigitkonur/example-mcp-server-stdio\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/example-mcp-server-stdio/branches)[Tags](/yigitkonur/example-mcp-server-stdio/tags)\n\n[](/yigitkonur/example-mcp-server-stdio/branches)[](/yigitkonur/example-mcp-server-stdio/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[15 Commits](/yigitkonur/example-mcp-server-stdio/commits/main/)\n\n[](/yigitkonur/example-mcp-server-stdio/commits/main/)\n\n[.github/workflows](/yigitkonur/example-mcp-server-stdio/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/example-mcp-server-stdio/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.husky](/yigitkonur/example-mcp-server-stdio/tree/main/.husky \".husky\")\n\n[.husky](/yigitkonur/example-mcp-server-stdio/tree/main/.husky \".husky\")\n\n[src](/yigitkonur/example-mcp-server-stdio/tree/main/src \"src\")\n\n[src](/yigitkonur/example-mcp-server-stdio/tree/main/src \"src\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-stdio/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-stdio/blob/main/.dockerignore \".dockerignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-stdio/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-stdio/blob/main/.gitignore \".gitignore\")\n\n[.nvmrc](/yigitkonur/example-mcp-server-stdio/blob/main/.nvmrc \".nvmrc\")\n\n[.nvmrc](/yigitkonur/example-mcp-server-stdio/blob/main/.nvmrc \".nvmrc\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-stdio/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-stdio/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-stdio/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-stdio/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-stdio/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-stdio/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-stdio/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-stdio/blob/main/Dockerfile \"Dockerfile\")\n\n[README.md](/yigitkonur/example-mcp-server-stdio/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/example-mcp-server-stdio/blob/main/README.md \"README.md\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-stdio/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-stdio/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-stdio/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-stdio/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-stdio/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-stdio/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/example-mcp-server-stdio/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/example-mcp-server-stdio/blob/main/package.json \"package.json\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-stdio/blob/main/smithery.yaml \"smithery.yaml\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-stdio/blob/main/smithery.yaml \"smithery.yaml\")\n\n[tsconfig.eslint.json](/yigitkonur/example-mcp-server-stdio/blob/main/tsconfig.eslint.json \"tsconfig.eslint.json\")\n\n[tsconfig.eslint.json](/yigitkonur/example-mcp-server-stdio/blob/main/tsconfig.eslint.json \"tsconfig.eslint.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-stdio/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-stdio/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n**[STDIO](https://github.com/yigitkonur/example-mcp-server-stdio) | [Stateful HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http) | [Stateless HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless) | [SSE](https://github.com/yigitkonur/example-mcp-server-sse)**\n\n* * *\n\n# üéì MCP STDIO Server - Educational Reference\n\n[](#-mcp-stdio-server---educational-reference)\n\n**A Production-Ready Model Context Protocol Server Teaching STDIO Transport and Process Isolation Best Practices**\n\n[![MCP Version](https://camo.githubusercontent.com/41b162b8e409c1a7944643ef63d3b126a69901d1ae2814c0f471b2c132760fa9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d43502d312e302e302d626c7565)](https://modelcontextprotocol.io) [![TypeScript](https://camo.githubusercontent.com/93c2cf106a4832b140c7fe81c6014c78f3c81470c6e9e2b7cd2220005e72364f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d352e782d626c7565)](https://www.typescriptlang.org/) [![SDK](https://camo.githubusercontent.com/bc35472816981eab04c2dc5730d34dcda5e92a06b5dfa08687404e129af5d2a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53444b2d50726f64756374696f6e25323052656164792d677265656e)](https://github.com/modelcontextprotocol/typescript-sdk) [![Architecture](https://camo.githubusercontent.com/ccfd4c002ee2330f3673eef669012bbd73ca72aa99012294089a427e0e4505ed/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4172636869746563747572652d50726f6365737325323042617365642d676f6c64)](/yigitkonur/example-mcp-server-stdio/blob/main)\n\n_Learn by building a world-class MCP server with a focus on security, simplicity, and architectural resilience._\n\n## üéØ Project Goal & Core Concepts\n\n[](#-project-goal--core-concepts)\n\nThis repository is a **deeply educational reference implementation** that demonstrates how to build a production-quality MCP server using the **STDIO (Standard I/O)** transport. It is the definitive guide for creating secure, efficient, and resilient locally-running tools.\n\nThrough a fully-functional calculator server, this project will teach you:\n\n1.  **üèóÔ∏è Clean Architecture & Design**: Master a layered architecture that separates business logic, protocol wiring, and state management, making the code maintainable and testable.\n2.  **‚öôÔ∏è Protocol & Transport Mastery**: Correctly implement the `StdioServerTransport` by learning the critical distinction between the `stdout` stream (for JSON-RPC messages) and the `stderr` stream (for all logging).\n3.  **üõ°Ô∏è Resilient Error Handling**: Implement a \"fail-fast\" error philosophy using `McpError` to ensure predictable, protocol-compliant failure states and prevent leaking internal details.\n4.  **üîí Inherent Security**: Leverage the **natural security boundary of process isolation**, which prevents network-based attacks and contains the server in a secure sandbox provided by the operating system.\n\n## ü§î When to Use This Architecture\n\n[](#-when-to-use-this-architecture)\n\nThe STDIO transport is the simplest and most secure MCP transport. Its process-based architecture makes it the ideal choice for:\n\n-   **IDE & Editor Extensions:** Integrating AI-powered tools directly into development environments like VS Code.\n-   **Command-Line Tools (CLIs):** Building powerful, local command-line applications that leverage LLMs.\n-   **Desktop Applications:** Embedding MCP capabilities into native desktop applications as managed subprocesses.\n-   **Secure Local Agents:** Any scenario where tools must run locally without exposing network ports, ensuring maximum security and data privacy.\n\n## üöÄ Quick Start\n\n[](#-quick-start)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Node.js ‚â• 20.0.0\n-   npm or yarn\n-   A basic understanding of how parent/child processes communicate.\n\n### Installation & Running\n\n[](#installation--running)\n\n# Clone the repository\ngit clone https://github.com/yigitkonur/example-mcp-server-stdio\ncd example-mcp-server-stdio\n\n# Install dependencies\nnpm install\n\n# Build the project (compiles TypeScript to dist/)\nnpm run build\n\n### Essential Commands\n\n[](#essential-commands)\n\nnpm run dev        # Development mode with hot-reload (uses tsx)\nnpm run build      # Compile TypeScript to JavaScript in \\`dist/\\`\nnpm run start      # Run the compiled server (listens on stdio)\nnpm run lint       # Run code quality checks with ESLint\nnpm run typecheck  # Run the TypeScript compiler for type checking\nnpm run pipeline   # Full build pipeline with zero-warning enforcement\nnpm run all        # Complete pipeline + smoke test verification\n\n## üìê Architecture Overview\n\n[](#-architecture-overview)\n\n### High-Level Principles\n\n[](#high-level-principles)\n\n1.  **Process Isolation:** The server is a separate OS process, providing a hardware-enforced security boundary.\n2.  **Stream-Based Communication:** All protocol messages are newline-delimited JSON-RPC 2.0 objects exchanged over `stdin` and `stdout`.\n3.  **Dedicated Logging Channel:** All non-protocol output (logs, debug messages) **must** be written to `stderr`.\n4.  **Zero Network Footprint:** The server does not open any network ports, eliminating entire classes of vulnerabilities.\n\n### Architectural Diagram\n\n[](#architectural-diagram)\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   stdin (JSON-RPC)   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                 ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ                  ‚îÇ\n‚îÇ   MCP Client    ‚îÇ                      ‚îÇ   MCP Server     ‚îÇ\n‚îÇ   (Parent)      ‚îÇ <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   (Subprocess)   ‚îÇ\n‚îÇ                 ‚îÇ  stdout (JSON-RPC)   ‚îÇ                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Code Structure\n\n[](#code-structure)\n\nThe source code is intentionally structured for clarity and maintainability.\n\n-   `src/types.ts`: Contains only pure data structures, static constants, and the global logger. It acts as the project's stable \"header file.\"\n-   `src/server.ts`: The single source of truth for all logic, organized into clear, layered sections:\n    1.  **Global State:** Defines the server's in-memory state (e.g., `calculationHistory`).\n    2.  **Core Business Logic:** Contains pure, testable functions (e.g., `factorial`, `performBasicCalculation`) that are completely unaware of the MCP protocol.\n    3.  **MCP Wiring:** A set of `register...` functions that connect the business logic to the MCP SDK, defining tools, resources, and prompts. This is where the application's capabilities are composed.\n    4.  **Execution:** The `main` function that bootstraps the server, handles the process lifecycle, and implements graceful shutdown.\n\n## üîß World-Class Best Practices\n\n[](#-world-class-best-practices)\n\nThis server is built on a foundation of non-negotiable best practices for creating professional, resilient software.\n\n### 1\\. Architecture: Composition over Configuration\n\n[](#1-architecture-composition-over-configuration)\n\nInstead of a single, monolithic function, the server's capabilities are built using a clean, compositional pattern. The main `createCalculatorServer` factory assembles the final server by calling a series of dedicated, single-responsibility functions.\n\n**The Principle:** This pattern makes the server's features immediately discoverable by reading the factory's body. It's organized, scalable, and easy to reason about.\n\n// ‚úÖ In src/server.ts\nexport async function createCalculatorServer() {\n  // 1. Create the server instance\n  const server \\= new McpServer(...);\n\n  // 2. Compose the server's capabilities by calling focused functions\n  registerCoreTools(server);\n  registerExtendedTools(server);\n  registerResources(server);\n  registerPrompts(server);\n  registerManagementTools(server);\n\n  // 3. Return the fully configured server\n  return server;\n}\n\n### 2\\. Logic: Co-location of Schemas and Handlers\n\n[](#2-logic-co-location-of-schemas-and-handlers)\n\nEach tool, resource, or prompt is defined as a self-contained unit. Its Zod schema, metadata, and handler logic are co-located, making the code easy to understand, modify, and test.\n\n**The Principle:** A developer should be able to understand everything about a tool by looking at a single, focused block of code, without hunting through other files.\n\n// ‚úÖ In src/server.ts inside a registration function\nconst calculateInputSchema \\= { a: z.number() /\\* ... \\*/ };\nconst calculateOutputSchema \\= { value: z.number() /\\* ... \\*/ };\n\nserver.registerTool(\n  'calculate',\n  {\n    title: 'Calculate',\n    inputSchema: calculateInputSchema,\n    outputSchema: calculateOutputSchema,\n  },\n  async (params) \\=> {\n    /\\* Handler logic here \\*/\n  },\n);\n\n### 3\\. Resilience: Advanced Error Handling Strategy\n\n[](#3-resilience-advanced-error-handling-strategy)\n\nThis server implements a sophisticated, multi-layered error handling approach that distinguishes between protocol-level and application-level failures.\n\n**The Principle:** Never `throw new Error()`. Always throw an instance of `McpError` from the SDK. This ensures the client _always_ receives a well-formed JSON-RPC error response and prevents internal details like stack traces from ever leaking.\n\n// ‚ùå ANTI-PATTERN: Generic, non-compliant, leaks implementation details.\n// if (b === 0) throw new Error('Cannot divide by zero!');\n\n// ‚úÖ BEST PRACTICE: Protocol-compliant, specific, and safe.\nimport { McpError, ErrorCode } from '@modelcontextprotocol/sdk';\n\nif (b \\=== 0) {\n  // The client can programmatically react to the \\`InvalidParams\\` code.\n  throw new McpError(ErrorCode.InvalidParams, 'Division by zero is not allowed.');\n}\n\n**Advanced Pattern - Application vs Protocol Errors:** Some tools (like `batch_calculate`) demonstrate application-level error handling where individual item failures don't fail the entire operation, providing granular error feedback while maintaining overall tool success.\n\n### 4\\. Transport: Protocol-Safe Logging to `stderr`\n\n[](#4-transport-protocol-safe-logging-to-stderr)\n\nThis is the most critical rule for STDIO transport. `stdout` is a sacred data channel reserved exclusively for JSON-RPC messages.\n\n**The Principle:** All logging, debugging, and other non-protocol text **must** be written to the `stderr` stream to avoid corrupting the communication channel.\n\n// ‚úÖ In src/types.ts\nexport const log \\= {\n  // Uses console.error() to write to the stderr stream.\n  info: (msg: string) \\=> console.error(\\`\\[INFO\\] ${msg}\\`),\n  error: (msg: string) \\=> console.error(\\`\\[ERROR\\] ${msg}\\`),\n};\n\n## üìä Features Implemented\n\n[](#-features-implemented)\n\nThis server implements a comprehensive set of capabilities to demonstrate the full power of the Model Context Protocol.\n\n_(The features table remains the same as it accurately reflects the server's capabilities.)_\n\n... (Tools, Resources, Prompts tables here) ...\n\n## üß™ Testing & Validation\n\n[](#-testing--validation)\n\n_(This section remains the same.)_\n\n... (Manual Request, Inspector, Test Suite sections here) ...\n\n## üè≠ Deployment & Configuration\n\n[](#-deployment--configuration)\n\nA STDIO server is not deployed like a web server. It is designed to be **executed as a child process** by a parent application.\n\n### Configuration\n\n[](#configuration)\n\nThe server's behavior can be modified with command-line flags.\n\nFlag\n\nDescription\n\n`--debug`\n\nEnables verbose debug logging to `stderr`.\n\n`--help`\n\nShows the help message and exits.\n\n### Environment Variables\n\n[](#environment-variables)\n\nVariable\n\nDescription\n\nDefault\n\n`SAMPLE_TOOL_NAME`\n\n**(Educational)** Demonstrates dynamic tool registration via environment variables. When set, adds a simple echo tool with the specified name that takes a `value` parameter and returns `test string print: {value}`. This pattern shows how MCP servers can be configured at runtime.\n\nNone\n\n### Production Readiness Checklist\n\n[](#production-readiness-checklist)\n\n-    **Process Isolation:** The OS provides a natural security sandbox.\n-    **Input Validation:** Zod schemas are used for all incoming tool arguments.\n-    **No Network Exposure:** The server does not listen on any network ports.\n-    **Sanitized Errors:** Using `McpError` ensures no internal details are ever leaked.\n-    **Robust Lifecycle Management:** The `main` function implements modular signal handlers (`SIGINT`, `SIGTERM`) and global exception handlers to ensure the server always exits cleanly with a specific exit code, making it a reliable citizen in any process-managed environment.\n-    **Compositional Architecture:** Clean separation of setup functions (`setupGracefulShutdown`, `setupGlobalErrorHandlers`) for maximum maintainability and testability.\n\n**Monitoring:**\n\n-   **Health & Logs:** The parent process is responsible for monitoring the child process's health and capturing its `stderr` stream for logging.\n-   **Resources:** Standard OS tools (`top`, `htop`) can be used to monitor CPU and memory usage.\n\n## About\n\nSimple, educational STDIO-based MCP server example demonstrating best practices for various protocols.\n\n### Topics\n\n[mcp](/topics/mcp \"Topic: mcp\") [model-context-protocol](/topics/model-context-protocol \"Topic: model-context-protocol\") [mcp-server](/topics/mcp-server \"Topic: mcp-server\") [stdio-mcp](/topics/stdio-mcp \"Topic: stdio-mcp\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/example-mcp-server-stdio/activity)\n\n### Stars\n\n[**0** stars](/yigitkonur/example-mcp-server-stdio/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/example-mcp-server-stdio/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/example-mcp-server-stdio/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-stdio&report=yigitkonur+%28user%29)\n\n## [Contributors 2](/yigitkonur/example-mcp-server-stdio/graphs/contributors)\n\n¬†¬†### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [JavaScript 91.5%](/yigitkonur/example-mcp-server-stdio/search?l=javascript)\n-   [TypeScript 5.5%](/yigitkonur/example-mcp-server-stdio/search?l=typescript)\n-   [Dockerfile 3.0%](/yigitkonur/example-mcp-server-stdio/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless\n\nGitHub - yigitkonur/example-mcp-server-streamable-http-stateless: Simple, educational Stateless Streamable HTTP-based MCP server example demonstrating best practices for various protocols.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[example-mcp-server-streamable-http-stateless](/yigitkonur/example-mcp-server-streamable-http-stateless)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless) You must be signed in to change notification settings\n-   [Fork 2](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n-   [Star 0](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n    \n\nSimple, educational Stateless Streamable HTTP-based MCP server example demonstrating best practices for various protocols.\n\n[0 stars](/yigitkonur/example-mcp-server-streamable-http-stateless/stargazers) [2 forks](/yigitkonur/example-mcp-server-streamable-http-stateless/forks) [Branches](/yigitkonur/example-mcp-server-streamable-http-stateless/branches) [Tags](/yigitkonur/example-mcp-server-streamable-http-stateless/tags) [Activity](/yigitkonur/example-mcp-server-streamable-http-stateless/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless) You must be signed in to change notification settings\n\n# yigitkonur/example-mcp-server-streamable-http-stateless\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/example-mcp-server-streamable-http-stateless/branches)[Tags](/yigitkonur/example-mcp-server-streamable-http-stateless/tags)\n\n[](/yigitkonur/example-mcp-server-streamable-http-stateless/branches)[](/yigitkonur/example-mcp-server-streamable-http-stateless/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[17 Commits](/yigitkonur/example-mcp-server-streamable-http-stateless/commits/main/)\n\n[](/yigitkonur/example-mcp-server-streamable-http-stateless/commits/main/)\n\n[.github/workflows](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.husky](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/.husky \".husky\")\n\n[.husky](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/.husky \".husky\")\n\n[src](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/src \"src\")\n\n[src](/yigitkonur/example-mcp-server-streamable-http-stateless/tree/main/src \"src\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.dockerignore \".dockerignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.gitignore \".gitignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/Dockerfile \"Dockerfile\")\n\n[README.md](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/README.md \"README.md\")\n\n[docker-compose.dev.yml](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/docker-compose.dev.yml \"docker-compose.dev.yml\")\n\n[docker-compose.dev.yml](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/docker-compose.dev.yml \"docker-compose.dev.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/package.json \"package.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n**[STDIO](https://github.com/yigitkonur/example-mcp-server-stdio) | [Stateful HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http) | [Stateless HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless) | [SSE](https://github.com/yigitkonur/example-mcp-server-sse)**\n\n* * *\n\n# üéì MCP Stateless HTTP Streamable Server - Educational Reference\n\n[](#-mcp-stateless-http-streamable-server---educational-reference)\n\n**A Production-Ready Model Context Protocol Server Teaching Stateless Architecture and Scalability Best Practices**\n\n[![MCP Version](https://camo.githubusercontent.com/41b162b8e409c1a7944643ef63d3b126a69901d1ae2814c0f471b2c132760fa9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d43502d312e302e302d626c7565)](https://spec.modelcontextprotocol.io) [![TypeScript](https://camo.githubusercontent.com/93c2cf106a4832b140c7fe81c6014c78f3c81470c6e9e2b7cd2220005e72364f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d352e782d626c7565)](https://www.typescriptlang.org/) [![SDK](https://camo.githubusercontent.com/bc35472816981eab04c2dc5730d34dcda5e92a06b5dfa08687404e129af5d2a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53444b2d50726f64756374696f6e25323052656164792d677265656e)](https://github.com/modelcontextprotocol/typescript-sdk) [![Architecture](https://camo.githubusercontent.com/e5c24e8232a3cb646b64c27aaadd7221ceaf009c55054c06499ae6feea0d10ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4172636869746563747572652d5472756525323053746174656c6573732d676f6c64)](/yigitkonur/example-mcp-server-streamable-http-stateless/blob/main)\n\n_Learn by building a world-class MCP server designed for infinite scalability, security, and maintainability._\n\n## üéØ Project Goal & Core Concepts\n\n[](#-project-goal--core-concepts)\n\nThis repository is a **deeply educational reference implementation** that demonstrates how to build a production-quality MCP server using a **truly stateless architecture**. This design is the gold standard for modern, cloud-native services.\n\nThrough a fully-functional calculator server, this project will teach you:\n\n1.  **üèóÔ∏è Clean Architecture & Design**: Master the **\"fresh instance per request\"** pattern for infinite scaling and learn to structure your code with a clean separation of concerns (`types.ts` for data contracts, `server.ts` for logic).\n2.  **‚öôÔ∏è Protocol & Transport Mastery**: Correctly implement the `StreamableHTTPServerTransport` in its **stateless mode**, delegating all low-level protocol validation to the SDK.\n3.  **üîí Production-Grade Security**: Implement non-negotiable security layers, including **rate limiting**, request size validation, **DNS rebinding protection**, and strict CORS policies.\n4.  **‚ö° Resilient Error Handling**: Implement a \"fail-fast\" and \"no-leaks\" error policy using specific, protocol-compliant `McpError` types for predictable and secure failure modes.\n5.  **üìà Production Observability**: Build a server that is transparent and monitorable from day one with structured logging, health check endpoints, and Prometheus-compatible metrics.\n\n## ü§î When to Use This Architecture\n\n[](#-when-to-use-this-architecture)\n\nA stateless architecture is the optimal choice for environments where scalability, resilience, and operational simplicity are paramount.\n\n-   **Serverless Platforms:** Perfect for deployment to AWS Lambda, Vercel, Google Cloud Functions, or any \"Function-as-a-Service\" platform.\n-   **Auto-Scaling Environments:** Ideal for container orchestrators like Kubernetes, where a Horizontal Pod Autoscaler can add or remove server replicas based on traffic, with no need for session affinity (\"sticky sessions\").\n-   **High-Traffic APIs:** When you need to serve a large number of independent requests and cannot be constrained by the memory or state of a single server.\n-   **Simplified Operations:** Eliminates the need for a shared state store (like Redis), reducing infrastructure complexity and maintenance overhead.\n\n## üöÄ Quick Start\n\n[](#-quick-start)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Node.js ‚â• 20.0.0\n-   npm or yarn\n-   Docker (for containerized deployment)\n\n### Installation & Running\n\n[](#installation--running)\n\n# Clone the repository\ngit clone https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless\ncd example-mcp-server-streamable-http-stateless\n\n# Install dependencies\nnpm install\n\n# Build the project (compiles TypeScript to dist/)\nnpm run build\n\n# Start the server in development mode (port 1071)\nnpm run dev\n\n### Essential Commands\n\n[](#essential-commands)\n\nnpm run dev        # Development mode with hot-reload (uses tsx)\nnpm run build      # Compile TypeScript to JavaScript in \\`dist/\\`\nnpm run start      # Run the production-ready compiled server\nnpm run lint       # Run code quality checks with ESLint\nnpm run lint:ci    # Run lint with zero warnings enforced\nnpm run typecheck  # TypeScript type checking\nnpm run format     # Format code with Prettier\nnpm run pipeline   # Full CI pipeline (clean + typecheck + lint + format + build)\nnpm run all        # Complete pipeline + smoke test\n\n## üìê Architecture Overview\n\n[](#-architecture-overview)\n\n### Key Principles\n\n[](#key-principles)\n\nThis server's architecture is defined by a commitment to modern best practices for building scalable and maintainable services.\n\n1.  **Stateless by Design:** The server shares absolutely no state between requests. Every request is handled in complete isolation.\n2.  **Ephemeral Instances & Explicit Cleanup:** The core of this pattern is creating a new `McpServer` and `Transport` for every request. These instances are explicitly destroyed when the request completes to prevent memory leaks.\n3.  **Clean Code Architecture:** The codebase is intentionally split into `types.ts` (for data contracts, schemas, and constants) and `server.ts` (for runtime logic), promoting maintainability and a clear separation of concerns.\n4.  **Resilient Error Handling:** The server uses a \"fail-fast\" and \"no-leaks\" error policy, throwing specific `McpError` types for predictable failures and wrapping all unexpected errors in a generic, safe response.\n5.  **Production Observability:** The server exposes `/health` and `/metrics` endpoints from the start, making it transparent and easy to monitor in production environments.\n\n### Architectural Diagrams\n\n[](#architectural-diagrams)\n\n#### Logical Request Flow\n\n[](#logical-request-flow)\n\nThis diagram shows how a single request is processed in our stateless model.\n\n```\n      Load Balancer (No Sticky Sessions Needed)\n               |\n    +----------+----------+----------+\n    |          |          |          |\n Server 1   Server 2   Server 3   Server N  (Each server is identical)\n    |\n    | (Inside a single server handling one request)\n    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  HTTP Request (POST/GET)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n             ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Express.js Middleware    ‚îÇ\n‚îÇ  (CORS, Rate Limit, Size) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  handleMCPRequest Function‚îÇ\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ ‚îÇ Ephemeral McpServer   ‚îÇ ‚îÇ Create -> Connect -> Handle -> Destroy\n‚îÇ ‚îÇ Ephemeral Transport   ‚îÇ ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n             ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   HTTP Response / SSE     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n#### Code Structure\n\n[](#code-structure)\n\nThis diagram shows how the source code is organized for maximum clarity and maintainability.\n\n```\nsrc/\n‚îú‚îÄ‚îÄ types.ts      # Data Contracts (Schemas, Constants, Type Interfaces)\n|                 #  - The \"what\" of our application.\n|                 #  - Stable, logic-free, and reusable.\n|\n‚îî‚îÄ‚îÄ server.ts     # Runtime Logic (Server, Handlers, Tools, Middleware)\n                  #  - The \"how\" of our application.\n                  #  - Implements all behavior and depends on types.ts.\n```\n\n## üîß Core Implementation Patterns\n\n[](#-core-implementation-patterns)\n\nThis section highlights the most important, production-ready patterns demonstrated in this repository.\n\n### Pattern 1: The \"Per-Request Instance\" Lifecycle\n\n[](#pattern-1-the-per-request-instance-lifecycle)\n\n**The Principle:** To guarantee statelessness and prevent memory leaks, we follow a strict **create-use-destroy** lifecycle for server and transport objects within the scope of a single HTTP request handler.\n\n**The Implementation:**\n\n// src/server.ts\nconst handleMCPRequest \\= async (req: Request, res: Response) \\=> {\n  try {\n    // 1. CREATE: A fresh server and a stateless transport are created.\n    const server \\= createMCPServer();\n    const transport \\= new StreamableHTTPServerTransport({\n      sessionIdGenerator: undefined, // CRITICAL: This enables stateless mode.\n    });\n\n    // 2. CONNECT & HANDLE: The ephemeral instances process the single request.\n    await server.connect(transport);\n    await transport.handleRequest(req, res, req.body);\n\n    // 3. CLEANUP: Once the connection closes, we MUST destroy the instances.\n    res.on('close', () \\=> {\n      transport.close();\n      server.close(); // This prevents memory leaks.\n    });\n  } catch (error) {\n    // ... global error handling ...\n  }\n};\n\n### Pattern 2: Resilient & Secure Error Handling\n\n[](#pattern-2-resilient--secure-error-handling)\n\n**The Principle:** The server follows a \"fail-fast\" and \"no-leaks\" error policy. Predictable errors are reported with specific, protocol-compliant codes, while unexpected errors are caught and sanitized to prevent leaking internal details.\n\n**The Implementation:**\n\n1.  **Specific, Actionable Errors**: Predictable user errors, like division by zero, throw a specific `McpError`. This allows the client application to understand the failure and prompt the user for a correction.\n    \n    // In the 'calculate' tool for the 'divide' operation:\n    if (b \\=== 0) {\n      // Throw a structured error that the client can parse.\n      throw new McpError(ErrorCode.InvalidParams, 'Division by zero is not allowed.');\n    }\n    \n2.  **The \"Safety Net\" for Unexpected Errors**: The main `handleMCPRequest` function is wrapped in a `try...catch` block that acts as a safety net. It catches any unhandled exception, logs it internally, and returns a generic, safe error to the client.\n    \n    // In src/server.ts -> handleMCPRequest\n    } catch (error) {\n      // 1. Log the full, detailed error for internal debugging.\n      requestLogger.error('Unhandled error in MCP request handler', { error });\n    \n      // 2. Send a generic, protocol-compliant error to the client.\n      //    This prevents leaking stack traces or implementation details.\n      res.status(500).json({\n        jsonrpc: '2.0',\n        error: {\n          code: ErrorCode.InternalError,\n          message: 'An internal server error occurred.',\n        },\n        id: req.body?.id || null,\n      });\n    }\n    \n\n### Pattern 3: Strict Separation of Concerns (`types.ts` vs. `server.ts`)\n\n[](#pattern-3-strict-separation-of-concerns-typests-vs-serverts)\n\n**The Principle:** A clean architecture separates data contracts (the \"what\") from implementation logic (the \"how\"). This makes the code easier to maintain, test, and reason about.\n\n**The Implementation:**\n\n-   **`src/types.ts`**: This file contains only data definitions. It has no runtime logic. It defines all Zod schemas for input validation, shared constants, and TypeScript interfaces. It is the stable foundation of the application.\n-   **`src/server.ts`**: This file contains all runtime logic. It imports the data contracts from `types.ts` and uses them to implement the server's behavior, including the Express app, middleware, tool handlers, and startup sequence.\n\n### Pattern 4: Production-Ready Observability\n\n[](#pattern-4-production-ready-observability)\n\n**The Principle:** A production service must be transparent. This server includes built-in endpoints for health checks and metrics, allowing it to be easily integrated into modern monitoring and orchestration systems.\n\n**The Implementation:**\n\n-   **`/health`:** A simple endpoint that returns a `200 OK` status with basic uptime and memory information. Perfect for load balancers and container readiness probes.\n-   **`/metrics`:** Exposes key performance indicators (KPIs) like request duration and tool execution times in a **Prometheus-compatible format**, ready to be scraped by monitoring systems like Prometheus or Grafana.\n\n## üß™ Testing & Validation\n\n[](#-testing--validation)\n\n### Health & Metrics\n\n[](#health--metrics)\n\nVerify the server's operational status.\n\n# Check basic health (responds with 200 OK if running)\ncurl http://localhost:1071/health\n\n# Check Prometheus-style metrics for monitoring systems\ncurl http://localhost:1071/metrics\n\n### Manual Request\n\n[](#manual-request)\n\nSend a direct `curl` request to test a tool's functionality.\n\n#### Testing a Success Case\n\n[](#testing-a-success-case)\n\n# Test the 'calculate' tool\ncurl -X POST \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Accept: application/json, text/event-stream\" \\\\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{\"name\":\"calculate\",\"arguments\":{\"a\":15,\"b\":7,\"op\":\"add\"}},\"id\":1}' \\\\\n  http://localhost:1071/mcp\n\n#### Testing an Error Case\n\n[](#testing-an-error-case)\n\nThis command intentionally triggers the `InvalidParams` error to demonstrate the server's resilient error handling.\n\n# Test division by zero\ncurl -X POST \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Accept: application/json, text/event-stream\" \\\\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{\"name\":\"calculate\",\"arguments\":{\"a\":10,\"b\":0,\"op\":\"divide\"}},\"id\":2}' \\\\\n  http://localhost:1071/mcp\n\n# Expected Error Response:\n# {\n#   \"jsonrpc\": \"2.0\",\n#   \"error\": {\n#     \"code\": -32602,\n#     \"message\": \"Division by zero is not allowed.\"\n#   },\n#   \"id\": 2\n# }\n\n### Interactive Testing with MCP Inspector\n\n[](#interactive-testing-with-mcp-inspector)\n\nUse the official inspector for a rich, interactive testing experience.\n\n# The inspector connects to the server's endpoint via HTTP.\nnpx @modelcontextprotocol/inspector --cli http://localhost:1071/mcp\n\n## üè≠ Deployment & Configuration\n\n[](#-deployment--configuration)\n\n### Configuration\n\n[](#configuration)\n\nThe server is configured using environment variables, making it perfect for containerized deployments.\n\nVariable\n\nDescription\n\nDefault\n\n`PORT`\n\nThe port for the HTTP server to listen on.\n\n`1071`\n\n`LOG_LEVEL`\n\nLogging verbosity (`debug`, `info`, `warn`, `error`).\n\n`info`\n\n`CORS_ORIGIN`\n\nAllowed origin for CORS. **Must be restricted in production.**\n\n`*`\n\n`RATE_LIMIT_MAX`\n\nMax requests per window per IP.\n\n`1000`\n\n`RATE_LIMIT_WINDOW`\n\nRate limit window in milliseconds.\n\n`900000` (15 min)\n\n`NODE_ENV`\n\nSets the environment. Use `production` for Express optimizations.\n\n`development`\n\n`SAMPLE_TOOL_NAME`\n\n**(Educational)** Demonstrates dynamic tool registration via environment variables. When set, adds a simple echo tool with the specified name that takes a `value` parameter and returns `test string print: {value}`. This pattern shows how MCP servers can be configured at runtime.\n\nNone\n\n### Deployment\n\n[](#deployment)\n\nThis server is designed from the ground up for modern, scalable deployment platforms. The included multi-stage `Dockerfile` and `docker-compose.yml` provide a secure and efficient container.\n\n-   **Serverless:** The `handleMCPRequest` function can be exported directly as a serverless function handler for platforms like Vercel or AWS Lambda.\n-   **Kubernetes:** The Docker image is ready to be deployed with a Horizontal Pod Autoscaler (HPA), allowing the cluster to automatically scale replicas up and down based on CPU or request load.\n\n## About\n\nSimple, educational Stateless Streamable HTTP-based MCP server example demonstrating best practices for various protocols.\n\n### Topics\n\n[serverless](/topics/serverless \"Topic: serverless\") [mcp](/topics/mcp \"Topic: mcp\") [stateless](/topics/stateless \"Topic: stateless\") [model-context-protocol](/topics/model-context-protocol \"Topic: model-context-protocol\") [mcp-server](/topics/mcp-server \"Topic: mcp-server\") [streamable-http](/topics/streamable-http \"Topic: streamable-http\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/example-mcp-server-streamable-http-stateless/activity)\n\n### Stars\n\n[**0** stars](/yigitkonur/example-mcp-server-streamable-http-stateless/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/example-mcp-server-streamable-http-stateless/watchers)\n\n### Forks\n\n[**2** forks](/yigitkonur/example-mcp-server-streamable-http-stateless/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http-stateless&report=yigitkonur+%28user%29)\n\n## [Contributors 2](/yigitkonur/example-mcp-server-streamable-http-stateless/graphs/contributors)\n\n¬†¬†### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [TypeScript 96.0%](/yigitkonur/example-mcp-server-streamable-http-stateless/search?l=typescript)\n-   [Dockerfile 2.6%](/yigitkonur/example-mcp-server-streamable-http-stateless/search?l=dockerfile)\n-   [JavaScript 1.4%](/yigitkonur/example-mcp-server-streamable-http-stateless/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/example-mcp-server-sse\n\nGitHub - yigitkonur/example-mcp-server-sse: Simple, educational SSE-based MCP server example demonstrating best practices for various protocols.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-sse)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-sse)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fexample-mcp-server-sse)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[example-mcp-server-sse](/yigitkonur/example-mcp-server-sse)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-sse) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-sse)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-sse)\n    \n\nSimple, educational SSE-based MCP server example demonstrating best practices for various protocols.\n\n[1 star](/yigitkonur/example-mcp-server-sse/stargazers) [0 forks](/yigitkonur/example-mcp-server-sse/forks) [Branches](/yigitkonur/example-mcp-server-sse/branches) [Tags](/yigitkonur/example-mcp-server-sse/tags) [Activity](/yigitkonur/example-mcp-server-sse/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-sse)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-sse) You must be signed in to change notification settings\n\n# yigitkonur/example-mcp-server-sse\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/example-mcp-server-sse/branches)[Tags](/yigitkonur/example-mcp-server-sse/tags)\n\n[](/yigitkonur/example-mcp-server-sse/branches)[](/yigitkonur/example-mcp-server-sse/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[16 Commits](/yigitkonur/example-mcp-server-sse/commits/main/)\n\n[](/yigitkonur/example-mcp-server-sse/commits/main/)\n\n[.github/workflows](/yigitkonur/example-mcp-server-sse/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/example-mcp-server-sse/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.husky](/yigitkonur/example-mcp-server-sse/tree/main/.husky \".husky\")\n\n[.husky](/yigitkonur/example-mcp-server-sse/tree/main/.husky \".husky\")\n\n[src](/yigitkonur/example-mcp-server-sse/tree/main/src \"src\")\n\n[src](/yigitkonur/example-mcp-server-sse/tree/main/src \"src\")\n\n[typescript-sdk](/yigitkonur/example-mcp-server-sse/blob/main/typescript-sdk \"typescript-sdk\")\n\n[typescript-sdk](/yigitkonur/example-mcp-server-sse/blob/main/typescript-sdk \"typescript-sdk\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-sse/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-sse/blob/main/.dockerignore \".dockerignore\")\n\n[.env.example](/yigitkonur/example-mcp-server-sse/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/example-mcp-server-sse/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/example-mcp-server-sse/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-sse/blob/main/.gitignore \".gitignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-sse/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-sse/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-sse/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-sse/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-sse/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-sse/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-sse/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-sse/blob/main/Dockerfile \"Dockerfile\")\n\n[README.md](/yigitkonur/example-mcp-server-sse/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/example-mcp-server-sse/blob/main/README.md \"README.md\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-sse/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-sse/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-sse/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-sse/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-sse/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-sse/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/example-mcp-server-sse/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/example-mcp-server-sse/blob/main/package.json \"package.json\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-sse/blob/main/smithery.yaml \"smithery.yaml\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-sse/blob/main/smithery.yaml \"smithery.yaml\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-sse/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-sse/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n**[STDIO](https://github.com/yigitkonur/example-mcp-server-stdio) | [Stateful HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http) | [Stateless HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless) | [SSE](https://github.com/yigitkonur/example-mcp-server-sse)**\n\n* * *\n\n# üéì MCP Stateful HTTP Server (Singleton Pattern) - Educational Reference\n\n[](#-mcp-stateful-http-server-singleton-pattern---educational-reference)\n\n**A Production-Ready Model Context Protocol Server Teaching Singleton Architecture and In-Memory State Best Practices**\n\n[![MCP Version](https://camo.githubusercontent.com/41b162b8e409c1a7944643ef63d3b126a69901d1ae2814c0f471b2c132760fa9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d43502d312e302e302d626c7565)](https://modelcontextprotocol.io) [![TypeScript](https://camo.githubusercontent.com/93c2cf106a4832b140c7fe81c6014c78f3c81470c6e9e2b7cd2220005e72364f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d352e782d626c7565)](https://www.typescriptlang.org/) [![SDK](https://camo.githubusercontent.com/bc35472816981eab04c2dc5730d34dcda5e92a06b5dfa08687404e129af5d2a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53444b2d50726f64756374696f6e25323052656164792d677265656e)](https://github.com/modelcontextprotocol/typescript-sdk) [![Architecture](https://camo.githubusercontent.com/e9ae36b1a0862fcf1f27c7fdaa774b731425378402040389457091c8ff24f79f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4172636869746563747572652d53696e676c65746f6e2532305365727665722d676f6c64)](/yigitkonur/example-mcp-server-sse/blob/main)\n\n_Learn by building a world-class MCP server with a focus on efficiency, clean architecture, and production-grade resilience._\n\n## üéØ Project Goal & Core Concepts\n\n[](#-project-goal--core-concepts)\n\nThis repository is a **deeply educational reference implementation** that demonstrates how to build a production-quality MCP server using the **Stateful Singleton Server** pattern. It is the perfect starting point for creating stateful services that are efficient, robust, and easy to understand.\n\nThrough a fully-functional calculator server, this project will teach you:\n\n1.  **üèóÔ∏è Architecture & Design**: Master the **Singleton Server Pattern**, where a single, shared `McpServer` instance manages all business logic and state, while lightweight, per-session transports handle client connections.\n2.  **‚öôÔ∏è Protocol & Transport Mastery**: Correctly implement the modern **`StreamableHTTPServerTransport`**, using a single `/sse` endpoint to handle the entire connection lifecycle (initialization, commands, and streaming).\n3.  **üõ°Ô∏è Production-Grade Resilience**: Implement non-negotiable production features like **graceful shutdowns** to prevent data loss, robust CORS policies, and a `/health` check endpoint for monitoring.\n4.  **‚ö° State & Resource Management**: Learn to manage session state efficiently using a simple **in-memory map** (`sessionId -> transport`), which is a clean and performant approach for single-node deployments.\n5.  **üö® Protocol-Compliant Error Handling**: Understand the critical difference between generic errors and protocol-aware errors by using **`McpError` with specific `ErrorCode`s** to communicate failures clearly and effectively to clients.\n\n## ü§î When to Use This Architecture\n\n[](#-when-to-use-this-architecture)\n\nThe Singleton Server pattern is a powerful and efficient model. It is the ideal choice for:\n\n-   **Single-Instance Deployments:** Perfect for applications running on a single server or virtual machine where all user sessions are handled by one process.\n-   **Rapid Prototyping:** The simplest way to get a stateful MCP server running without the complexity of an external database or cache.\n-   **Services with Volatile State:** Suitable for applications where session data does not need to persist if the server restarts.\n-   **Foundation for Scalability:** This architecture can be extended with an external state store (like Redis) to support horizontal scaling, as demonstrated in the \"Stateful HTTP\" reference implementation.\n\n## üöÄ Quick Start\n\n[](#-quick-start)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Node.js ‚â• 20.0.0\n-   npm or yarn\n-   A basic understanding of TypeScript, Express.js, and JSON-RPC.\n\n### Installation & Running\n\n[](#installation--running)\n\n# Clone the repository\ngit clone https://github.com/yigitkonur/example-mcp-server-sse\ncd example-mcp-server-sse\n\n# Install dependencies\nnpm install\n\n# Start the server (defaults to port 1923)\nnpm start\n\n### Essential Commands\n\n[](#essential-commands)\n\nnpm run dev        # Development mode with hot-reload (uses tsx)\nnpm run build      # TypeScript compilation to dist/\nnpm run start      # Run the production-ready server\nnpm run typecheck  # TypeScript validation\nnpm run lint       # ESLint validation\nnpm run inspector  # Launch the MCP Inspector for interactive testing\n\n## üìê Architecture Overview\n\n[](#-architecture-overview)\n\n### Key Principles\n\n[](#key-principles)\n\nThis server is built on a set of core principles that define its efficiency and maintainability.\n\n1.  **Singleton Server Core:** One `McpServer` instance containing all tools, resources, and business logic is created at startup. This is memory-efficient and provides a single, authoritative source for application state.\n2.  **Per-Session Transports:** Each connecting client is assigned its own lightweight `StreamableHTTPServerTransport`. These transports are stored in a simple in-memory map, keyed by the session ID.\n3.  **Unified Endpoint:** All MCP communication occurs over a single HTTP endpoint (`/sse`). The SDK's transport layer intelligently routes `POST`, `GET`, and `DELETE` requests internally.\n4.  **Decoupled Logic:** The business logic (defined in the `createCalculatorServer` factory function) is functionally decoupled from the web server transport layer (the Express app), even though they reside in the same `server.ts` file for project simplicity. This separation makes the code easier to reason about and test.\n\n### Architectural Diagram\n\n[](#architectural-diagram)\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      Express HTTP Server            ‚îÇ  ‚Üê API Layer (Single /sse Endpoint)\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ   Middleware (CORS, JSON Parsing)   ‚îÇ  ‚Üê Web Server Configuration\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ   In-Memory Transport Store         ‚îÇ  ‚Üê Simple Session State\n‚îÇ    (Session ID ‚Üí Transport Map)     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ    Singleton MCP Server Core        ‚îÇ  ‚Üê SHARED Business Logic\n‚îÇ  ‚Ä¢ Tools ‚Ä¢ Resources ‚Ä¢ Prompts      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  StreamableHTTP Transport Layer     ‚îÇ  ‚Üê MCP Protocol Engine\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üîß Core Implementation Patterns\n\n[](#-core-implementation-patterns)\n\nThis section highlights the most critical, non-negotiable best practices demonstrated in this server.\n\n### Pattern 1: The Singleton Server Instance\n\n[](#pattern-1-the-singleton-server-instance)\n\n**The Principle:** To ensure shared state (like `calculationHistory`) and efficient memory usage, a single, global `McpServer` instance is created when the application starts. This instance is then shared across all user connections.\n\n**The Implementation:**\n\n// src/server.ts\n\n// The factory function defines all server capabilities.\nfunction createCalculatorServer(): McpServer { /\\* ... all tools ... \\*/ }\n\n// ‚úÖ BEST PRACTICE: Create ONE shared McpServer instance at startup.\nconst sharedMcpServer: McpServer \\= createCalculatorServer();\nconsole.log('\\[Server\\] Shared Calculator MCP Server instance created.');\n\n// This 'sharedMcpServer' will be connected to every new client transport.\\`\\`\\`\n\n### Pattern 2: Per\\-Session Transport Management\n\n\\*\\*The Principle:\\*\\* The main \\`/sse\\` route handler acts as a dispatcher. It checks for a session ID in the request header. If valid, it retrieves the existing transport. If not, it creates a new transport for the new session, connects it to the singleton server, and stores it for future requests. This logic is the heart of stateful session management.\n\n\\*\\*The Implementation:\\*\\*\n\\`\\`\\`typescript\n// src/server.ts\nconst transports: { \\[sessionId: string\\]: StreamableHTTPServerTransport } \\= {};\n\napp.all('/sse', async (req, res) \\=> {\n  const sessionId \\= req.headers\\['mcp-session-id'\\] as string | undefined;\n\n  if (sessionId && transports\\[sessionId\\]) {\n    // Existing Session: Reuse the transport from our in-memory map.\n    // ...\n  } else if (isInitializeRequest(req.body)) {\n    // New Session: Create a new transport for the client.\n    // ...\n    // Store the new transport for future requests.\n    transports\\[newSessionId\\] \\= transport;\n  } else {\n    // Invalid Request: Respond with an HTTP 400/404 error.\n    // ...\n  }\n\n  // Delegate to the correct transport to handle the request.\n  await transport.handleRequest(req, res, req.body);\n});\n\n### Pattern 3: Protocol-Compliant Error Handling\n\n[](#pattern-3-protocol-compliant-error-handling)\n\n**The Principle:** A robust server must clearly distinguish between a server failure and invalid user input. Throwing a generic `Error` is an anti-pattern because it results in a vague \"Internal Server Error\" for the client. The best practice is to throw a specific `McpError` with a standard `ErrorCode`.\n\n**The Implementation:**\n\n// src/server.ts - inside the 'calculate' tool\n\n// ‚ùå ANTI-PATTERN: This hides the true cause of the error from the client.\n// throw new Error('Division by zero is not allowed');\n\n// ‚úÖ BEST PRACTICE: Use McpError with a specific ErrorCode.\n// This tells the client that the user's parameters were invalid, allowing\n// the client application to display a helpful error message to the user.\nif (op \\=== 'divide' && b \\=== 0) {\n  throw new McpError(\n    ErrorCode.InvalidParams, // The user's input was invalid.\n    'Division by zero is not allowed.',\n  );\n}\n\n**Additional Hardening:**\n\n// Type-safe catch blocks treat errors as 'unknown' for maximum safety\n...\n} catch (error: unknown) {\n    const errorMessage \\= error instanceof Error ? error.message : String(error);\n    // Handle safely without assuming error type\n}\n\n### Pattern 4: Production-Ready Graceful Shutdown\n\n[](#pattern-4-production-ready-graceful-shutdown)\n\n**The Principle:** A production server must never be killed abruptly. A graceful shutdown handler ensures that all active connections are properly closed, pending operations are finished, and resources are released before the process exits.\n\n**The Implementation:**\n\n// src/server.ts\nconst httpServer \\= app.listen(PORT /\\* ... \\*/);\n\nconst shutdown \\= () \\=> {\n  // 1. Close all active client transports to notify clients.\n  for (const sessionId in transports) {\n    transports\\[sessionId\\]?.close();\n  }\n\n  // 2. Stop the HTTP server from accepting new connections.\n  httpServer.close(() \\=> {\n    console.log('\\[Server\\] HTTP server closed.');\n    process.exit(0);\n  });\n\n  // 3. Force exit after a timeout to prevent hanging.\n  setTimeout(() \\=> {\n    process.exit(1);\n  }, 5000);\n};\n\nprocess.on('SIGINT', shutdown); // Ctrl+C\nprocess.on('SIGTERM', shutdown); // \\`docker stop\\`\n\n## üß™ Testing & Validation\n\n[](#-testing--validation)\n\n### Health & Metrics\n\n[](#health--metrics)\n\nA `/health` endpoint is included for monitoring and diagnostics.\n\n# Check the server's health and active session count\ncurl http://localhost:1923/health\n\n**Expected Response:**\n\n{\n  \"status\": \"healthy\",\n  \"activeSessions\": 0,\n  \"transport\": \"streamableHttp\",\n  \"uptime\": 15.3,\n  \"memory\": {\n    /\\* ... memory usage details ... \\*/\n  }\n}\n\n### Manual Request (`curl`)\n\n[](#manual-request-curl)\n\nTest the full connection lifecycle using `curl`.\n\n# Terminal 1: Initialize a session and capture the Mcp-Session-Id header.\nSESSION\\_ID=$(curl -si -X POST http://localhost:1923/sse \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{}}' \\\\\n  | grep -i 'Mcp-Session-Id' | awk '{print $2}' | tr -d '\\\\r')\n\necho \"Acquired Session ID: $SESSION\\_ID\"\n\n# Terminal 2: Use the session ID to call a tool.\ncurl -X POST http://localhost:1923/sse \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Mcp-Session-Id: $SESSION\\_ID\" \\\\\n  -d '{\"jsonrpc\": \"2.0\",\"id\": 2,\"method\": \"tools/call\",\"params\": {\"name\": \"calculate\",\"arguments\": {\"op\": \"divide\", \"a\": 10, \"b\": 0}}}'\n\n**Expected Error Response (from the `b: 0` invalid param):**\n\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"error\": { \"code\": \\-32602, \"message\": \"Division by zero is not allowed.\" }\n}\n\n### Interactive Testing with MCP Inspector\n\n[](#interactive-testing-with-mcp-inspector)\n\nUse the official inspector CLI to interactively explore and test all of the server's capabilities.\n\n# This command connects the inspector to your running server.\nnpm run inspector\n\n## üè≠ Deployment & Configuration\n\n[](#-deployment--configuration)\n\n### Configuration\n\n[](#configuration)\n\nThe server is configured using environment variables:\n\nVariable\n\nDescription\n\nDefault\n\n`PORT`\n\nThe port for the HTTP server to listen on.\n\n`1923`\n\n`CORS_ORIGIN`\n\nAllowed origin for CORS requests. **Should be set to a specific domain in production.**\n\n`*`\n\n`SAMPLE_TOOL_NAME`\n\n**(Educational)** Demonstrates dynamic tool registration via environment variables. When set, adds a simple echo tool with the specified name that takes a `value` parameter and returns `test string print: {value}`. This pattern shows how MCP servers can be configured at runtime.\n\nNone\n\n### Deployment\n\n[](#deployment)\n\nThis server is designed for a **single-node deployment**.\n\n-   **State Management:** Because session state is stored in the server's memory, all requests for a given session _must_ be routed to the same server process.\n-   **Scaling:** This architecture does not scale horizontally out-of-the-box. To run multiple instances, you would need a load balancer configured with **\"sticky sessions\"** (session affinity). For true horizontal scaling, see the \"Stateful HTTP\" reference implementation which uses Redis.\n-   **Deployment:** It can be run as a standalone Node.js process or containerized using a `Dockerfile`.\n\n## üõ°Ô∏è Error Handling Philosophy\n\n[](#Ô∏è-error-handling-philosophy)\n\nThis server demonstrates a robust error handling strategy that is critical for production MCP servers:\n\n### User Errors vs. Server Errors\n\n[](#user-errors-vs-server-errors)\n\nA critical distinction is made between invalid user input and true server failures:\n\n-   **`ErrorCode.InvalidParams`**: Thrown when the user provides bad data (e.g., dividing by zero). This tells the client \"you made a mistake.\"\n-   **`ErrorCode.InternalError`**: Thrown for unexpected server-side issues. This tells the client \"we made a mistake.\"\n\n### Implementation Benefits\n\n[](#implementation-benefits)\n\n-   **No Leaked Details:** Errors are wrapped in `McpError` to prevent internal details like stack traces from being sent to the client.\n-   **Clear Client Communication:** Clients receive specific error codes that enable them to provide helpful feedback to users.\n-   **Transport-Level Errors:** The `/sse` endpoint returns specific HTTP 400/404 errors for session-related issues, separating them from tool execution failures.\n\n### Example in Action\n\n[](#example-in-action)\n\nWhen a user attempts division by zero, the server responds with:\n\n{ \"jsonrpc\": \"2.0\", \"error\": { \"code\": \\-32602, \"message\": \"Division by zero is not allowed.\" } }\n\nThis tells the client exactly what went wrong and allows for graceful error handling in the user interface.\n\n## Key Architectural Takeaways\n\n[](#key-architectural-takeaways)\n\n-   **The Singleton Pattern is Efficient:** For single-node deployments, using one server instance with many lightweight transports is highly memory-efficient.\n-   **Decouple Logic from Transport:** Keeping business logic (`createCalculatorServer`) separate from the web framework (`Express`) makes the code cleaner, more testable, and easier to maintain.\n-   **Errors are Part of the Protocol:** Handling errors correctly with `McpError` is not just a detail‚Äîit is a core feature of a robust and reliable server that enables clients to build better user experiences.\n-   **Plan for Production:** Features like graceful shutdowns and health checks are not afterthoughts; they are fundamental requirements for any service that needs to be reliable.\n\n## About\n\nSimple, educational SSE-based MCP server example demonstrating best practices for various protocols.\n\n### Topics\n\n[mcp](/topics/mcp \"Topic: mcp\") [sse](/topics/sse \"Topic: sse\") [server-sent-events](/topics/server-sent-events \"Topic: server-sent-events\") [model-context-protocol](/topics/model-context-protocol \"Topic: model-context-protocol\") [mcp-server](/topics/mcp-server \"Topic: mcp-server\") [mcp-sse](/topics/mcp-sse \"Topic: mcp-sse\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/example-mcp-server-sse/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/example-mcp-server-sse/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/example-mcp-server-sse/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/example-mcp-server-sse/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-sse&report=yigitkonur+%28user%29)\n\n## [Contributors 2](/yigitkonur/example-mcp-server-sse/graphs/contributors)\n\n-    [![@yigitkonur](https://avatars.githubusercontent.com/u/9989650?s=64&v=4)](https://github.com/yigitkonur)[**yigitkonur** Yigit Konur](https://github.com/yigitkonur)\n-    [![@claude](https://avatars.githubusercontent.com/u/81847?s=64&v=4)](https://github.com/claude)[**claude** Claude](https://github.com/claude)\n\n## Languages\n\n-   [JavaScript 93.3%](/yigitkonur/example-mcp-server-sse/search?l=javascript)\n-   [TypeScript 3.6%](/yigitkonur/example-mcp-server-sse/search?l=typescript)\n-   [Dockerfile 3.1%](/yigitkonur/example-mcp-server-sse/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/example-mcp-server-streamable-http\n\nGitHub - yigitkonur/example-mcp-server-streamable-http: Simple, educational Stateful Streamable HTTP-based MCP server example demonstrating best practices for various protocols.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fexample-mcp-server-streamable-http)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[example-mcp-server-streamable-http](/yigitkonur/example-mcp-server-streamable-http)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http)\n    \n\nSimple, educational Stateful Streamable HTTP-based MCP server example demonstrating best practices for various protocols.\n\n[1 star](/yigitkonur/example-mcp-server-streamable-http/stargazers) [0 forks](/yigitkonur/example-mcp-server-streamable-http/forks) [Branches](/yigitkonur/example-mcp-server-streamable-http/branches) [Tags](/yigitkonur/example-mcp-server-streamable-http/tags) [Activity](/yigitkonur/example-mcp-server-streamable-http/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fexample-mcp-server-streamable-http) You must be signed in to change notification settings\n\n# yigitkonur/example-mcp-server-streamable-http\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/example-mcp-server-streamable-http/branches)[Tags](/yigitkonur/example-mcp-server-streamable-http/tags)\n\n[](/yigitkonur/example-mcp-server-streamable-http/branches)[](/yigitkonur/example-mcp-server-streamable-http/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[16 Commits](/yigitkonur/example-mcp-server-streamable-http/commits/main/)\n\n[](/yigitkonur/example-mcp-server-streamable-http/commits/main/)\n\n[.github/workflows](/yigitkonur/example-mcp-server-streamable-http/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.github/workflows](/yigitkonur/example-mcp-server-streamable-http/tree/main/.github/workflows \"This path skips through empty directories\")\n\n[.husky](/yigitkonur/example-mcp-server-streamable-http/tree/main/.husky \".husky\")\n\n[.husky](/yigitkonur/example-mcp-server-streamable-http/tree/main/.husky \".husky\")\n\n[src](/yigitkonur/example-mcp-server-streamable-http/tree/main/src \"src\")\n\n[src](/yigitkonur/example-mcp-server-streamable-http/tree/main/src \"src\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.dockerignore \".dockerignore\")\n\n[.dockerignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.dockerignore \".dockerignore\")\n\n[.env.example](/yigitkonur/example-mcp-server-streamable-http/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/example-mcp-server-streamable-http/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.gitignore \".gitignore\")\n\n[.nvmrc](/yigitkonur/example-mcp-server-streamable-http/blob/main/.nvmrc \".nvmrc\")\n\n[.nvmrc](/yigitkonur/example-mcp-server-streamable-http/blob/main/.nvmrc \".nvmrc\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierignore](/yigitkonur/example-mcp-server-streamable-http/blob/main/.prettierignore \".prettierignore\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[.prettierrc.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/.prettierrc.json \".prettierrc.json\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-streamable-http/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[CLAUDE.md](/yigitkonur/example-mcp-server-streamable-http/blob/main/CLAUDE.md \"CLAUDE.md\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-streamable-http/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/example-mcp-server-streamable-http/blob/main/Dockerfile \"Dockerfile\")\n\n[Makefile](/yigitkonur/example-mcp-server-streamable-http/blob/main/Makefile \"Makefile\")\n\n[Makefile](/yigitkonur/example-mcp-server-streamable-http/blob/main/Makefile \"Makefile\")\n\n[README.md](/yigitkonur/example-mcp-server-streamable-http/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/example-mcp-server-streamable-http/blob/main/README.md \"README.md\")\n\n[docker-compose.dev.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.dev.yml \"docker-compose.dev.yml\")\n\n[docker-compose.dev.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.dev.yml \"docker-compose.dev.yml\")\n\n[docker-compose.prod.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.prod.yml \"docker-compose.prod.yml\")\n\n[docker-compose.prod.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.prod.yml \"docker-compose.prod.yml\")\n\n[docker-compose.secrets.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.secrets.yml \"docker-compose.secrets.yml\")\n\n[docker-compose.secrets.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.secrets.yml \"docker-compose.secrets.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/example-mcp-server-streamable-http/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-streamable-http/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[eslint.config.mjs](/yigitkonur/example-mcp-server-streamable-http/blob/main/eslint.config.mjs \"eslint.config.mjs\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/package-lock.json \"package-lock.json\")\n\n[package-lock.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/package-lock.json \"package-lock.json\")\n\n[package.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/package.json \"package.json\")\n\n[package.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/package.json \"package.json\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-streamable-http/blob/main/smithery.yaml \"smithery.yaml\")\n\n[smithery.yaml](/yigitkonur/example-mcp-server-streamable-http/blob/main/smithery.yaml \"smithery.yaml\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/tsconfig.json \"tsconfig.json\")\n\n[tsconfig.json](/yigitkonur/example-mcp-server-streamable-http/blob/main/tsconfig.json \"tsconfig.json\")\n\nView all files\n\n## Repository files navigation\n\n**[STDIO](https://github.com/yigitkonur/example-mcp-server-stdio) | [Stateful HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http) | [Stateless HTTP](https://github.com/yigitkonur/example-mcp-server-streamable-http-stateless) | [SSE](https://github.com/yigitkonur/example-mcp-server-sse)**\n\n* * *\n\n# üéì MCP Stateful HTTP Streamable Server - Educational Reference\n\n[](#-mcp-stateful-http-streamable-server---educational-reference)\n\n**A Production-Ready Model Context Protocol Server Teaching Hybrid Storage, Distributed Systems, and Resilient Error Handling**\n\n[![MCP Version](https://camo.githubusercontent.com/41b162b8e409c1a7944643ef63d3b126a69901d1ae2814c0f471b2c132760fa9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d43502d312e302e302d626c7565)](https://spec.modelcontextprotocol.io) [![TypeScript](https://camo.githubusercontent.com/93c2cf106a4832b140c7fe81c6014c78f3c81470c6e9e2b7cd2220005e72364f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d352e782d626c7565)](https://www.typescriptlang.org/) [![SDK](https://camo.githubusercontent.com/bc35472816981eab04c2dc5730d34dcda5e92a06b5dfa08687404e129af5d2a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53444b2d50726f64756374696f6e25323052656164792d677265656e)](https://github.com/modelcontextprotocol/typescript-sdk) [![Architecture](https://camo.githubusercontent.com/10b2b79ccb358cd40e3ad208346d4c572dd1d4ab40995bc65fccdad976ff84d5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4172636869746563747572652d526573696c69656e742532304879627269642d676f6c64)](/yigitkonur/example-mcp-server-streamable-http/blob/main)\n\n_Learn by building a world-class, horizontally-scalable MCP server that is robust by design._\n\n## üéØ Project Goal & Core Concepts\n\n[](#-project-goal--core-concepts)\n\nThis repository is a **masterclass in building distributed systems** with the Model Context Protocol. It is a comprehensive reference implementation that demonstrates how to build a robust, scalable, and fault-tolerant MCP server using a **stateful, hybrid-storage architecture**.\n\nThis project is designed to teach five core concepts:\n\n1.  **üèóÔ∏è Clean Architecture**: Master a clean separation of concerns by organizing code into a `types.ts` for data contracts and a `server.ts` for application logic.\n2.  **‚öôÔ∏è Hybrid Storage (Strategy Pattern)**: Implement a system that runs with zero dependencies locally (in-memory) and seamlessly transitions to a distributed architecture using Redis for production.\n3.  **üîí Scalability & Zero-Downtime**: Build a system that scales horizontally and supports zero-downtime deployments by externalizing state and eliminating the need for \"sticky sessions\".\n4.  **‚ö° Advanced State Management**: Learn critical patterns for distributed systems, including **storage abstraction** (`ISessionStore`), **race condition prevention**, and **just-in-time instance reconstruction**.\n5.  üõ°Ô∏è **Resilience & Predictability**: Implement a robust error handling strategy using **specific, typed errors** and a **global error boundary** to build a server that fails gracefully and predictably.\n\n## ü§î When to Use This Architecture\n\n[](#-when-to-use-this-architecture)\n\nThis stateful, distributed architecture is the ideal choice for complex, high-availability applications:\n\n-   **Enterprise Applications:** Systems that require persistent user sessions and must remain available during deployments or node failures.\n-   **Collaborative Tools:** Scenarios where multiple users or agents interact with a shared context that must be centrally managed.\n-   **Multi-Turn Conversational Agents:** Complex chatbots or agents that need to remember the entire history of an interaction to provide coherent responses.\n-   **Any system where losing session state or failing unpredictably is unacceptable.**\n\n## üöÄ Quick Start\n\n[](#-quick-start)\n\nThis server is designed to work in two modes: a simple local mode and a scalable production mode.\n\n### 1\\. Zero-Configuration Local Development\n\n[](#1-zero-configuration-local-development)\n\nRun the server instantly on your machine with zero external dependencies.\n\n# Clone the repository\ngit clone https://github.com/yigitkonur/example-mcp-server-streamable-http\ncd example-mcp-server-streamable-http\n\n# Install dependencies\nnpm install\n\n# Start the server in development mode (uses in-memory storage)\nnpm run dev\n\n# The server starts on port 1453 with the message:\n# ‚úÖ Using In-Memory for single-node state management.\n\n### 2\\. Production Mode with Docker & Redis\n\n[](#2-production-mode-with-docker--redis)\n\nTest the full distributed architecture using the provided Docker Compose setup.\n\n# Make sure Docker is running on your machine\n# This single command starts the server and a Redis instance\ndocker-compose up --build\n\n# The server starts on port 1453 and connects to the Redis container:\n# ‚úÖ Using Redis for distributed state management.\n# INFO: Redis Client Connected\n\n## üìê Architecture Overview\n\n[](#-architecture-overview)\n\n### Code & File Structure\n\n[](#code--file-structure)\n\nThis project follows a clean architecture with a deliberate separation of concerns.\n\n```\nsrc/\n‚îú‚îÄ‚îÄ types.ts    # Data Contracts: Interfaces, Zod Schemas, Custom Errors\n‚îî‚îÄ‚îÄ server.ts   # Application Logic: Storage Impls, Server Factory, HTTP Endpoints\n```\n\n### Key Architectural Principles\n\n[](#key-architectural-principles)\n\n1.  **Storage Abstraction (Strategy Pattern):** The core application logic is decoupled from the storage mechanism (`in-memory` vs. `Redis`) via an `ISessionStore` interface defined in `types.ts`.\n2.  **Stateless Nodes, Stateful System:** Individual server nodes hold only a temporary cache of session objects. The authoritative state lives in a central store (Redis), allowing the system as a whole to be stateful and resilient.\n3.  **Just-in-Time Reconstruction:** Any server node can handle a request for any session ID. If a session is not in a node's local cache, it is reconstructed on-the-fly from the central store. **This eliminates the need for sticky sessions.**\n4.  **Predictable Error Handling:** The server uses a multi-layered error strategy. It throws specific, typed errors for known failure modes (like an invalid session) and uses a global Express error handler as a safety net to catch all unexpected issues, ensuring the client always receives a secure, protocol-compliant error response.\n\n### Architectural Diagrams\n\n[](#architectural-diagrams)\n\n#### Single-Node Mode (Local Development)\n\n[](#single-node-mode-local-development)\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          Express Server                 ‚îÇ\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ ‚îÇ   Global Error Handler (Safety Net) ‚îÇ ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îÇ  Rate Limiting | CORS | Health Checks  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ      In-Memory Session Store            ‚îÇ\n‚îÇ         (Ephemeral Map<id, Data>)       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ   Per-Session MCP Server Instances      ‚îÇ\n‚îÇ     (Cached in an in-memory Map)        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n#### Distributed Mode (Production)\n\n[](#distributed-mode-production)\n\n```\n        Load Balancer (No Sticky Sessions)\n                   |\n    +--------------+--------------+\n    |              |              |\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Server A‚îÇ    ‚îÇ Server B‚îÇ    ‚îÇ Server C‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    |              |              |\n    +--------------+--------------+\n                   |\n     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n     ‚îÇ       Redis Cluster        ‚îÇ\n     ‚îÇ (Authoritative Session &   ‚îÇ\n     ‚îÇ      Event Store)          ‚îÇ\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üîß Core Implementation Patterns\n\n[](#-core-implementation-patterns)\n\nThis section highlights the most important code patterns that define this architecture.\n\n### Pattern 1: Storage Abstraction (`ISessionStore`)\n\n[](#pattern-1-storage-abstraction-isessionstore)\n\n**The Principle:** Code to an interface, not a concrete implementation. This decouples our application logic from the storage technology.\n\n**The Implementation (`src/types.ts`):**\n\n// The contract that any storage backend must adhere to.\nexport interface ISessionStore {\n  get(sessionId: string): Promise<SessionData | null\\>;\n  set(sessionId: string, data: SessionData): Promise<void\\>;\n  // ... and other methods\n}\n\n// Application logic in server.ts only ever interacts with this interface.\n\n### Pattern 2: Just-in-Time Instance Reconstruction\n\n[](#pattern-2-just-in-time-instance-reconstruction)\n\n**The Principle:** To achieve horizontal scalability without sticky sessions, any server node must be able to handle a request for any active session.\n\n**The Implementation (`src/server.ts`):**\n\n// DRY Implementation: Single helper function eliminates code duplication\nasync function getOrCreateInstances(\n  sessionId: string,\n): Promise<{ transport: StreamableHTTPServerTransport; server: McpServer }\\> {\n  // 1. Check high-performance local cache first\n  let instances \\= sessionInstances.get(sessionId);\n  if (instances) return instances;\n\n  // 2. Verify session exists in authoritative persistent store\n  const sessionData \\= await sessionStore.get(sessionId);\n  if (!sessionData) {\n    throw new SessionNotFoundError('Session does not exist or has expired.', { sessionId });\n  }\n\n  // 3. Reconstruct instances from persistent state\n  console.log(\\`Reconstructing instances for session ${sessionId} on this node\\`);\n  // ... reconstruction logic ...\n\n  return instances;\n}\n\n// Used consistently across POST, GET, and DELETE endpoints\nconst instances \\= await getOrCreateInstances(sessionId);\n\n### Pattern 3: Critical Initialization Order\n\n[](#pattern-3-critical-initialization-order)\n\n**The Principle:** To prevent race conditions in a distributed system, the session record must be saved to the persistent store _before_ the `McpServer` instance is created.\n\n**The Implementation (`src/server.ts`):**\n\n// 1. A new session request arrives. Generate a session ID.\nconst newSessionId \\= randomUUID();\n\n// 2. Create the initial session data object.\nconst sessionData \\= createNewSessionData();\n\n// 3. CRITICAL: Persist the session data to Redis/memory FIRST.\nawait sessionStore.set(newSessionId, sessionData);\n\n// 4. NOW it's safe to create the McpServer instance, which may need to read this data.\nconst server \\= await createMCPServer(newSessionId);\n\n### Pattern 4: Resilient & Predictable Error Handling\n\n[](#pattern-4-resilient--predictable-error-handling)\n\n**The Principle:** A robust server fails predictably. We use specific error types for known issues and a global safety net for everything else.\n\n**The Implementation:**\n\n**1\\. Define Custom, Specific Errors (`src/types.ts`):** We create a hierarchy of error classes to represent distinct failure modes.\n\n// A base class for all our application's errors.\nexport class CalculatorServerError extends McpError {\n  /\\* ... \\*/\n}\n\n// A specific error for when a session is not found.\nexport class SessionNotFoundError extends CalculatorServerError {\n  /\\* ... \\*/\n}\n\n// A specific error for when a database/Redis operation fails.\nexport class StorageOperationFailedError extends CalculatorServerError {\n  /\\* ... \\*/\n}\n\n**2\\. Throw Specific Errors in Logic (`src/server.ts`):** Instead of returning generic errors, our code throws these specific types.\n\n// Inside an HTTP handler...\nconst sessionData \\= await sessionStore.get(sessionId);\nif (!sessionData) {\n  // This is a known, predictable failure. Throw the specific error.\n  throw new SessionNotFoundError('Session not found or expired', { sessionId });\n}\n\n**3\\. Complete Error Boundary Coverage (`src/server.ts`):** Every endpoint throws specific errors instead of direct HTTP responses, ensuring 100% coverage by the global handler. This prevents any error from bypassing our safety net.\n\n// All endpoints throw errors instead of sending responses directly\nif (!sessionId) {\n  throw new McpError(ErrorCode.InvalidRequest, 'Mcp-Session-Id header is required');\n}\n\n// Global Express middleware catches ALL errors\napp.use((err: Error, req: Request, res: Response, next: express.NextFunction) \\=> {\n  // 1. Log the full, detailed error for our internal records.\n  console.error('\\[GLOBAL ERROR HANDLER\\] Unhandled error caught:', err);\n\n  // 2. Handle specific error types with proper codes and context\n  let code \\= ErrorCode.InternalError;\n  let message \\= 'An internal server error occurred.';\n  let data: unknown \\= undefined;\n\n  if (err instanceof CalculatorServerError) {\n    code \\= err.code;\n    message \\= err.message;\n    data \\= err.context; // Include contextual information for debugging\n  } else if (err instanceof McpError) {\n    code \\= err.code;\n    message \\= err.message;\n    data \\= err.data;\n  }\n\n  // 3. Always send protocol-compliant JSON-RPC error responses\n  res.status(500).json({ jsonrpc: '2.0', id: rpcId, error: { code, message, data } });\n});\n\n## üìä Features Implemented\n\n[](#-features-implemented)\n\nThis server implements a comprehensive set of capabilities to demonstrate a production-grade system.\n\nFeature\n\nDescription\n\nKey Pattern Demonstrated\n\n**Hybrid Storage**\n\nSwitches between in-memory and Redis via `USE_REDIS` env var.\n\n**Strategy Pattern** and environment-based configuration.\n\n**Persistent History**\n\nCalculation history is saved as part of the session data.\n\n**Stateful Tool Use:** Tools modify session state which is then persisted.\n\n**Gold-Standard Error Handling**\n\nComplete error boundary coverage with typed errors and comprehensive TSDoc documentation.\n\n**Multi-Layered Defense:** Custom error hierarchy + global safety net + contextual error data.\n\n**DRY Code Architecture**\n\nSingle `getOrCreateInstances` helper eliminates reconstruction logic duplication.\n\n**Maintainability:** Critical patterns abstracted into reusable, well-documented functions.\n\n**Health Checks**\n\n`/health` endpoint reports server status, including Redis connectivity.\n\n**Observability:** Providing critical system status for monitoring.\n\n**Prometheus Metrics**\n\n`/metrics` endpoint exposes `mcp_active_sessions` and more.\n\n**Monitoring:** Exposing key performance indicators for a metrics scraper.\n\n**Complete Documentation**\n\nEvery tool, resource, and prompt handler documents exact failure modes with `@throws` annotations.\n\n**Predictable APIs:** Clear contracts for all failure scenarios.\n\n## üß™ Testing & Validation\n\n[](#-testing--validation)\n\n### Health & Metrics\n\n[](#health--metrics)\n\nVerify the server's operational status and view its metrics. The `/health` endpoint is aware of the storage mode.\n\n# Check basic health (works in both modes)\ncurl http://localhost:1453/health\n\n# In Redis mode, a healthy response will include:\n# \"storageMode\": \"redis\", \"redis\": \"ready\"\n\n# Check Prometheus-style metrics\ncurl http://localhost:1453/metrics\n\n### Manual Request (with `curl`)\n\n[](#manual-request-with-curl)\n\nUse `curl` to test the full session lifecycle.\n\n# 1. Initialize a session and capture the Mcp-Session-Id header\nSESSION\\_ID=$(curl -i -X POST http://localhost:1453/mcp \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"1.0.0\"}}' \\\\\n  | grep -i Mcp-Session-Id | awk '{print $2}' | tr -d '\\\\r')\n\necho \"Acquired Session ID: $SESSION\\_ID\"\n\n# 2. Use the session ID to call a tool\ncurl -X POST http://localhost:1453/mcp \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Mcp-Session-Id: $SESSION\\_ID\" \\\\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/call\",\"params\":{\"name\":\"calculate\",\"arguments\":{\"a\":100,\"b\":50,\"op\":\"add\"}}}'\n\n### Interactive Testing with MCP Inspector\n\n[](#interactive-testing-with-mcp-inspector)\n\nUse the official inspector to interactively test the stateful server.\n\n# The inspector will handle the session ID automatically.\nnpx @modelcontextprotocol/inspector --cli http://localhost:1453/mcp\n\n## üè≠ Deployment & Configuration\n\n[](#-deployment--configuration)\n\n### Configuration\n\n[](#configuration)\n\nThe server is configured using environment variables.\n\nVariable\n\nDescription\n\nDefault\n\n`PORT`\n\nThe port for the HTTP server to listen on.\n\n`1453`\n\n`USE_REDIS`\n\n**Set to `true` to enable Redis for distributed state.**\n\n`false`\n\n`REDIS_URL`\n\nThe connection string for the Redis instance.\n\n`redis://localhost:6379`\n\n`LOG_LEVEL`\n\nLogging verbosity (`debug`, `info`, `warn`, `error`).\n\n`info`\n\n`CORS_ORIGIN`\n\nAllowed origin for CORS. Use a specific domain in production.\n\n`*`\n\n`SAMPLE_TOOL_NAME`\n\n**(Educational)** Demonstrates dynamic tool registration via environment variables. When set, adds a simple echo tool with the specified name that takes a `value` parameter and returns `test string print: {value}`. This pattern shows how MCP servers can be configured at runtime.\n\nNone\n\n### Production Deployment\n\n[](#production-deployment)\n\nThis server is designed for high-availability, horizontally-scaled deployments.\n\n-   **Containerization:** The multi-stage `Dockerfile` creates a lean, secure production image. The `docker-compose.yml` file is ready for multi-replica scaling (`docker-compose up --scale mcp-server=4`).\n-   **Load Balancing:** Deploy behind any standard load balancer. **Sticky sessions are not required** due to the \"Just-in-Time Reconstruction\" architecture.\n-   **Zero-Downtime Updates:** Because session state is externalized to Redis, you can perform rolling deployments of new server versions without interrupting or losing active user sessions.\n\n## About\n\nSimple, educational Stateful Streamable HTTP-based MCP server example demonstrating best practices for various protocols.\n\n### Topics\n\n[mcp](/topics/mcp \"Topic: mcp\") [model-context-protocol](/topics/model-context-protocol \"Topic: model-context-protocol\") [mcp-server](/topics/mcp-server \"Topic: mcp-server\") [streamable-http](/topics/streamable-http \"Topic: streamable-http\") [stateful-mcp](/topics/stateful-mcp \"Topic: stateful-mcp\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/example-mcp-server-streamable-http/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/example-mcp-server-streamable-http/stargazers)\n\n### Watchers\n\n[**0** watching](/yigitkonur/example-mcp-server-streamable-http/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/example-mcp-server-streamable-http/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fexample-mcp-server-streamable-http&report=yigitkonur+%28user%29)\n\n## [Contributors 2](/yigitkonur/example-mcp-server-streamable-http/graphs/contributors)\n\n-    [![@yigitkonur](https://avatars.githubusercontent.com/u/9989650?s=64&v=4)](https://github.com/yigitkonur)[**yigitkonur** Yigit Konur](https://github.com/yigitkonur)\n-    [![@claude](https://avatars.githubusercontent.com/u/81847?s=64&v=4)](https://github.com/claude)[**claude** Claude](https://github.com/claude)\n\n## Languages\n\n-   [TypeScript 94.8%](/yigitkonur/example-mcp-server-streamable-http/search?l=typescript)\n-   [Makefile 2.4%](/yigitkonur/example-mcp-server-streamable-http/search?l=makefile)\n-   [Dockerfile 1.6%](/yigitkonur/example-mcp-server-streamable-http/search?l=dockerfile)\n-   [JavaScript 1.2%](/yigitkonur/example-mcp-server-streamable-http/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/dataflow-enhanced-gcs-decompress\n\nGitHub - yigitkonur/dataflow-enhanced-gcs-decompress: This project enhances Google's Bulk Decompress Cloud Storage Files Dataflow template by adding flexible output structure, resumability, and efficient processing while maintaining the original core functionality.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fdataflow-enhanced-gcs-decompress)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[dataflow-enhanced-gcs-decompress](/yigitkonur/dataflow-enhanced-gcs-decompress)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress)\n    \n\nThis project enhances Google's Bulk Decompress Cloud Storage Files Dataflow template by adding flexible output structure, resumability, and efficient processing while maintaining the original core functionality.\n\n[1 star](/yigitkonur/dataflow-enhanced-gcs-decompress/stargazers) [0 forks](/yigitkonur/dataflow-enhanced-gcs-decompress/forks) [Branches](/yigitkonur/dataflow-enhanced-gcs-decompress/branches) [Tags](/yigitkonur/dataflow-enhanced-gcs-decompress/tags) [Activity](/yigitkonur/dataflow-enhanced-gcs-decompress/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress) You must be signed in to change notification settings\n\n# yigitkonur/dataflow-enhanced-gcs-decompress\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/dataflow-enhanced-gcs-decompress/branches)[Tags](/yigitkonur/dataflow-enhanced-gcs-decompress/tags)\n\n[](/yigitkonur/dataflow-enhanced-gcs-decompress/branches)[](/yigitkonur/dataflow-enhanced-gcs-decompress/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[4 Commits](/yigitkonur/dataflow-enhanced-gcs-decompress/commits/main/)\n\n[](/yigitkonur/dataflow-enhanced-gcs-decompress/commits/main/)\n\n[EnhancedBulkCompressor.java](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/EnhancedBulkCompressor.java \"EnhancedBulkCompressor.java\")\n\n[EnhancedBulkCompressor.java](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/EnhancedBulkCompressor.java \"EnhancedBulkCompressor.java\")\n\n[EnhancedBulkCompressor.py](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/EnhancedBulkCompressor.py \"EnhancedBulkCompressor.py\")\n\n[EnhancedBulkCompressor.py](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/EnhancedBulkCompressor.py \"EnhancedBulkCompressor.py\")\n\n[README.md](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/README.md \"README.md\")\n\nView all files\n\n## Repository files navigation\n\n# Enhanced Bulk GCS File Decompressor\n\n[](#enhanced-bulk-gcs-file-decompressor)\n\n## Overview\n\n[](#overview)\n\nThis project is an enhanced version of Google's Bulk Decompress Cloud Storage Files Dataflow template. It builds upon the original template, adding new features while maintaining the core functionality provided by Google.\n\n## Key Enhancements\n\n[](#key-enhancements)\n\n-   **Flexible Output Structure**: Unlike the original template which outputs to a single directory, this version preserves the original directory structure of the input files.\n-   **Resumability**: Implements a check for existing decompressed files, allowing jobs to be resumed without redundant processing.\n-   **Efficient Processing**: Skips already decompressed files, saving time and resources on subsequent runs.\n\n## Original Features (Maintained from Google's Template)\n\n[](#original-features-maintained-from-googles-template)\n\n-   Decompresses files from Google Cloud Storage (GCS)\n-   Supports multiple compression formats: BZIP2, DEFLATE, GZIP\n-   Provides error logging for failed decompression attempts\n\n## Prerequisites\n\n[](#prerequisites)\n\n-   Google Cloud Platform account\n-   Google Cloud SDK installed and configured\n-   Java Development Kit (JDK) 11 or later\n-   Apache Maven\n\n## Setup\n\n[](#setup)\n\n1.  Clone this repository:\n    \n    ```\n    git clone https://github.com/your-username/enhanced-bulk-gcs-decompressor.git\n    cd enhanced-bulk-gcs-decompressor\n    ```\n    \n2.  Set up your Google Cloud project:\n    \n    ```\n    export PROJECT_ID=your-project-id\n    export REGION=your-preferred-region\n    gcloud config set project $PROJECT_ID\n    ```\n    \n3.  Create a staging bucket (if not already existing):\n    \n    ```\n    export STAGING_BUCKET=gs://$PROJECT_ID-dataflow-staging\n    gsutil mb -p $PROJECT_ID -l $REGION $STAGING_BUCKET\n    ```\n    \n\n## Building the Template\n\n[](#building-the-template)\n\nBuild the enhanced Dataflow template using Maven:\n\n```\nmvn clean package -DskipTests -Dexec.mainClass=com.google.cloud.teleport.templates.BulkDecompressor -Dexec.args=\"--runner=DataflowRunner --project=$PROJECT_ID --stagingLocation=$STAGING_BUCKET/staging --templateLocation=$STAGING_BUCKET/templates/BulkDecompressor --region=$REGION\"\n```\n\n## Input File Patterns & Output Structure\n\n[](#input-file-patterns--output-structure)\n\nThis enhanced template supports flexible input file patterns, allowing you to target specific files or directories for decompression. Here are some examples:\n\n1.  Decompress all .gz files in a specific directory:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/path/to/files/*.gz\n    ```\n    \n2.  Decompress files with a specific prefix:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/path/to/files/data_*.gz\n    ```\n    \n3.  Decompress files in a date-based directory structure:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/YYYY/MM/DD/*.gz\n    ```\n    \n    This pattern will match files like `gs://your-input-bucket/2023/05/15/data.gz`.\n    \n4.  Decompress files across multiple subdirectories:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/**/*.gz\n    ```\n    \n    This pattern will recursively match all .gz files in any subdirectory.\n    \n5.  Decompress specific file types across a date range:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/2023/0[1-6]/*/data.json.gz\n    ```\n    \n    This pattern will match `data.json.gz` files in any day of the first six months of 2023.\n    \n6.  Decompress files with multiple extensions:\n    \n    ```\n    inputFilePattern=gs://your-input-bucket/path/to/files/*.{gz,bz2}\n    ```\n    \n    This pattern will match both .gz and .bz2 files.\n    \n\nRemember to enclose your pattern in quotes if it contains special characters when running the job:\n\n```\ngcloud dataflow jobs run bulk-decompress-job \\\n  --gcs-location=$STAGING_BUCKET/templates/BulkDecompressor \\\n  --region=$REGION \\\n  --parameters \\\ninputFilePattern=\"gs://your-input-bucket/2023/0[1-6]/*/data.json.gz\",\\\noutputBucket=gs://your-output-bucket,\\\noutputFailureFile=gs://your-output-bucket/failures.csv\n```\n\n## Running the Job\n\n[](#running-the-job)\n\nExecute the Dataflow job using the following command:\n\n```\ngcloud dataflow jobs run bulk-decompress-job \\\n  --gcs-location=$STAGING_BUCKET/templates/BulkDecompressor \\\n  --region=$REGION \\\n  --parameters \\\ninputFilePattern=gs://your-input-bucket/path/to/files/*/*/*/*.gz,\\\noutputBucket=gs://your-output-bucket,\\\noutputFailureFile=gs://your-output-bucket/failures.csv\n```\n\nReplace `your-input-bucket` and `your-output-bucket` with your actual GCS bucket names. Note that the input file pattern can now include multiple directory levels.\n\n## Monitoring\n\n[](#monitoring)\n\nMonitor your Dataflow job using:\n\n```\ngcloud dataflow jobs list --region=$REGION\n```\n\nOr visit the Google Cloud Console for a visual representation of the job's progress.\n\n## Output\n\n[](#output)\n\nDecompressed files will be written to the specified output bucket, maintaining the original directory structure. For example, an input file at `gs://input-bucket/2023/05/01/data.json.gz` will be decompressed to `gs://output-bucket/2023/05/01/data.json`. Any decompression failures will be logged in the specified failure file.\n\n## Contributing\n\n[](#contributing)\n\nWhile contributions to improve the template are welcome, please be mindful that this is an enhanced version of Google's original work. Significant changes should be considered carefully to maintain compatibility and the spirit of the original template.\n\n## License\n\n[](#license)\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](/yigitkonur/dataflow-enhanced-gcs-decompress/blob/main/LICENSE) file for details. The original template is the work of Google LLC and is subject to their licensing terms.\n\n## Acknowledgments\n\n[](#acknowledgments)\n\nThis project is based on the Bulk Decompress Cloud Storage Files template created by Google. We express our gratitude to the original authors for their valuable work, which served as the foundation for these enhancements.\n\nFor the original template and other Google-provided Dataflow templates, please visit: [Google Cloud Dataflow Templates](https://github.com/GoogleCloudPlatform/DataflowTemplates)\n\n## About\n\nThis project enhances Google's Bulk Decompress Cloud Storage Files Dataflow template by adding flexible output structure, resumability, and efficient processing while maintaining the original core functionality.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/dataflow-enhanced-gcs-decompress/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/dataflow-enhanced-gcs-decompress/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/dataflow-enhanced-gcs-decompress/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/dataflow-enhanced-gcs-decompress/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdataflow-enhanced-gcs-decompress&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/dataflow-enhanced-gcs-decompress/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=dataflow-enhanced-gcs-decompress)\n\nNo packages published  \n\n## Languages\n\n-   [Java 58.4%](/yigitkonur/dataflow-enhanced-gcs-decompress/search?l=java)\n-   [Python 41.6%](/yigitkonur/dataflow-enhanced-gcs-decompress/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/kv-backup-enhanced\n\nGitHub - yigitkonur/kv-backup-enhanced: A script to help you to download 1200 Cloudflare KV records per 5 minute (restricted due to Cloudflare's global rate limit)                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fkv-backup-enhanced)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fkv-backup-enhanced)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fkv-backup-enhanced)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[kv-backup-enhanced](/yigitkonur/kv-backup-enhanced)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fkv-backup-enhanced) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fkv-backup-enhanced)\n-   [Star 2](/login?return_to=%2Fyigitkonur%2Fkv-backup-enhanced)\n    \n\nA script to help you to download 1200 Cloudflare KV records per 5 minute (restricted due to Cloudflare's global rate limit)\n\n### License\n\n[Unlicense license](/yigitkonur/kv-backup-enhanced/blob/main/LICENSE)\n\n[2 stars](/yigitkonur/kv-backup-enhanced/stargazers) [0 forks](/yigitkonur/kv-backup-enhanced/forks) [Branches](/yigitkonur/kv-backup-enhanced/branches) [Tags](/yigitkonur/kv-backup-enhanced/tags) [Activity](/yigitkonur/kv-backup-enhanced/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fkv-backup-enhanced)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fkv-backup-enhanced) You must be signed in to change notification settings\n\n# yigitkonur/kv-backup-enhanced\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/kv-backup-enhanced/branches)[Tags](/yigitkonur/kv-backup-enhanced/tags)\n\n[](/yigitkonur/kv-backup-enhanced/branches)[](/yigitkonur/kv-backup-enhanced/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[2 Commits](/yigitkonur/kv-backup-enhanced/commits/main/)\n\n[](/yigitkonur/kv-backup-enhanced/commits/main/)\n\n[.gitignore](/yigitkonur/kv-backup-enhanced/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/kv-backup-enhanced/blob/main/.gitignore \".gitignore\")\n\n[LICENSE](/yigitkonur/kv-backup-enhanced/blob/main/LICENSE \"LICENSE\")\n\n[LICENSE](/yigitkonur/kv-backup-enhanced/blob/main/LICENSE \"LICENSE\")\n\n[README.md](/yigitkonur/kv-backup-enhanced/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/kv-backup-enhanced/blob/main/README.md \"README.md\")\n\n[backup.py](/yigitkonur/kv-backup-enhanced/blob/main/backup.py \"backup.py\")\n\n[backup.py](/yigitkonur/kv-backup-enhanced/blob/main/backup.py \"backup.py\")\n\nView all files\n\n## Repository files navigation\n\n# Workers KV Backup\n\n[](#workers-kv-backup)\n\nCopy the content of a Workers KV namespace locally with enhanced performance and reliability for backup or data processing needs.\n\n## Credits\n\n[](#credits)\n\nThis project is an enhanced version of the original script created by [xtuc](https://github.com/xtuc/kv-backup/). The original script can be found [here](https://github.com/xtuc/kv-backup/).\n\n## Enhancements\n\n[](#enhancements)\n\nThis enhanced version includes several improvements over the original script:\n\n1.  **Asynchronous Programming**: Utilizes `aiohttp` and `asyncio` for non-blocking operations, significantly improving performance.\n2.  **Rate Limiting**: Implements rate limiting using a semaphore and calculated delay to comply with Cloudflare's rate limits.\n3.  **Retry Mechanism**: Adds exponential backoff for retrying failed requests, improving reliability.\n4.  **Cursor Management**: Saves and loads the cursor to handle large datasets and resume from where it left off in case of interruptions.\n5.  **Graceful Shutdown**: Adds a signal handler to save the cursor on termination, ensuring data integrity.\n6.  **Worker and Producer Pattern**: Separates fetching keys and downloading values into distinct functions for better modularity.\n7.  **Configuration Constants**: Defines constants for configurable parameters like batch size, number of workers, and retry settings.\n8.  **Using `uvloop`**: Sets `uvloop` as the event loop policy for better performance.\n9.  **Improved Argument Parsing**: Enhances argument parsing for better configurability via command-line arguments.\n10.  **Debug Mode**: Adds a debug mode for detailed logging to help with troubleshooting and understanding the script's behavior.\n\n## Usage\n\n[](#usage)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Python 3.6+\n-   `aiohttp` and `uvloop` libraries\n\nYou can install the required libraries using pip:\n\npip install aiohttp uvloop\n\n### Running the Script\n\n[](#running-the-script)\n\npython3 ./backup.py --api\\_token=... --cf\\_account\\_id=... --kv\\_namespace\\_id=...\n\n### Flags\n\n[](#flags)\n\n-   `api_token`: Cloudflare's API token (Permission: Workers KV readonly)\n-   `cf_account_id`: Cloudflare's Account ID\n-   `kv_namespace_id`: Workers KV's namespace ID\n-   `dest`: Optional, backup location (default is `./data`)\n-   `num_workers`: Optional, number of worker processes (default is 8)\n-   `debug`: Optional, enable debug mode for detailed logging\n\n### Example\n\n[](#example)\n\npython3 ./backup.py --api\\_token=your\\_api\\_token --cf\\_account\\_id=your\\_account\\_id --kv\\_namespace\\_id=your\\_namespace\\_id --dest=./backup --num\\_workers=10 --debug\n\n### Debug Mode\n\n[](#debug-mode)\n\nThe debug mode provides detailed logging to help with troubleshooting and understanding the script's behavior. When enabled, it logs additional information such as:\n\n-   When a key already exists and is being skipped.\n-   When a key is being downloaded.\n-   Detailed retry information, including backoff times.\n-   Detailed information about cursor loading and saving.\n-   Any errors encountered during the process.\n\nTo enable debug mode, simply add the `--debug` flag when running the script.\n\n## How It Works\n\n[](#how-it-works)\n\n1.  **Fetch Keys**: The script fetches keys in batches from the Workers KV namespace using the provided API token, account ID, and namespace ID.\n2.  **Download Values**: It downloads the values for each key and saves them to the specified destination directory.\n3.  **Rate Limiting**: Ensures requests stay within Cloudflare's rate limits using a semaphore and calculated delay.\n4.  **Retry Mechanism**: Retries failed requests with exponential backoff in case of rate limiting or other transient errors.\n5.  **Cursor Management**: Saves the cursor to a file after each batch to allow resuming from where it left off.\n6.  **Graceful Shutdown**: Handles termination signals to save the current cursor state before shutting down.\n7.  **Debug Mode**: Provides detailed logging to help with troubleshooting and understanding the script's behavior.\n\n## About\n\nA script to help you to download 1200 Cloudflare KV records per 5 minute (restricted due to Cloudflare's global rate limit)\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### License\n\n[Unlicense license](#Unlicense-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/kv-backup-enhanced/activity)\n\n### Stars\n\n[**2** stars](/yigitkonur/kv-backup-enhanced/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/kv-backup-enhanced/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/kv-backup-enhanced/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fkv-backup-enhanced&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/kv-backup-enhanced/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=kv-backup-enhanced)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/kv-backup-enhanced/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/DuckJSONLyzer\n\nGitHub - yigitkonur/DuckJSONLyzer: Fast JSONL analyzer using DuckDB. Validate data integrity and explore JSON structures at lightning speed thanks to ducks ü¶Ü.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FDuckJSONLyzer)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FDuckJSONLyzer)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2FDuckJSONLyzer)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[DuckJSONLyzer](/yigitkonur/DuckJSONLyzer)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2FDuckJSONLyzer) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2FDuckJSONLyzer)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2FDuckJSONLyzer)\n    \n\nFast JSONL analyzer using DuckDB. Validate data integrity and explore JSON structures at lightning speed thanks to ducks ü¶Ü.\n\n[1 star](/yigitkonur/DuckJSONLyzer/stargazers) [0 forks](/yigitkonur/DuckJSONLyzer/forks) [Branches](/yigitkonur/DuckJSONLyzer/branches) [Tags](/yigitkonur/DuckJSONLyzer/tags) [Activity](/yigitkonur/DuckJSONLyzer/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2FDuckJSONLyzer)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2FDuckJSONLyzer) You must be signed in to change notification settings\n\n# yigitkonur/DuckJSONLyzer\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/DuckJSONLyzer/branches)[Tags](/yigitkonur/DuckJSONLyzer/tags)\n\n[](/yigitkonur/DuckJSONLyzer/branches)[](/yigitkonur/DuckJSONLyzer/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[1 Commit](/yigitkonur/DuckJSONLyzer/commits/main/)\n\n[](/yigitkonur/DuckJSONLyzer/commits/main/)\n\n[README.md](/yigitkonur/DuckJSONLyzer/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/DuckJSONLyzer/blob/main/README.md \"README.md\")\n\n[jsonl\\_analyzer.py](/yigitkonur/DuckJSONLyzer/blob/main/jsonl_analyzer.py \"jsonl_analyzer.py\")\n\n[jsonl\\_analyzer.py](/yigitkonur/DuckJSONLyzer/blob/main/jsonl_analyzer.py \"jsonl_analyzer.py\")\n\nView all files\n\n## Repository files navigation\n\n# DuckJSONLyzer - Universal JSONL Analyzer\n\n[](#duckjsonlyzer---universal-jsonl-analyzer)\n\n## Introduction\n\n[](#introduction)\n\nDuckJSONLyzer is a robust and versatile tool for processing and analyzing JSONL (JSON Lines) files of any structure. DuckJSONLyzer provides valuable insights into the composition and distribution of data within JSONL files, making it an essential tool for data analysts, engineers, and scientists working with JSON-structured data. It is also helpful to analyze cardinality of data to use smart formats of some database features like Clickhouse's LowCardinality.\n\n### Key Features:\n\n[](#key-features)\n\n-   Dynamic schema inference\n-   Flexible field analysis\n-   Configurable output formats (TSV, CSV, JSON)\n-   Scalable processing using DuckDB\n-   Support for nested JSON structures\n\n## Why DuckDB?\n\n[](#why-duckdb)\n\nDuckJSONLyzer leverages DuckDB, an embedded analytical database, for several compelling reasons:\n\n1.  **Performance**: DuckDB is designed for analytical queries and can process large amounts of data quickly, often outperforming traditional SQL databases for read-heavy workloads.\n2.  **Embedded Nature**: As an embedded database, DuckDB doesn't require a separate server process, simplifying deployment and usage.\n3.  **Column-Oriented Storage**: This design is optimal for analytical queries, allowing for efficient aggregations and scans over large datasets.\n4.  **SQL Support**: DuckDB supports a wide range of SQL operations, enabling complex data manipulations and analyses.\n5.  **Memory Efficiency**: DuckDB can handle datasets larger than available RAM through intelligent buffer management and spilling to disk when necessary.\n\nDuckDB can efficiently process gigabytes to terabytes of data, depending on available system resources. For extremely large datasets (multiple terabytes), you may need to consider distributed processing solutions.\n\n## How It Works\n\n[](#how-it-works)\n\n1.  **Schema Inference**: DuckJSONLyzer first analyzes a sample of the input JSONL file to infer the schema, including nested structures.\n2.  **Data Loading**: It processes the entire file in chunks, flattening nested structures and loading the data into a DuckDB table.\n3.  **Report Generation**: Finally, it generates reports for each field, counting the occurrences of each unique value.\n\n## Input Example\n\n[](#input-example)\n\nA JSONL file consists of one JSON object per line. For example:\n\n{\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"hobbies\": \\[\"reading\", \"swimming\"\\]}\n{\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"hobbies\": \\[\"gaming\", \"cooking\"\\]}\n{\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"hobbies\": \\[\"traveling\", \"photography\"\\]}\n\n## Output Example\n\n[](#output-example)\n\nFor the \"age\" field, the output in TSV format might look like:\n\nCount   Value\n2       30\n1       25\n1       35\n\n## Data Integrity\n\n[](#data-integrity)\n\nDuckJSONLyzer helps maintain data integrity by:\n\n1.  **Identifying Inconsistencies**: By analyzing value distributions, it can highlight unexpected values or patterns.\n2.  **Type Inference**: The schema inference process reveals the data types used in each field, helping identify type inconsistencies.\n3.  **Null Value Analysis**: It shows the count of null values for each field, which can indicate data completeness issues.\n4.  **Cardinality Assessment**: The tool helps in understanding the cardinality of each field, which can be crucial for data modeling and query optimization.\n\n## Database Schema Design\n\n[](#database-schema-design)\n\nDuckJSONLyzer is invaluable for database schema design:\n\n1.  **Field Discovery**: It uncovers all fields present in the JSONL data, including nested structures, ensuring no data is overlooked in schema design.\n2.  **Data Type Suggestion**: By inferring data types, it provides a starting point for choosing appropriate database column types.\n3.  **Cardinality Insights**: Understanding the number of unique values in each field helps in deciding on indexing strategies and choosing between normalized and denormalized designs.\n4.  **Nested Structure Handling**: It reveals nested structures in the data, allowing for informed decisions on whether to normalize these structures or store them as JSON/JSONB in supporting databases.\n\n## Usage\n\n[](#usage)\n\npython jsonl\\_analyzer.py \\[OPTIONS\\] INPUT\\_FILE\n\n### Options:\n\n[](#options)\n\n-   `--output-dir, -o`: Directory to save output files (default: current directory)\n-   `--fields, -f`: Fields to generate reports for (default: all fields)\n-   `--top-results`: Limit the number of results in each report\n-   `--db-file`: DuckDB database file (default: in-memory database)\n-   `--chunk-size`: Chunk size for processing JSONL (default: 1000)\n-   `--output-format`: Output format for reports (choices: tsv, csv, json; default: tsv)\n-   `--max-depth`: Maximum depth for nested field analysis\n-   `--dry-run`: Show what would be done without actually processing\n\n## Performance and Scalability\n\n[](#performance-and-scalability)\n\n-   DuckJSONLyzer can handle large JSONL files efficiently due to chunk-based processing and DuckDB's performance.\n-   For very large files, consider increasing the chunk size and using a file-based DuckDB database instead of in-memory processing.\n-   The `max-depth` option can limit processing time for deeply nested structures at the cost of detail in the analysis.\n\n## Best Practices\n\n[](#best-practices)\n\n1.  Start with a small sample of your data to understand the structure and adjust options accordingly.\n2.  Use the `--dry-run` option to preview the operation before processing large files.\n3.  When dealing with large files, use a file-based DuckDB database and adjust the chunk size for optimal performance.\n4.  Utilize the `--fields` option to focus on specific fields of interest in large datasets.\n\n## Troubleshooting\n\n[](#troubleshooting)\n\n-   If you encounter memory issues, try reducing the chunk size or using a file-based DuckDB database.\n-   For errors related to JSON parsing, check your input file for malformed JSON objects.\n-   If certain fields are missing from the analysis, ensure that the `max-depth` is set high enough to capture all nested levels.\n\n## Future Development\n\n[](#future-development)\n\nPotential areas for improvement include:\n\n-   Parallel processing for even faster analysis of large datasets\n-   More advanced statistical analyses of field values\n-   Integration with data visualization tools for graphical reporting\n\n## About\n\nFast JSONL analyzer using DuckDB. Validate data integrity and explore JSON structures at lightning speed thanks to ducks ü¶Ü.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/DuckJSONLyzer/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/DuckJSONLyzer/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/DuckJSONLyzer/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/DuckJSONLyzer/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FDuckJSONLyzer&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/DuckJSONLyzer/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=DuckJSONLyzer)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/DuckJSONLyzer/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/n8n-docker-ffmpeg\n\nGitHub - yigitkonur/n8n-docker-ffmpeg: Repository for setting up n8n with ffmpeg using Docker Compose, including beginner-friendly instructions and systemd service configuration for automatic startup.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-docker-ffmpeg)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-docker-ffmpeg)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fn8n-docker-ffmpeg)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[n8n-docker-ffmpeg](/yigitkonur/n8n-docker-ffmpeg)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-docker-ffmpeg) You must be signed in to change notification settings\n-   [Fork 31](/login?return_to=%2Fyigitkonur%2Fn8n-docker-ffmpeg)\n-   [Star 32](/login?return_to=%2Fyigitkonur%2Fn8n-docker-ffmpeg)\n    \n\nRepository for setting up n8n with ffmpeg using Docker Compose, including beginner-friendly instructions and systemd service configuration for automatic startup.\n\n[32 stars](/yigitkonur/n8n-docker-ffmpeg/stargazers) [31 forks](/yigitkonur/n8n-docker-ffmpeg/forks) [Branches](/yigitkonur/n8n-docker-ffmpeg/branches) [Tags](/yigitkonur/n8n-docker-ffmpeg/tags) [Activity](/yigitkonur/n8n-docker-ffmpeg/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fn8n-docker-ffmpeg)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fn8n-docker-ffmpeg) You must be signed in to change notification settings\n\n# yigitkonur/n8n-docker-ffmpeg\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/n8n-docker-ffmpeg/branches)[Tags](/yigitkonur/n8n-docker-ffmpeg/tags)\n\n[](/yigitkonur/n8n-docker-ffmpeg/branches)[](/yigitkonur/n8n-docker-ffmpeg/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/n8n-docker-ffmpeg/commits/main/)\n\n[](/yigitkonur/n8n-docker-ffmpeg/commits/main/)\n\n[.env](/yigitkonur/n8n-docker-ffmpeg/blob/main/.env \".env\")\n\n[.env](/yigitkonur/n8n-docker-ffmpeg/blob/main/.env \".env\")\n\n[Dockerfile](/yigitkonur/n8n-docker-ffmpeg/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/n8n-docker-ffmpeg/blob/main/Dockerfile \"Dockerfile\")\n\n[README.md](/yigitkonur/n8n-docker-ffmpeg/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/n8n-docker-ffmpeg/blob/main/README.md \"README.md\")\n\n[docker-compose.yml](/yigitkonur/n8n-docker-ffmpeg/blob/main/docker-compose.yml \"docker-compose.yml\")\n\n[docker-compose.yml](/yigitkonur/n8n-docker-ffmpeg/blob/main/docker-compose.yml \"docker-compose.yml\")\n\nView all files\n\n## Repository files navigation\n\n# n8n with ffmpeg and Docker Compose\n\n[](#n8n-with-ffmpeg-and-docker-compose)\n\nThis repository provides a Docker Compose setup for running n8n with ffmpeg support. It also includes instructions for setting up the service to run on system startup using systemd.\n\n## Prerequisites\n\n[](#prerequisites)\n\n-   Docker: [Install Docker](https://docs.docker.com/get-docker/)\n-   Docker Compose: [Install Docker Compose](https://docs.docker.com/compose/install/)\n\n## Setup\n\n[](#setup)\n\n### 1\\. Clone the Repository\n\n[](#1-clone-the-repository)\n\nOpen a terminal and run the following commands:\n\ngit clone https://github.com/yourusername/n8n-docker-ffmpeg.git\ncd n8n-docker-ffmpeg\n\n### 2\\. Create a `.env` File\n\n[](#2-create-a-env-file)\n\nCreate a file named `.env` in the root of the repository and add the following variables:\n\nN8N\\_HOST\\=n8n.local\nN8N\\_PORT\\=5678\nN8N\\_PROTOCOL\\=https\nNODE\\_ENV\\=production\nWEBHOOK\\_URL\\=https://n8n.local/\nGENERIC\\_TIMEZONE\\=America/New\\_York\n\n### 3\\. Build and Run the Containers\n\n[](#3-build-and-run-the-containers)\n\nBuild the Docker containers and start the services:\n\ndocker-compose build --no-cache\ndocker-compose up -d\n\n### 4\\. Access n8n\n\n[](#4-access-n8n)\n\nOpen your browser and go to `https://n8n.local`.\n\n## Setting up as a Systemd Service\n\n[](#setting-up-as-a-systemd-service)\n\nTo ensure the Docker Compose services start on system boot, follow these steps:\n\n### 1\\. Create the Systemd Service File\n\n[](#1-create-the-systemd-service-file)\n\nCreate a new systemd service file:\n\nsudo nano /etc/systemd/system/docker-compose.service\n\nCopy the following content into the file:\n\n\\[Unit\\]\nDescription\\=Docker Compose Service\nAfter\\=network.target docker.service\nRequires\\=docker.service\n\n\\[Service\\]\nType\\=oneshot\nUser\\=root\nWorkingDirectory\\=/path/to/your/n8n-docker-ffmpeg\nExecStart\\=/usr/bin/docker-compose build --no-cache\nExecStartPost\\=/usr/bin/docker-compose up -d\nRemainAfterExit\\=true\n\n\\[Install\\]\nWantedBy\\=multi-user.target\n\n### 2\\. Reload Systemd and Enable the Service\n\n[](#2-reload-systemd-and-enable-the-service)\n\nsudo systemctl daemon-reload\nsudo systemctl enable docker-compose.service\nsudo systemctl start docker-compose.service\n\n### Manual Startup\n\n[](#manual-startup)\n\nIf you prefer to start the services manually, navigate to the project directory and run:\n\ncd /path/to/your/n8n-docker-ffmpeg\ndocker-compose build --no-cache\ndocker-compose up -d\n\n## Volumes\n\n[](#volumes)\n\n-   `caddy_data`: Stores Caddy server data.\n-   `n8n_data`: Stores n8n workflow data.\n\n## Additional Information\n\n[](#additional-information)\n\n-   The `Dockerfile` installs Docker CLI and ffmpeg inside the n8n container to enable additional functionality.\n-   Ensure that the `caddy_config` directory and `Caddyfile` are correctly set up in your project directory.\n\nFor more detailed instructions and troubleshooting, please refer to the official documentation of [Docker](https://docs.docker.com/) and [n8n](https://docs.n8n.io/).\n\n### Using n8n with ffmpeg\n\n[](#using-n8n-with-ffmpeg)\n\nWith ffmpeg installed in your n8n Docker container, you can leverage the power of ffmpeg directly within your n8n workflows. This allows you to process media files as part of your automation sequences.\n\n#### Example: Convert MP4 to MP3\n\n[](#example-convert-mp4-to-mp3)\n\nIn this example, we'll demonstrate how to use the Execute Command node in n8n to convert an MP4 video file to an MP3 audio file.\n\n1.  **Add the Execute Command Node**\n    \n    -   Open your n8n editor and create a new workflow.\n    -   Add an \"Execute Command\" node to your workflow.\n2.  **Configure the Execute Command Node**\n    \n    -   Set the \"Command\" field to the ffmpeg command for converting MP4 to MP3.\n        \n    -   Example command:\n        \n        ffmpeg -i /files/input-video.mp4 -q:a 0 -map a /files/output-audio.mp3\n        \n    -   Here's the detailed configuration:\n        \n        -   **Command**: `ffmpeg`\n        -   **Parameters**: `-i /files/input-video.mp4 -q:a 0 -map a /files/output-audio.mp3`\n3.  **Place Input File and Define Output Location**\n    \n    -   Ensure the input MP4 file (`input-video.mp4`) is placed in the `/files` directory within your n8n container.\n    -   The converted MP3 file (`output-audio.mp3`) will be saved in the same directory.\n4.  **Execute the Workflow**\n    \n    -   Execute the workflow to run the ffmpeg command.\n    -   Check the `/files` directory for the newly created `output-audio.mp3` file.\n\n#### Detailed Steps\n\n[](#detailed-steps)\n\n1.  **Place the Input File**: Copy your MP4 file to the `local_files` directory on your host machine, which maps to `/files` inside the n8n container.\n    \n    cp /path/to/your/input-video.mp4 /path/to/n8n-docker-ffmpeg/local\\_files/\n    \n2.  **Create and Configure the Workflow**: Follow the steps above to create a workflow in n8n and configure the Execute Command node.\n    \n3.  **Run the Workflow**: Execute your workflow in n8n. After the workflow completes, you can find the converted MP3 file in the `local_files` directory.\n    \n\n### Benefits\n\n[](#benefits)\n\n-   **Automation**: Automate media file conversions as part of larger workflows.\n-   **Flexibility**: Use any ffmpeg command within n8n for various media processing tasks.\n-   **Ease of Use**: Simplify media processing tasks without leaving your n8n environment.\n\nBy following these steps, you can easily incorporate media processing into your n8n workflows, leveraging the powerful capabilities of ffmpeg directly within your automation sequences.\n\n## About\n\nRepository for setting up n8n with ffmpeg using Docker Compose, including beginner-friendly instructions and systemd service configuration for automatic startup.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/n8n-docker-ffmpeg/activity)\n\n### Stars\n\n[**32** stars](/yigitkonur/n8n-docker-ffmpeg/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/n8n-docker-ffmpeg/watchers)\n\n### Forks\n\n[**31** forks](/yigitkonur/n8n-docker-ffmpeg/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fn8n-docker-ffmpeg&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/n8n-docker-ffmpeg/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=n8n-docker-ffmpeg)\n\nNo packages published  \n\n## Languages\n\n-   [Dockerfile 100.0%](/yigitkonur/n8n-docker-ffmpeg/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/fastapi-http-proxy-with-caching\n\nGitHub - yigitkonur/fastapi-http-proxy-with-caching: A FastAPI-based HTTP proxy with request caching using Redis, designed to forward requests while caching responses for efficient repeated queries.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ffastapi-http-proxy-with-caching)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ffastapi-http-proxy-with-caching)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Ffastapi-http-proxy-with-caching)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[fastapi-http-proxy-with-caching](/yigitkonur/fastapi-http-proxy-with-caching)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Ffastapi-http-proxy-with-caching) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Ffastapi-http-proxy-with-caching)\n-   [Star 10](/login?return_to=%2Fyigitkonur%2Ffastapi-http-proxy-with-caching)\n    \n\nA FastAPI-based HTTP proxy with request caching using Redis, designed to forward requests while caching responses for efficient repeated queries.\n\n[10 stars](/yigitkonur/fastapi-http-proxy-with-caching/stargazers) [1 fork](/yigitkonur/fastapi-http-proxy-with-caching/forks) [Branches](/yigitkonur/fastapi-http-proxy-with-caching/branches) [Tags](/yigitkonur/fastapi-http-proxy-with-caching/tags) [Activity](/yigitkonur/fastapi-http-proxy-with-caching/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Ffastapi-http-proxy-with-caching)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Ffastapi-http-proxy-with-caching) You must be signed in to change notification settings\n\n# yigitkonur/fastapi-http-proxy-with-caching\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/fastapi-http-proxy-with-caching/branches)[Tags](/yigitkonur/fastapi-http-proxy-with-caching/tags)\n\n[](/yigitkonur/fastapi-http-proxy-with-caching/branches)[](/yigitkonur/fastapi-http-proxy-with-caching/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[5 Commits](/yigitkonur/fastapi-http-proxy-with-caching/commits/main/)\n\n[](/yigitkonur/fastapi-http-proxy-with-caching/commits/main/)\n\n[app](/yigitkonur/fastapi-http-proxy-with-caching/tree/main/app \"app\")\n\n[app](/yigitkonur/fastapi-http-proxy-with-caching/tree/main/app \"app\")\n\n[.env.example](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/.gitignore \".gitignore\")\n\n[Dockerfile](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/Dockerfile \"Dockerfile\")\n\n[Dockerfile](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/Dockerfile \"Dockerfile\")\n\n[README.md](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/README.md \"README.md\")\n\n[main.py](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/main.py \"main.py\")\n\n[main.py](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/main.py \"main.py\")\n\n[requirements.txt](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/fastapi-http-proxy-with-caching/blob/main/requirements.txt \"requirements.txt\")\n\nView all files\n\n## Repository files navigation\n\n# üöÄ FastAPI Transparent Proxy üöÄ\n\n[](#-fastapi-transparent-proxy-)\n\n### Stop paying for duplicate API calls. Start caching like a pro.\n\n[](#stop-paying-for-duplicate-api-calls-start-caching-like-a-pro)\n\n**_The ultimate transparent HTTP proxy for no-code platforms. It sits between your automations and expensive APIs, caching responses based on MD5 hashes so identical requests return instantly._**\n\n[![python](https://camo.githubusercontent.com/4808218212ece46251343f2523af0fdfb557a5bef0edc616a82c5480e8cd2da2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) [![fastapi](https://camo.githubusercontent.com/ee156a544c678f4857fb274b0b51778e314c50fa337186bdd24a90e2c2944220/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d302e3130392b2d3030393638382e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![platform](https://camo.githubusercontent.com/7f42215b2c1c3bcfff02d35ad7de2b7fa485b9259eb48dab60d00d2c4ed72420/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77735f7c5f446f636b65722d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![zero config](https://camo.githubusercontent.com/18476f2f60291c58201bb083d0df13d618ae4896c4bf94d59e6453f40cef1692/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f776974686f75745f72656469732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/18476f2f60291c58201bb083d0df13d618ae4896c4bf94d59e6453f40cef1692/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a99efb88f5f7a65726f5f636f6e6669672d776f726b735f776974686f75745f72656469732d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![n8n ready](https://camo.githubusercontent.com/79b828b4c4884c45f0b7feb67cd85c1db29afff77a093d2fb7616e05873d0ce4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94a75f6e6f2d2d636f64655f72656164792d6e386e5f7c5f4d616b655f7c5f5a61706965722d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/79b828b4c4884c45f0b7feb67cd85c1db29afff77a093d2fb7616e05873d0ce4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94a75f6e6f2d2d636f64655f72656164792d6e386e5f7c5f4d616b655f7c5f5a61706965722d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® Key Features**](#-feature-breakdown-the-secret-sauce) ‚Ä¢ [**üéÆ Usage & Examples**](#-usage-fire-and-forget) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-other-methods)\n\n* * *\n\n**FastAPI Transparent Proxy** is the caching layer your no-code automations wish they had. Stop making the same API calls over and over. This proxy sits between your n8n/Make/Zapier workflows and expensive third-party APIs, returning cached responses for identical requests‚Äîsaving bandwidth, reducing latency, and cutting your API bills.\n\n### üß†\n\n[](#)\n\n**MD5 Deduplication**  \nSame request = same cache key\n\n### ‚ö°\n\n[](#-1)\n\n**Sub-ms Response**  \nCache hits are instant\n\n### üîå\n\n[](#-2)\n\n**Zero Config**  \nWorks without Redis too\n\nHow it slaps:\n\n-   **You:** Point your n8n HTTP Request node to this proxy\n-   **Proxy:** Hashes the request, checks cache, returns or forwards\n-   **Result:** First call hits the API, next 1000 identical calls return instantly\n-   **Your wallet:** üìà\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nManually deduplicating API calls in no-code is a nightmare. This proxy makes other approaches look ancient.\n\n**‚ùå The Old Way (Pain)**\n\n**‚úÖ The Proxy Way (Glory)**\n\n1.  Build complex \"check if already fetched\" logic\n2.  Store results in Airtable/Notion/Sheets\n3.  Add branches: \"if cached then skip\"\n4.  Debug why your workflow is 47 nodes\n5.  Pay for 1000 duplicate API calls anyway\n\n1.  Deploy this proxy (one command)\n2.  Change your API URL to proxy URL\n3.  Done. Caching is automatic.\n4.  Watch your API costs drop 90%\n5.  Go grab a coffee. ‚òï\n\nWe're not just forwarding requests. We're building **deterministic cache keys** from MD5 hashes of `method + URL + headers + body`, so identical business requests always hit the same cache entry‚Äîeven across different workflow runs.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\nPlatform\n\nOne-liner\n\nüê≥ **Docker**\n\n`docker run -p 8000:8000 ghcr.io/yigitkonur/fastapi-proxy`\n\nüêç **Python**\n\n`pip install -r requirements.txt && uvicorn main:app`\n\n‚òÅÔ∏è **Railway/Render**\n\nDeploy from GitHub, set `REDIS_URL` env var\n\n### Quick Install (Python)\n\n[](#quick-install-python)\n\n# Clone and enter\ngit clone https://github.com/yigitkonur/fastapi-http-proxy-with-caching.git\ncd fastapi-http-proxy-with-caching\n\n# Setup virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # Windows: venv\\\\Scripts\\\\activate\npip install -r requirements.txt\n\n# Run (works immediately, even without Redis!)\nuvicorn main:app --host 0.0.0.0 --port 8000\n\n> **‚ú® Zero Config:** The proxy starts in **degraded mode** without Redis‚Äîrequests still work, just without caching. Add Redis when you're ready for the full experience.\n\n* * *\n\n## üéÆ Usage: Fire and Forget\n\n[](#-usage-fire-and-forget)\n\n### Basic Proxy Request\n\n[](#basic-proxy-request)\n\n# Instead of calling the API directly...\ncurl -X POST \"https://expensive-api.com/data\" -d '{\"query\": \"foo\"}'\n\n# Route through the proxy:\ncurl -X POST \"http://localhost:8000/proxy?url=https://expensive-api.com/data\" \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\"query\": \"foo\"}'\n\n### Response Format\n\n[](#response-format)\n\n{\n  \"success\": true,\n  \"cached\": true,\n  \"cache\\_key\": \"a1b2c3d4e5f67890\",\n  \"status\\_code\": 200,\n  \"data\": { \"your\": \"api response\" }\n}\n\nThe `cached: true` means you just saved an API call. üéâ\n\n### In n8n\n\n[](#in-n8n)\n\n1.  Add an **HTTP Request** node\n2.  Set URL to: `http://your-proxy:8000/proxy?url=https://actual-api.com/endpoint`\n3.  Configure method, headers, body as normal\n4.  Every identical request now returns from cache\n\n### Advanced Options\n\n[](#advanced-options)\n\n# Force fresh request (bypass cache)\ncurl \"http://localhost:8000/proxy?url=https://api.com/data&bypass\\_cache=true\"\n\n# Custom cache TTL (2 hours instead of default 1 hour)\ncurl \"http://localhost:8000/proxy?url=https://api.com/data&cache\\_ttl=7200\"\n\n### Health & Admin Endpoints\n\n[](#health--admin-endpoints)\n\n# Health check (great for load balancers)\ncurl http://localhost:8000/health\n# ‚Üí {\"status\": \"healthy\", \"redis\\_connected\": true, \"version\": \"2.0.0\"}\n\n# Cache statistics\ncurl http://localhost:8000/cache/stats\n# ‚Üí {\"total\\_keys\": 1547, \"memory\\_usage\": \"2.3M\", \"prefix\": \"proxy:cache:\"}\n\n# Nuclear option: clear all cache\ncurl -X DELETE http://localhost:8000/cache\n# ‚Üí {\"deleted\": 1547, \"message\": \"Cleared 1547 cached entries\"}\n\n* * *\n\n## ‚ú® Feature Breakdown: The Secret Sauce\n\n[](#-feature-breakdown-the-secret-sauce)\n\nFeature\n\nWhat It Does\n\nWhy You Care\n\n**üß† MD5 Hashing**  \nDeterministic keys\n\nHashes `method + URL + headers + body` into cache key\n\nIdentical requests always return same cached response\n\n**‚ö° Graceful Degradation**  \nNo Redis? No problem\n\nStarts without Redis, just skips caching\n\nDeploy anywhere, add Redis later\n\n**üîÑ All HTTP Methods**  \nNot just POST\n\nGET, POST, PUT, DELETE, PATCH all supported\n\nWorks with any API pattern\n\n**‚è∞ Flexible TTL**  \nPer-request control\n\nDefault 1 hour, override per request\n\nCache static data longer, dynamic shorter\n\n**üéØ Cache Bypass**  \nWhen you need fresh\n\n`bypass_cache=true` skips cache\n\nForce refresh when needed\n\n**üìä Health Checks**  \nProduction ready\n\n`/health` endpoint with Redis status\n\nPerfect for k8s liveness probes\n\n**üîß Legacy Support**  \nDrop-in replacement\n\n`/webhook-test/post-response` still works\n\nMigrate existing workflows gradually\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\nAll settings via environment variables. Copy `.env.example` to `.env`:\n\ncp .env.example .env\n\nVariable\n\nDefault\n\nDescription\n\n`REDIS_URL`\n\n`redis://localhost:6379/0`\n\nRedis connection (or Upstash URL)\n\n`CACHE_TTL_SECONDS`\n\n`3600`\n\nDefault cache lifetime (1 hour)\n\n`CACHE_PREFIX`\n\n`proxy:cache:`\n\nRedis key prefix\n\n`PROXY_TIMEOUT_SECONDS`\n\n`30`\n\nTimeout for proxied requests\n\n`DEBUG`\n\n`false`\n\nEnable verbose logging\n\n### Using Upstash (Serverless Redis)\n\n[](#using-upstash-serverless-redis)\n\n[Upstash](https://upstash.com/) is perfect for this‚Äîpay only for what you use:\n\n1.  Create a database at [console.upstash.com](https://console.upstash.com)\n2.  Copy your Redis URL\n3.  Set in `.env`:\n    \n    ```\n    REDIS_URL=redis://default:YOUR_PASSWORD@YOUR_ENDPOINT.upstash.io:6379\n    ```\n    \n\n**Cost**: ~$0.20 per 100K cached requests. If you're making 1M duplicate calls/month, that's **$2 vs whatever you're paying now**.\n\n* * *\n\n## üèóÔ∏è Project Structure\n\n[](#Ô∏è-project-structure)\n\n```\n‚îú‚îÄ‚îÄ main.py                 # Entry point (thin wrapper)\n‚îú‚îÄ‚îÄ app/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py        # Package metadata\n‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI app factory + lifespan\n‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Pydantic settings from env\n‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Request/response schemas\n‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py    # Service injection\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache.py       # Redis + MD5 hashing logic\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ proxy.py       # HTTP forwarding logic\n‚îÇ   ‚îî‚îÄ‚îÄ routes/\n‚îÇ       ‚îú‚îÄ‚îÄ proxy.py       # /proxy endpoint\n‚îÇ       ‚îî‚îÄ‚îÄ health.py      # /health, /cache/stats\n‚îú‚îÄ‚îÄ requirements.txt       # Pinned dependencies\n‚îú‚îÄ‚îÄ Dockerfile            # Multi-stage production build\n‚îú‚îÄ‚îÄ .env.example          # Configuration template\n‚îî‚îÄ‚îÄ README.md\n```\n\n* * *\n\n## üê≥ Deployment\n\n[](#-deployment)\n\n### Docker (Recommended)\n\n[](#docker-recommended)\n\n# Build\ndocker build -t fastapi-proxy .\n\n# Run (without Redis - degraded mode)\ndocker run -p 8000:8000 fastapi-proxy\n\n# Run with Redis\ndocker run -p 8000:8000 -e REDIS\\_URL=redis://host:6379 fastapi-proxy\n\n### Docker Compose (with Redis)\n\n[](#docker-compose-with-redis)\n\nversion: '3.8'\nservices:\n  proxy:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - REDIS\\_URL=redis://redis:6379/0\n    depends\\_on:\n      - redis\n  redis:\n    image: redis:alpine\n    volumes:\n      - redis\\_data:/data\nvolumes:\n  redis\\_data:\n\n### Systemd (Linux Server)\n\n[](#systemd-linux-server)\n\n\\[Unit\\]\nDescription\\=FastAPI Transparent Proxy\nAfter\\=network.target\n\n\\[Service\\]\nUser\\=www-data\nWorkingDirectory\\=/opt/fastapi-proxy\nEnvironment\\=\"PATH=/opt/fastapi-proxy/venv/bin\"\nExecStart\\=/opt/fastapi-proxy/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8000\nRestart\\=always\n\n\\[Install\\]\nWantedBy\\=multi-user.target\n\n* * *\n\n## üî• Common Issues & Quick Fixes\n\n[](#-common-issues--quick-fixes)\n\n**Expand for troubleshooting tips**\n\nProblem\n\nSolution\n\n**\"Redis unavailable\" warning**\n\nExpected without Redis. The proxy still works, just without caching. Add `REDIS_URL` when ready.\n\n**Cache not working**\n\nCheck `redis_connected: true` in `/health`. Verify your `REDIS_URL` is correct.\n\n**Timeout errors**\n\nIncrease `PROXY_TIMEOUT_SECONDS`. Some APIs are slow.\n\n**Cache key collisions**\n\nShouldn't happen‚ÄîMD5 is deterministic. If you're seeing wrong cached responses, check if you're modifying headers unintentionally.\n\n**High memory usage**\n\nSet `CACHE_TTL_SECONDS` lower, or use `/cache` DELETE endpoint to clear.\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\n# Clone\ngit clone https://github.com/yigitkonur/fastapi-http-proxy-with-caching.git\ncd fastapi-http-proxy-with-caching\n\n# Setup\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Run with hot reload\nuvicorn main:app --reload\n\n# Run tests (coming soon)\npytest\n\n* * *\n\n**Built with üî• because paying for duplicate API calls is a soul-crushing waste of money.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n## About\n\nA FastAPI-based HTTP proxy with request caching using Redis, designed to forward requests while caching responses for efficient repeated queries.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/fastapi-http-proxy-with-caching/activity)\n\n### Stars\n\n[**10** stars](/yigitkonur/fastapi-http-proxy-with-caching/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/fastapi-http-proxy-with-caching/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/fastapi-http-proxy-with-caching/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Ffastapi-http-proxy-with-caching&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/fastapi-http-proxy-with-caching/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=fastapi-http-proxy-with-caching)\n\nNo packages published  \n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [Python 97.0%](/yigitkonur/fastapi-http-proxy-with-caching/search?l=python)\n-   [Dockerfile 3.0%](/yigitkonur/fastapi-http-proxy-with-caching/search?l=dockerfile)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/data-preparation-for-fine-tuning\n\nGitHub - yigitkonur/data-preparation-for-fine-tuning: A Python project for preparing and analyzing datasets from JSONL files. It includes tools for shuffling, categorizing, and generating reports on dataset content.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdata-preparation-for-fine-tuning)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdata-preparation-for-fine-tuning)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fdata-preparation-for-fine-tuning)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[data-preparation-for-fine-tuning](/yigitkonur/data-preparation-for-fine-tuning)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fdata-preparation-for-fine-tuning) You must be signed in to change notification settings\n-   [Fork 2](/login?return_to=%2Fyigitkonur%2Fdata-preparation-for-fine-tuning)\n-   [Star 16](/login?return_to=%2Fyigitkonur%2Fdata-preparation-for-fine-tuning)\n    \n\nA Python project for preparing and analyzing datasets from JSONL files. It includes tools for shuffling, categorizing, and generating reports on dataset content.\n\n[16 stars](/yigitkonur/data-preparation-for-fine-tuning/stargazers) [2 forks](/yigitkonur/data-preparation-for-fine-tuning/forks) [Branches](/yigitkonur/data-preparation-for-fine-tuning/branches) [Tags](/yigitkonur/data-preparation-for-fine-tuning/tags) [Activity](/yigitkonur/data-preparation-for-fine-tuning/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fdata-preparation-for-fine-tuning)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fdata-preparation-for-fine-tuning) You must be signed in to change notification settings\n\n# yigitkonur/data-preparation-for-fine-tuning\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/data-preparation-for-fine-tuning/branches)[Tags](/yigitkonur/data-preparation-for-fine-tuning/tags)\n\n[](/yigitkonur/data-preparation-for-fine-tuning/branches)[](/yigitkonur/data-preparation-for-fine-tuning/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/data-preparation-for-fine-tuning/commits/main/)\n\n[](/yigitkonur/data-preparation-for-fine-tuning/commits/main/)\n\n[README.md](/yigitkonur/data-preparation-for-fine-tuning/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/data-preparation-for-fine-tuning/blob/main/README.md \"README.md\")\n\n[config.ini](/yigitkonur/data-preparation-for-fine-tuning/blob/main/config.ini \"config.ini\")\n\n[config.ini](/yigitkonur/data-preparation-for-fine-tuning/blob/main/config.ini \"config.ini\")\n\n[dataset-chooser.py](/yigitkonur/data-preparation-for-fine-tuning/blob/main/dataset-chooser.py \"dataset-chooser.py\")\n\n[dataset-chooser.py](/yigitkonur/data-preparation-for-fine-tuning/blob/main/dataset-chooser.py \"dataset-chooser.py\")\n\n[dataset-evaluator.py](/yigitkonur/data-preparation-for-fine-tuning/blob/main/dataset-evaluator.py \"dataset-evaluator.py\")\n\n[dataset-evaluator.py](/yigitkonur/data-preparation-for-fine-tuning/blob/main/dataset-evaluator.py \"dataset-evaluator.py\")\n\nView all files\n\n## Repository files navigation\n\n#### Introduction\n\n[](#introduction)\n\nWelcome to the `data-preparation-for-fine-tuning` project, a robust and versatile Python toolkit designed for the meticulous preparation and comprehensive analysis of datasets from JSONL files. Our tools, `dataset-chooser.py` and `dataset-evaluator.py`, are not just scripts but powerful instruments in your data science arsenal. They enable users to homogenize datasets based on pre-specified weights for each category, particularly focusing on assistant responses. This feature is especially beneficial for fine-tuning machine learning models, ensuring the dataset aligns perfectly with your specific needs and biases are minimized.\n\n#### JSONL File Format\n\n[](#jsonl-file-format)\n\nOur scripts work with datasets in JSONL format. Each line in a JSONL file is a valid JSON object. Here's a glimpse of what our dataset might look like:\n\n{\"messages\": \\[{\"role\": \"system\", \"content\": \"Classify...\"}, {\"role\": \"user\", \"content\": \"saas...\"}, {\"role\": \"assistant\", \"content\": \"History\"}\\]}\n{\"messages\": \\[{\"role\": \"system\", \"content\": \"Classify...\"}, {\"role\": \"user\", \"content\": \"diskussionsrunden...\"}, {\"role\": \"assistant\", \"content\": \"Retail\"}\\]}\n{\"messages\": \\[{\"role\": \"system\", \"content\": \"Classify...\"}, {\"role\": \"user\", \"content\": \"polis...\"}, {\"role\": \"assistant\", \"content\": \"Consumer Electronics\"}\\]}\n\n#### Installation and Setup\n\n[](#installation-and-setup)\n\n1.  **Clone the Repository:**\n    \n    ```\n    git clone https://github.com/yourusername/data-preparation-for-fine-tuning.git\n    cd data-preparation-for-fine-tuning\n    ```\n    \n2.  **Dependencies:** Python 3.6+ is required. Install dependencies using:\n    \n    ```\n    pip install pandas rich configparser\n    ```\n    \n\n#### Configuration\n\n[](#configuration)\n\n1.  **config.ini File:** Create this in the root directory. Modify paths and weights to suit your dataset:\n    \n    \\[Paths\\]\n    jsonl\\_directory = /path/to/jsonl/files\n    output\\_file = /path/to/output/dataset.jsonl\n    \n    \\[Weights\\]\n    category\\_weights = {\n        \"Category1\": 0.05,\n        ...\n    }\n    \n    \\[Settings\\]\n    total\\_examples = 1000000\n    \n\n#### Usage\n\n[](#usage)\n\n1.  **Dataset Preparation (`dataset-chooser.py`):** Reads, shuffles, and categorizes JSONL files. Tailor your dataset for specific modeling needs.\n    \n    ```\n    python dataset-chooser.py\n    ```\n    \n2.  **Dataset Analysis (`dataset-evaluator.py`):** Analyzes the prepared dataset, providing insightful metrics and distributions.\n    \n    ```\n    python dataset-evaluator.py\n    ```\n    \n\n[![CleanShot 2024-01-01 at 17 52 05@2x](https://private-user-images.githubusercontent.com/9989650/293600727-ee8bb83e-1ef1-4fb7-a167-ba9098406da6.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NjksIm5iZiI6MTc2NDQ1NDY2OSwicGF0aCI6Ii85OTg5NjUwLzI5MzYwMDcyNy1lZThiYjgzZS0xZWYxLTRmYjctYTE2Ny1iYTkwOTg0MDZkYTYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzQ5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWQ0ZWZhMTI0MjJmNjYxNDhmOGYwNmQyYzU0MzFmNjg5MTI5N2Y3YTVlMWRmNzFjN2UxMjE5ZmE3ODY4ZjBlOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.T9InZ7fm77mYA-INJOGLuYtRzGgog5eJhIRZFHkVfqw)](https://private-user-images.githubusercontent.com/9989650/293600727-ee8bb83e-1ef1-4fb7-a167-ba9098406da6.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NjksIm5iZiI6MTc2NDQ1NDY2OSwicGF0aCI6Ii85OTg5NjUwLzI5MzYwMDcyNy1lZThiYjgzZS0xZWYxLTRmYjctYTE2Ny1iYTkwOTg0MDZkYTYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzQ5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWQ0ZWZhMTI0MjJmNjYxNDhmOGYwNmQyYzU0MzFmNjg5MTI5N2Y3YTVlMWRmNzFjN2UxMjE5ZmE3ODY4ZjBlOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.T9InZ7fm77mYA-INJOGLuYtRzGgog5eJhIRZFHkVfqw)\n\n#### Use Cases\n\n[](#use-cases)\n\n-   **Model Training:** Prepare balanced or weighted datasets for training machine learning models, ensuring diverse representation across categories.\n-   **Data Analysis:** Gain insights into the composition of your datasets, identifying prevalent themes or gaps in data.\n-   **Custom Dataset Creation:** Generate datasets tailored to specific research or business needs, focusing on relevant categories.\n\n#### Fine-Tuning Models with Homogenized Data\n\n[](#fine-tuning-models-with-homogenized-data)\n\nUtilizing `data-preparation-for-fine-tuning`, you can fine-tune machine learning models with data that's been carefully balanced or weighted according to your specifications. This process involves:\n\n1.  Defining category weights in `config.ini` to reflect the desired emphasis in your dataset.\n2.  Running `dataset-chooser.py` to prepare a dataset that adheres to these weights.\n3.  Using the processed dataset to train models, ensuring the data is representative and aligned with your goals.\n\nThis approach is particularly useful in scenarios where certain categories need more representation or when trying to avoid biases inherent in unbalanced datasets.\n\n## About\n\nA Python project for preparing and analyzing datasets from JSONL files. It includes tools for shuffling, categorizing, and generating reports on dataset content.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/data-preparation-for-fine-tuning/activity)\n\n### Stars\n\n[**16** stars](/yigitkonur/data-preparation-for-fine-tuning/stargazers)\n\n### Watchers\n\n[**2** watching](/yigitkonur/data-preparation-for-fine-tuning/watchers)\n\n### Forks\n\n[**2** forks](/yigitkonur/data-preparation-for-fine-tuning/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fdata-preparation-for-fine-tuning&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/data-preparation-for-fine-tuning/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=data-preparation-for-fine-tuning)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/data-preparation-for-fine-tuning/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/GoogleSheets-Translator\n\nGitHub - yigitkonur/GoogleSheets-Translator: Automate translations in Google Sheets using Google Translate, optimized for handling large datasets with efficient batch processing (by using =GOOGLETRANSLATE formula to handle large amount of data effectively)                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGoogleSheets-Translator)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGoogleSheets-Translator)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2FGoogleSheets-Translator)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[GoogleSheets-Translator](/yigitkonur/GoogleSheets-Translator)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2FGoogleSheets-Translator) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2FGoogleSheets-Translator)\n-   [Star 1](/login?return_to=%2Fyigitkonur%2FGoogleSheets-Translator)\n    \n\nAutomate translations in Google Sheets using Google Translate, optimized for handling large datasets with efficient batch processing (by using =GOOGLETRANSLATE formula to handle large amount of data effectively)\n\n[1 star](/yigitkonur/GoogleSheets-Translator/stargazers) [0 forks](/yigitkonur/GoogleSheets-Translator/forks) [Branches](/yigitkonur/GoogleSheets-Translator/branches) [Tags](/yigitkonur/GoogleSheets-Translator/tags) [Activity](/yigitkonur/GoogleSheets-Translator/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2FGoogleSheets-Translator)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2FGoogleSheets-Translator) You must be signed in to change notification settings\n\n# yigitkonur/GoogleSheets-Translator\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/GoogleSheets-Translator/branches)[Tags](/yigitkonur/GoogleSheets-Translator/tags)\n\n[](/yigitkonur/GoogleSheets-Translator/branches)[](/yigitkonur/GoogleSheets-Translator/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[4 Commits](/yigitkonur/GoogleSheets-Translator/commits/main/)\n\n[](/yigitkonur/GoogleSheets-Translator/commits/main/)\n\n[README.md](/yigitkonur/GoogleSheets-Translator/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/GoogleSheets-Translator/blob/main/README.md \"README.md\")\n\n[Translator.gs](/yigitkonur/GoogleSheets-Translator/blob/main/Translator.gs \"Translator.gs\")\n\n[Translator.gs](/yigitkonur/GoogleSheets-Translator/blob/main/Translator.gs \"Translator.gs\")\n\n[new-version.gs](/yigitkonur/GoogleSheets-Translator/blob/main/new-version.gs \"new-version.gs\")\n\n[new-version.gs](/yigitkonur/GoogleSheets-Translator/blob/main/new-version.gs \"new-version.gs\")\n\nView all files\n\n## Repository files navigation\n\n# GoogleSheets-Translator\n\n[](#googlesheets-translator)\n\nAutomate translations in Google Sheets using the Google Translate formula. This script is designed to efficiently handle large datasets, processing translations in batches and updating the sheet with minimal delay.\n\n## Features\n\n[](#features)\n\n-   **Batch Translation**: Processes translations in manageable chunks to avoid spreadsheet performance issues.\n-   **Customizable**: Easily configurable for different source/target languages and columns.\n-   **User-Friendly**: Provides a simple menu in Google Sheets for easy access to the translation functionality.\n-   **Resumable**: Picks up where it left off, making it suitable for very large datasets.\n\n## Setup\n\n[](#setup)\n\n1.  **Open Your Google Sheet**: The sheet where you want to perform translations.\n2.  **Access Apps Script**: Go to `Extensions` > `Apps Script` in the Google Sheets menu.\n3.  **Create a New Script**: Replace any existing code with the contents of `Translator.gs`.\n4.  **Save and Close**: After pasting the code, save the project and close the script editor.\n5.  **Reload the Sheet**: Refresh your Google Sheets tab to see the new 'Translation Tools' menu.\n\n## Usage\n\n[](#usage)\n\n-   Click on the `Translation Tools` menu in your Google Sheet.\n-   Select `Translate Text` to start the translation process.\n-   The script will process translations in batches (default 500 rows at a time).\n-   You can rerun the script from the menu to process additional batches.\n\n## Configuration\n\n[](#configuration)\n\nEdit the `translateText` function in `Translator.gs` to change the configuration:\n\nconst config \\= {\n  sourceColumn: 'E',        // Column containing the original text\n  targetColumn: 'F',        // Column where the translated text will be placed\n  sourceLanguage: 'en',     // Source language (e.g., 'en' for English)\n  targetLanguage: 'pt',     // Target language (e.g., 'pt' for Portuguese)\n  chunkSize: 500,           // Number of rows processed in each batch\n  maxRow: 36774             // Maximum row to process\n};\n\n## About\n\nAutomate translations in Google Sheets using Google Translate, optimized for handling large datasets with efficient batch processing (by using =GOOGLETRANSLATE formula to handle large amount of data effectively)\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/GoogleSheets-Translator/activity)\n\n### Stars\n\n[**1** star](/yigitkonur/GoogleSheets-Translator/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/GoogleSheets-Translator/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/GoogleSheets-Translator/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGoogleSheets-Translator&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/GoogleSheets-Translator/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=GoogleSheets-Translator)\n\nNo packages published  \n\n## Languages\n\n-   [JavaScript 100.0%](/yigitkonur/GoogleSheets-Translator/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/Go-JSON-AzureSearch-Prepper\n\nGitHub - yigitkonur/Go-JSON-AzureSearch-Prepper: A Go utility for processing and combining JSON files, making them ready for integration with Azure Search AI.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2FGo-JSON-AzureSearch-Prepper)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[Go-JSON-AzureSearch-Prepper](/yigitkonur/Go-JSON-AzureSearch-Prepper)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper)\n-   [Star 2](/login?return_to=%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper)\n    \n\nA Go utility for processing and combining JSON files, making them ready for integration with Azure Search AI.\n\n[2 stars](/yigitkonur/Go-JSON-AzureSearch-Prepper/stargazers) [0 forks](/yigitkonur/Go-JSON-AzureSearch-Prepper/forks) [Branches](/yigitkonur/Go-JSON-AzureSearch-Prepper/branches) [Tags](/yigitkonur/Go-JSON-AzureSearch-Prepper/tags) [Activity](/yigitkonur/Go-JSON-AzureSearch-Prepper/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper) You must be signed in to change notification settings\n\n# yigitkonur/Go-JSON-AzureSearch-Prepper\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/Go-JSON-AzureSearch-Prepper/branches)[Tags](/yigitkonur/Go-JSON-AzureSearch-Prepper/tags)\n\n[](/yigitkonur/Go-JSON-AzureSearch-Prepper/branches)[](/yigitkonur/Go-JSON-AzureSearch-Prepper/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/Go-JSON-AzureSearch-Prepper/commits/main/)\n\n[](/yigitkonur/Go-JSON-AzureSearch-Prepper/commits/main/)\n\n[README.md](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/README.md \"README.md\")\n\n[main.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/main.go \"main.go\")\n\n[main.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/main.go \"main.go\")\n\n[processor.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/processor.go \"processor.go\")\n\n[processor.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/processor.go \"processor.go\")\n\n[types.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/types.go \"types.go\")\n\n[types.go](/yigitkonur/Go-JSON-AzureSearch-Prepper/blob/main/types.go \"types.go\")\n\nView all files\n\n## Repository files navigation\n\n# Go-JSON-AzureSearch-Prepper\n\n[](#go-json-azuresearch-prepper)\n\nA robust and efficient Go utility for processing large volumes of JSON files, preparing them for Azure Search AI. This tool handles concurrent processing of multiple JSON files, compiles them into a singular JSON and CSV format, and assigns unique identifiers to each entry, making the data ready for further use in Azure Search AI.\n\n## Features\n\n[](#features)\n\n-   **Concurrent JSON File Processing**: Optimized for handling large datasets with speed and efficiency.\n-   **Unique Identifier Assignment**: Each JSON entry is assigned a unique identifier, ensuring data integrity.\n-   **Customizable Output**: Generates both JSON and CSV outputs, suitable for various use cases including Azure Search AI.\n-   **Progress Tracking**: Includes a progress bar for real-time processing updates.\n-   **Modular Design**: Easily extendable for additional data processing needs.\n\n## Use Cases\n\n[](#use-cases)\n\n-   **Azure Search AI Preparation**: Prepare and aggregate data from multiple JSON files for Azure Search AI.\n-   **Data Transformation**: Transform JSON data into a structured CSV format for analytics and reporting.\n-   **Data Integration**: Integrate and consolidate disparate JSON data sources for unified processing and analysis.\n\n## Getting Started\n\n[](#getting-started)\n\n### Prerequisites\n\n[](#prerequisites)\n\n-   Go (version 1.16 or later)\n-   Basic understanding of Go project structure and modules\n\n### Installation\n\n[](#installation)\n\n1.  Clone the repository:\n    \n    git clone https://github.com/yourusername/Go-JSON-AzureSearch-Prepper.git\n    \n2.  Navigate to the project directory:\n    \n    cd Go-JSON-AzureSearch-Prepper\n    \n3.  Install dependencies:\n    \n    go get -v ./...\n    \n\n### Usage\n\n[](#usage)\n\n1.  Update the `processorConfig` in `main.go` with the paths to your input JSON files, output JSON file, and output CSV file.\n2.  Run the program:\n    \n    go run main.go\n    \n\n## About\n\nA Go utility for processing and combining JSON files, making them ready for integration with Azure Search AI.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/Go-JSON-AzureSearch-Prepper/activity)\n\n### Stars\n\n[**2** stars](/yigitkonur/Go-JSON-AzureSearch-Prepper/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/Go-JSON-AzureSearch-Prepper/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/Go-JSON-AzureSearch-Prepper/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2FGo-JSON-AzureSearch-Prepper&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/Go-JSON-AzureSearch-Prepper/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=Go-JSON-AzureSearch-Prepper)\n\nNo packages published  \n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n-   [Go 100.0%](/yigitkonur/Go-JSON-AzureSearch-Prepper/search?l=go)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/bulk-openai-embeddings-creator\n\nGitHub - yigitkonur/bulk-openai-embeddings-creator: A multi-threaded script & CLI tool for generating embeddings from text using multiple OpenAI endpoints. Supports resuming from previously processed data and customizable thread configurations                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fbulk-openai-embeddings-creator)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fbulk-openai-embeddings-creator)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fbulk-openai-embeddings-creator)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[bulk-openai-embeddings-creator](/yigitkonur/bulk-openai-embeddings-creator)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fbulk-openai-embeddings-creator) You must be signed in to change notification settings\n-   [Fork 1](/login?return_to=%2Fyigitkonur%2Fbulk-openai-embeddings-creator)\n-   [Star 8](/login?return_to=%2Fyigitkonur%2Fbulk-openai-embeddings-creator)\n    \n\nA multi-threaded script & CLI tool for generating embeddings from text using multiple OpenAI endpoints. Supports resuming from previously processed data and customizable thread configurations\n\n[8 stars](/yigitkonur/bulk-openai-embeddings-creator/stargazers) [1 fork](/yigitkonur/bulk-openai-embeddings-creator/forks) [Branches](/yigitkonur/bulk-openai-embeddings-creator/branches) [Tags](/yigitkonur/bulk-openai-embeddings-creator/tags) [Activity](/yigitkonur/bulk-openai-embeddings-creator/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fbulk-openai-embeddings-creator)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fbulk-openai-embeddings-creator) You must be signed in to change notification settings\n\n# yigitkonur/bulk-openai-embeddings-creator\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/bulk-openai-embeddings-creator/branches)[Tags](/yigitkonur/bulk-openai-embeddings-creator/tags)\n\n[](/yigitkonur/bulk-openai-embeddings-creator/branches)[](/yigitkonur/bulk-openai-embeddings-creator/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[7 Commits](/yigitkonur/bulk-openai-embeddings-creator/commits/main/)\n\n[](/yigitkonur/bulk-openai-embeddings-creator/commits/main/)\n\n[README.md](/yigitkonur/bulk-openai-embeddings-creator/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/bulk-openai-embeddings-creator/blob/main/README.md \"README.md\")\n\n[pinecone-pusher.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/pinecone-pusher.py \"pinecone-pusher.py\")\n\n[pinecone-pusher.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/pinecone-pusher.py \"pinecone-pusher.py\")\n\n[run.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/run.py \"run.py\")\n\n[run.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/run.py \"run.py\")\n\n[test.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/test.py \"test.py\")\n\n[test.py](/yigitkonur/bulk-openai-embeddings-creator/blob/main/test.py \"test.py\")\n\nView all files\n\n## Repository files navigation\n\n# Multi-threaded OpenAI Embedding Vector Generator\n\n[](#multi-threaded-openai-embedding-vector-generator)\n\nThis script generates embeddings from text using multiple OpenAI endpoints. It supports multi-threading and can resume from previously processed data. It reads data from Excel, CSV, or JSON files and writes output to TSV, CSV, or JSON files.\n\n## Requirements\n\n[](#requirements)\n\n-   Python 3\n-   pandas\n-   requests\n-   openai\n-   tqdm\n\nYou can install the necessary libraries using pip:\n\npip install pandas requests openai tqdm\n\n## Usage\n\n[](#usage)\n\nYou can run the script from the command line with the following arguments:\n\n-   `--input`: Path to the input file (Excel, CSV, or JSON format).\n-   `--output`: Path to the output file (TSV, CSV, or JSON format).\n-   `--config`: Path to the configuration file (JSON format).\n-   `--threads`: Number of worker threads (default is 50).\n-   `--batch-size`: Number of items to process before saving results (default is 100).\n-   `--retry-limit`: Number of times to retry on error (default is 5).\n\nHere's an example of how to run the script:\n\npython run.py --input input.xlsx --output output.tsv --config config.json --threads 100 --batch-size 200 --retry-limit 3\n\n## Configuration File\n\n[](#configuration-file)\n\nThe configuration file is a JSON file that contains the OpenAI endpoints and keys. It is useful when you need to setup multiple regions to avoid rate limit for millions of docs to get embed.\n\nHere's an example of what the configuration file might look like:\n\n{\n  \"openai\\_endpoints\\_and\\_keys\": \\[\n    \\[\"https://xxx.openai.azure.com\", \"api\\_key\\_1\", \"embed\"\\],\n    \\[\"https://xxx.openai.azure.com\", \"api\\_key\\_2\", \"embed\"\\],\n    \\[\"https://xxx.openai.azure.com\", \"api\\_key\\_3\", \"embed\"\\],\n    \\[\"https://xxx.openai.azure.com\", \"api\\_key\\_4\", \"embed\"\\]\n  \\]\n}\n\n## Input File\n\n[](#input-file)\n\nThe input file should be an Excel, CSV, or JSON file containing the text from which to generate embeddings. The script will dynamically handle the columns present in the input file.\n\n## Output File\n\n[](#output-file)\n\nThe output file will be a TSV, CSV, or JSON file containing the generated embeddings along with the original data. The script will create an additional `vectors` column for the embeddings.\n\n## Progress\n\n[](#progress)\n\nThe script displays a progress bar while processing the documents. It also logs information about the processing status and any errors that occur.\n\n[![CleanShot 2023-10-04 at 11 01 12](https://private-user-images.githubusercontent.com/9989650/272537920-6e0baadd-de70-45b1-af44-928a1a72e261.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NjksIm5iZiI6MTc2NDQ1NDY2OSwicGF0aCI6Ii85OTg5NjUwLzI3MjUzNzkyMC02ZTBiYWFkZC1kZTcwLTQ1YjEtYWY0NC05MjhhMWE3MmUyNjEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzQ5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDk4ZGJiOTFkMDE4YTU0OTdiYmJkM2YxYWJkZjNkY2Y5NDU4OGQ2MDYxYzU5Y2YxNDllNGViZTk5MjE0YWE2ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.aei5uxg153hLoFUJFs155zVV3Yql3xmioeqIu4N2Ims)](https://private-user-images.githubusercontent.com/9989650/272537920-6e0baadd-de70-45b1-af44-928a1a72e261.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NjksIm5iZiI6MTc2NDQ1NDY2OSwicGF0aCI6Ii85OTg5NjUwLzI3MjUzNzkyMC02ZTBiYWFkZC1kZTcwLTQ1YjEtYWY0NC05MjhhMWE3MmUyNjEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzQ5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDk4ZGJiOTFkMDE4YTU0OTdiYmJkM2YxYWJkZjNkY2Y5NDU4OGQ2MDYxYzU5Y2YxNDllNGViZTk5MjE0YWE2ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.aei5uxg153hLoFUJFs155zVV3Yql3xmioeqIu4N2Ims)\n\n## About\n\nA multi-threaded script & CLI tool for generating embeddings from text using multiple OpenAI endpoints. Supports resuming from previously processed data and customizable thread configurations\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/bulk-openai-embeddings-creator/activity)\n\n### Stars\n\n[**8** stars](/yigitkonur/bulk-openai-embeddings-creator/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/bulk-openai-embeddings-creator/watchers)\n\n### Forks\n\n[**1** fork](/yigitkonur/bulk-openai-embeddings-creator/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fbulk-openai-embeddings-creator&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/bulk-openai-embeddings-creator/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=bulk-openai-embeddings-creator)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/bulk-openai-embeddings-creator/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/pineconedb-appscript-integration-for-sheets\n\nGitHub - yigitkonur/pineconedb-appscript-integration-for-sheets: A Google Apps Script custom function to fetch similar categories from a vector database using OpenAI and Pinecone APIs. This function can be used directly in Google Sheets.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[pineconedb-appscript-integration-for-sheets](/yigitkonur/pineconedb-appscript-integration-for-sheets)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets) You must be signed in to change notification settings\n-   [Fork 0](/login?return_to=%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n-   [Star 4](/login?return_to=%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n    \n\nA Google Apps Script custom function to fetch similar categories from a vector database using OpenAI and Pinecone APIs. This function can be used directly in Google Sheets.\n\n[4 stars](/yigitkonur/pineconedb-appscript-integration-for-sheets/stargazers) [0 forks](/yigitkonur/pineconedb-appscript-integration-for-sheets/forks) [Branches](/yigitkonur/pineconedb-appscript-integration-for-sheets/branches) [Tags](/yigitkonur/pineconedb-appscript-integration-for-sheets/tags) [Activity](/yigitkonur/pineconedb-appscript-integration-for-sheets/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets) You must be signed in to change notification settings\n\n# yigitkonur/pineconedb-appscript-integration-for-sheets\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/pineconedb-appscript-integration-for-sheets/branches)[Tags](/yigitkonur/pineconedb-appscript-integration-for-sheets/tags)\n\n[](/yigitkonur/pineconedb-appscript-integration-for-sheets/branches)[](/yigitkonur/pineconedb-appscript-integration-for-sheets/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[4 Commits](/yigitkonur/pineconedb-appscript-integration-for-sheets/commits/main/)\n\n[](/yigitkonur/pineconedb-appscript-integration-for-sheets/commits/main/)\n\n[README.md](/yigitkonur/pineconedb-appscript-integration-for-sheets/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/pineconedb-appscript-integration-for-sheets/blob/main/README.md \"README.md\")\n\n[script.gs](/yigitkonur/pineconedb-appscript-integration-for-sheets/blob/main/script.gs \"script.gs\")\n\n[script.gs](/yigitkonur/pineconedb-appscript-integration-for-sheets/blob/main/script.gs \"script.gs\")\n\nView all files\n\n## Repository files navigation\n\n# Google Sheets VectorDB Integration\n\n[](#google-sheets-vectordb-integration)\n\n## Description\n\n[](#description)\n\nThis Google Sheets custom function enables you to fetch categories similar to a given keyword from a vector database. It employs the OpenAI API to generate a vector for the keyword and the Pinecone API to locate the most similar categories in the database.\n\nAs the AI department at Wope, we needed something like this to test the embeddings we've created üëá\n\n[![CleanShot 2023-10-04 at 17 27 17@2x](https://private-user-images.githubusercontent.com/9989650/272645083-29814a8d-0a62-4edc-970b-944ee1cc7fc3.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0NTA4My0yOTgxNGE4ZC0wYTYyLTRlZGMtOTcwYi05NDRlZTFjYzdmYzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDA5ODkwN2FiOGVmYjUwOWU0YmY2MTlkMWVhZTY5YTA0YmQ1ZGNiNWVmMzcwMDFmMjBmMDIwY2E3YjA2OWQ3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.f5D8-CEAl2LvgRLEv_jrIrnTJlwlV5hb4pWVth1Op9o)](https://private-user-images.githubusercontent.com/9989650/272645083-29814a8d-0a62-4edc-970b-944ee1cc7fc3.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0NTA4My0yOTgxNGE4ZC0wYTYyLTRlZGMtOTcwYi05NDRlZTFjYzdmYzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDA5ODkwN2FiOGVmYjUwOWU0YmY2MTlkMWVhZTY5YTA0YmQ1ZGNiNWVmMzcwMDFmMjBmMDIwY2E3YjA2OWQ3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.f5D8-CEAl2LvgRLEv_jrIrnTJlwlV5hb4pWVth1Op9o)\n\n## Prerequisites\n\n[](#prerequisites)\n\nTo use this function, you will need:\n\n-   A Google account to access Google Sheets.\n-   An OpenAI account to generate vectors for the keywords.\n-   A Pinecone account to find similar categories in the vector database.\n-   API keys for both OpenAI and Pinecone.\n\n## Configuration\n\n[](#configuration)\n\n1.  Open your Google Sheets document.\n2.  Click on `Extensions > Apps Script`.\n3.  Delete any code in the script editor and replace it with the code from the `VectorDB.gs` file in this repository.\n4.  Replace `'Your-OpenAI-API-Key'` and `'Your-Pinecone-API-Key'` with your actual OpenAI and Pinecone API keys.\n5.  Click on `File > Save`. You can name the project as you like, for example, \"VectorDB Integration\".\n6.  Close the Apps Script Editor.\n\n## Usage\n\n[](#usage)\n\nYou can use the `VECTORDB` function just like any other function in Google Sheets.\n\nHere are the parameters you can use:\n\n-   `keyword` (required): The keyword to find similar categories for. This will be a cell reference or a text string.\n-   `categories` (optional): An array of specific categories to return. Can include any categories present in your metadata. If not provided, the function will return all categories.\n-   `numResults` (optional): The number of results to return. Default is 1.\n-   `showScores` (optional): Whether to display the scores. Default is 0 (don't display scores). Set to 1 to display scores.\n-   `lastN` (optional): The number of last parts of the category path to display. If not provided, displays the full path. For example, if the category path is \"Level1 -> Level2 -> Level3\", setting `lastN` to 1 will display \"Level3\", setting `lastN` to 2 will display \"Level2 -> Level3\".\n-   `separator` (optional): The separator used in the category path. Default is ' -> '.\n\nHere are some examples of how to use the function:\n\n[![CleanShot 2023-10-04 at 17 28 48@2x](https://private-user-images.githubusercontent.com/9989650/272645528-9d0ff58f-24c6-4abf-a4c6-5dc56415ca81.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0NTUyOC05ZDBmZjU4Zi0yNGM2LTRhYmYtYTRjNi01ZGM1NjQxNWNhODEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2VkZjUyODAzZjg3MzIyYjY0YWE5OGYxYTAzYzU3NGFhYWFjM2YwZTVjNWE1ZGE0NzQxZjFmYTI3MWJmNjg5MiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.JajGJ-ZKrVdrwOg4e0UC5WGCU2X2SjkoTJerY2gdi5E)](https://private-user-images.githubusercontent.com/9989650/272645528-9d0ff58f-24c6-4abf-a4c6-5dc56415ca81.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0NTUyOC05ZDBmZjU4Zi0yNGM2LTRhYmYtYTRjNi01ZGM1NjQxNWNhODEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2VkZjUyODAzZjg3MzIyYjY0YWE5OGYxYTAzYzU3NGFhYWFjM2YwZTVjNWE1ZGE0NzQxZjFmYTI3MWJmNjg5MiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.JajGJ-ZKrVdrwOg4e0UC5WGCU2X2SjkoTJerY2gdi5E)\n\n-   `=VECTORDB(A2)`: Returns one result with all categories, without scores. Here, `A2` is the cell reference that contains the keyword.\n-   `=VECTORDB(\"keyword\")`: Returns one result with all categories, without scores. Here, `\"keyword\"` is the keyword string.\n-   `=VECTORDB(A2, {\"Category1\"})`: Returns one result with only `Category1`, without scores.\n-   `=VECTORDB(A2, {\"Category1\", \"Category2\"}, 3)`: Returns three results with `Category1` and `Category2`, without scores.\n-   `=VECTORDB(A2, {\"Category1\"}, 1, 1)`: Returns one result with `Category1`, with scores.\n-   `=VECTORDB(A2, {\"Category1\"}, 3, 1, 2)`: Returns three results with `Category1`, with scores, displaying the last two parts of the category path.\n\nWhen the function returns multiple results, each result will be in a separate row by default. If you want each result in a separate column instead, you can use the `TRANSPOSE()` function:\n\n-   `=TRANSPOSE(VECTORDB(A2))`: Returns one result with all categories, without scores, in a separate column.\n-   `=TRANSPOSE(VECTORDB(A2, {\"Category1\", \"Category2\"}, 3))`: Returns three results with `Category1` and `Category2`, without scores, each in a separate column.\n\n[![CleanShot 2023-10-04 at 17 15 20](https://private-user-images.githubusercontent.com/9989650/272642182-69094370-3b22-45f1-b89a-0e80f124967d.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0MjE4Mi02OTA5NDM3MC0zYjIyLTQ1ZjEtYjg5YS0wZTgwZjEyNDk2N2QuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MThiZWE4ZjkyNWU4YTg0NzE3MThhOTE2NzQ0MWJiYzVkNDZhYTQyNzE2ZTJlYTk4ZTg0MGEzNjVmMzgxMWZkZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.zWWNzsm4nmwtRnbL86pfefmXDxJaTzmHB_geJeoo8gE)](https://private-user-images.githubusercontent.com/9989650/272642182-69094370-3b22-45f1-b89a-0e80f124967d.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQ0NTQ5NzAsIm5iZiI6MTc2NDQ1NDY3MCwicGF0aCI6Ii85OTg5NjUwLzI3MjY0MjE4Mi02OTA5NDM3MC0zYjIyLTQ1ZjEtYjg5YS0wZTgwZjEyNDk2N2QuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjlUMjIxNzUwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MThiZWE4ZjkyNWU4YTg0NzE3MThhOTE2NzQ0MWJiYzVkNDZhYTQyNzE2ZTJlYTk4ZTg0MGEzNjVmMzgxMWZkZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.zWWNzsm4nmwtRnbL86pfefmXDxJaTzmHB_geJeoo8gE)\n\n## Support\n\n[](#support)\n\nIf you encounter any issues or have any questions about this function, please open an issue in this GitHub repository.\n\n## License\n\n[](#license)\n\nThis project is licensed under the MIT License. See the [LICENSE](/yigitkonur/pineconedb-appscript-integration-for-sheets/blob/main/LICENSE) file for details.\n\n## About\n\nA Google Apps Script custom function to fetch similar categories from a vector database using OpenAI and Pinecone APIs. This function can be used directly in Google Sheets.\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/pineconedb-appscript-integration-for-sheets/activity)\n\n### Stars\n\n[**4** stars](/yigitkonur/pineconedb-appscript-integration-for-sheets/stargazers)\n\n### Watchers\n\n[**1** watching](/yigitkonur/pineconedb-appscript-integration-for-sheets/watchers)\n\n### Forks\n\n[**0** forks](/yigitkonur/pineconedb-appscript-integration-for-sheets/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fpineconedb-appscript-integration-for-sheets&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/pineconedb-appscript-integration-for-sheets/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=pineconedb-appscript-integration-for-sheets)\n\nNo packages published  \n\n## Languages\n\n-   [JavaScript 100.0%](/yigitkonur/pineconedb-appscript-integration-for-sheets/search?l=javascript)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/context-aware-srt-translation-gpt\n\nGitHub - yigitkonur/context-aware-srt-translation-gpt: A repository trying to translate subtitles with GPT 3.5 Turbo without losing context (using the dynamic window context method).                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fcontext-aware-srt-translation-gpt)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[context-aware-srt-translation-gpt](/yigitkonur/context-aware-srt-translation-gpt)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt) You must be signed in to change notification settings\n-   [Fork 2](/login?return_to=%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt)\n-   [Star 31](/login?return_to=%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt)\n    \n\nA repository trying to translate subtitles with GPT 3.5 Turbo without losing context (using the dynamic window context method).\n\n[31 stars](/yigitkonur/context-aware-srt-translation-gpt/stargazers) [2 forks](/yigitkonur/context-aware-srt-translation-gpt/forks) [Branches](/yigitkonur/context-aware-srt-translation-gpt/branches) [Tags](/yigitkonur/context-aware-srt-translation-gpt/tags) [Activity](/yigitkonur/context-aware-srt-translation-gpt/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt) You must be signed in to change notification settings\n\n# yigitkonur/context-aware-srt-translation-gpt\n\n  \n\n¬†main\n\n[Branches](/yigitkonur/context-aware-srt-translation-gpt/branches)[Tags](/yigitkonur/context-aware-srt-translation-gpt/tags)\n\n[](/yigitkonur/context-aware-srt-translation-gpt/branches)[](/yigitkonur/context-aware-srt-translation-gpt/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[5 Commits](/yigitkonur/context-aware-srt-translation-gpt/commits/main/)\n\n[](/yigitkonur/context-aware-srt-translation-gpt/commits/main/)\n\n[src](/yigitkonur/context-aware-srt-translation-gpt/tree/main/src \"src\")\n\n[src](/yigitkonur/context-aware-srt-translation-gpt/tree/main/src \"src\")\n\n[tests](/yigitkonur/context-aware-srt-translation-gpt/tree/main/tests \"tests\")\n\n[tests](/yigitkonur/context-aware-srt-translation-gpt/tree/main/tests \"tests\")\n\n[.env.example](/yigitkonur/context-aware-srt-translation-gpt/blob/main/.env.example \".env.example\")\n\n[.env.example](/yigitkonur/context-aware-srt-translation-gpt/blob/main/.env.example \".env.example\")\n\n[.gitignore](/yigitkonur/context-aware-srt-translation-gpt/blob/main/.gitignore \".gitignore\")\n\n[.gitignore](/yigitkonur/context-aware-srt-translation-gpt/blob/main/.gitignore \".gitignore\")\n\n[README.md](/yigitkonur/context-aware-srt-translation-gpt/blob/main/README.md \"README.md\")\n\n[README.md](/yigitkonur/context-aware-srt-translation-gpt/blob/main/README.md \"README.md\")\n\n[requirements.txt](/yigitkonur/context-aware-srt-translation-gpt/blob/main/requirements.txt \"requirements.txt\")\n\n[requirements.txt](/yigitkonur/context-aware-srt-translation-gpt/blob/main/requirements.txt \"requirements.txt\")\n\n[run.py](/yigitkonur/context-aware-srt-translation-gpt/blob/main/run.py \"run.py\")\n\n[run.py](/yigitkonur/context-aware-srt-translation-gpt/blob/main/run.py \"run.py\")\n\nView all files\n\n## Repository files navigation\n\n# üé¨ context-aware-srt-translation üé¨\n\n[](#-context-aware-srt-translation-)\n\n### Stop translating subtitles line by line. Start shipping natural translations.\n\n[](#stop-translating-subtitles-line-by-line-start-shipping-natural-translations)\n\n**_The smarter subtitle translator. It reads your SRT, groups sequential lines for context, and uses GPT to produce translations that actually sound human._**\n\n[![python](https://camo.githubusercontent.com/4808218212ece46251343f2523af0fdfb557a5bef0edc616a82c5480e8cd2da2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3444383745362e7376673f7374796c653d666c61742d737175617265)](#) [![fastapi](https://camo.githubusercontent.com/ee156a544c678f4857fb274b0b51778e314c50fa337186bdd24a90e2c2944220/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d302e3130392b2d3030393638382e7376673f7374796c653d666c61742d737175617265)](#) ¬†¬†‚Ä¢¬†¬† [![license](https://camo.githubusercontent.com/b9394454fa0b691582af19a29fc9d773de827bf10e109544bf7b49f16e43032f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d4639413832352e7376673f7374796c653d666c61742d737175617265)](#) [![platform](https://camo.githubusercontent.com/be90eb7ad01a3a68618f2facaf2ae9ab2a0550276a4362e1a5167ad90411858e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f535f7c5f4c696e75785f7c5f57696e646f77732d3245443537332e7376673f7374796c653d666c61742d737175617265)](#)\n\n[![context window](https://camo.githubusercontent.com/df43c92e4a2c1fe6e247784191ef1a7b9bd407fe6908d866d5adf2c2ff5a0563/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f636f6e746578745f77696e646f772d67726f7570735f335f6c696e65735f61745f6f6e63652d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/df43c92e4a2c1fe6e247784191ef1a7b9bd407fe6908d866d5adf2c2ff5a0563/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa7a05f636f6e746578745f77696e646f772d67726f7570735f335f6c696e65735f61745f6f6e63652d3245443537332e7376673f7374796c653d666f722d7468652d6261646765) [![auto fallback](https://camo.githubusercontent.com/6c1866348b314cd12cd769cf0c3f9b19b46e8eabc33eecccd7d483b83cea56f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94845f6175746f5f66616c6c6261636b2d4f70656e41495fe286925f446565704c2d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/6c1866348b314cd12cd769cf0c3f9b19b46e8eabc33eecccd7d483b83cea56f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94845f6175746f5f66616c6c6261636b2d4f70656e41495fe286925f446565704c2d3245443537332e7376673f7374796c653d666f722d7468652d6261646765)\n\n### üß≠ Quick Navigation\n\n[](#-quick-navigation)\n\n[**‚ö° Get Started**](#-get-started-in-60-seconds) ‚Ä¢ [**‚ú® How It Works**](#-how-context-windows-work) ‚Ä¢ [**üéÆ API Usage**](#-api-usage) ‚Ä¢ [**‚öôÔ∏è Configuration**](#%EF%B8%8F-configuration) ‚Ä¢ [**üÜö Why This Slaps**](#-why-this-slaps-other-methods)\n\n* * *\n\n**context-aware-srt-translation** is the translator your subtitles deserve. Stop feeding GPT one line at a time and getting robotic, disconnected results. This service groups sequential subtitle lines together, giving the AI the context it needs to understand the conversation and produce translations that actually flow naturally.\n\n### üß†\n\n[](#)\n\n**Context Windows**  \n3 lines translated together\n\n### ‚ö°\n\n[](#-1)\n\n**Concurrent Processing**  \nParallel chunk translation\n\n### üîÑ\n\n[](#-2)\n\n**Auto Fallback**  \nOpenAI ‚Üí DeepL seamlessly\n\nHow it works:\n\n-   **You:** POST your SRT file to the API\n-   **Service:** Groups lines into context windows, translates concurrently\n-   **Result:** Natural translations that respect conversational flow\n-   **Bonus:** Full statistics on what happened\n\n* * *\n\n## üí• Why This Slaps Other Methods\n\n[](#-why-this-slaps-other-methods)\n\nLine-by-line translation is a vibe-killer. Context windows make other methods look ancient.\n\n**‚ùå Line-by-Line (Pain)**\n\n**‚úÖ Context Windows (Glory)**\n\n\"I think we should...\"  ‚Üí  \"Sanƒ±rƒ±m biz...\"\n\"...go there tomorrow\"  ‚Üí  \"...yarƒ±n oraya git\"\n\nDisconnected. Robotic. Wrong verb forms.\n\n\\[\"I think we should...\",\n \"...go there tomorrow\"\\]  ‚Üí  \n\\[\"Bence yarƒ±n oraya...\",\n \"...gitmeliyiz\"\\]\n\nConnected. Natural. Correct grammar.\n\nThe difference is **context**. When GPT sees the full thought, it understands the sentence structure, maintains speaker tone, and produces translations humans would actually write.\n\n* * *\n\n## üöÄ Get Started in 60 Seconds\n\n[](#-get-started-in-60-seconds)\n\n### 1\\. Clone & Install\n\n[](#1-clone--install)\n\ngit clone https://github.com/yigitkonur/context-aware-srt-translation-gpt.git\ncd context-aware-srt-translation-gpt\npython3 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n### 2\\. Configure\n\n[](#2-configure)\n\ncp .env.example .env\n# Add your OpenAI API key (required)\n# Add DeepL API key (optional fallback)\n\n### 3\\. Run\n\n[](#3-run)\n\npython run.py\n\nThe API is now live at `http://localhost:8000` üéâ\n\n* * *\n\n## üß† How Context Windows Work\n\n[](#-how-context-windows-work)\n\nInstead of translating each subtitle line individually (which loses context), this service groups sequential lines:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Traditional: Line 1 ‚Üí Translate ‚Üí Output 1    ‚îÇ\n‚îÇ               Line 2 ‚Üí Translate ‚Üí Output 2    ‚îÇ\n‚îÇ               Line 3 ‚Üí Translate ‚Üí Output 3    ‚îÇ\n‚îÇ               ‚ùå No context between lines       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Context Window:                                ‚îÇ\n‚îÇ  [Line 1, Line 2, Line 3] ‚Üí Translate Together  ‚îÇ\n‚îÇ               ‚Üì                                 ‚îÇ\n‚îÇ  [Output 1, Output 2, Output 3]                 ‚îÇ\n‚îÇ               ‚úÖ AI sees the full picture       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\nThis allows GPT to:\n\n-   **Maintain speaker continuity** ‚Äî Same character, same voice\n-   **Preserve conversation flow** ‚Äî Questions match answers\n-   **Handle split sentences** ‚Äî \"I think...\" + \"...we should go\" = coherent thought\n-   **Respect cultural context** ‚Äî Idioms translated appropriately\n\n* * *\n\n## üéÆ API Usage\n\n[](#-api-usage)\n\n### Translate Subtitles\n\n[](#translate-subtitles)\n\ncurl -X POST \"http://localhost:8000/subtitle-translate\" \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\n    \"srt\\_content\": \"1\\\\n00:00:01,000 --> 00:00:04,000\\\\nHello, how are you?\\\\n\\\\n2\\\\n00:00:05,000 --> 00:00:08,000\\\\nI am doing great, thanks!\",\n    \"source\\_language\": \"en\",\n    \"target\\_language\": \"tr\"\n  }'\n\n### Response\n\n[](#response)\n\n{\n  \"translated\\_srt\\_content\": \"1\\\\n00:00:01,000 --> 00:00:04,000\\\\nMerhaba, nasƒ±lsƒ±n?\\\\n\\\\n2\\\\n00:00:05,000 --> 00:00:08,000\\\\n√áok iyiyim, te≈üekk√ºrler!\",\n  \"status\": \"success\",\n  \"error\\_message\": null,\n  \"stats\": {\n    \"total\\_sentences\": 2,\n    \"translated\\_sentences\": 2,\n    \"failed\\_sentences\": 0,\n    \"success\\_rate\": 100.0,\n    \"openai\\_calls\": 1,\n    \"deepl\\_calls\": 0,\n    \"elapsed\\_seconds\": 1.23\n  }\n}\n\n### Health Check\n\n[](#health-check)\n\ncurl http://localhost:8000/health\n# {\"status\": \"healthy\", \"version\": \"2.0.0\"}\n\n* * *\n\n## ‚öôÔ∏è Configuration\n\n[](#Ô∏è-configuration)\n\nAll settings via environment variables:\n\nVariable\n\nDefault\n\nDescription\n\n`OPENAI_API_KEY`\n\n‚Äî\n\n**Required.** Your OpenAI API key\n\n`DEEPL_API_KEY`\n\n‚Äî\n\nOptional fallback service\n\n`OPENAI_MODEL`\n\n`gpt-4o-mini`\n\nModel for translations\n\n`OPENAI_TEMPERATURE`\n\n`0.3`\n\nLower = more consistent\n\n`CONTEXT_WINDOW_SIZE`\n\n`3`\n\nLines per translation chunk\n\n`MAX_CONCURRENT_REQUESTS`\n\n`10`\n\nParallel API calls\n\n`LOG_LEVEL`\n\n`INFO`\n\nLogging verbosity\n\n* * *\n\n## üìÅ Project Structure\n\n[](#-project-structure)\n\n```\nsrc/\n‚îú‚îÄ‚îÄ config.py              # Environment configuration\n‚îú‚îÄ‚îÄ models.py              # Pydantic request/response models\n‚îú‚îÄ‚îÄ srt_parser.py          # SRT parsing & reconstruction\n‚îú‚îÄ‚îÄ translator.py          # Main orchestration logic\n‚îú‚îÄ‚îÄ main.py                # FastAPI application\n‚îî‚îÄ‚îÄ services/\n    ‚îú‚îÄ‚îÄ base.py            # Service interface\n    ‚îú‚îÄ‚îÄ openai_service.py  # OpenAI implementation\n    ‚îî‚îÄ‚îÄ deepl_service.py   # DeepL fallback\n```\n\n* * *\n\n## üî• API Documentation\n\n[](#-api-documentation)\n\nInteractive docs available when running:\n\n-   **Swagger UI:** `http://localhost:8000/docs`\n-   **ReDoc:** `http://localhost:8000/redoc`\n\n* * *\n\n## üõ†Ô∏è Development\n\n[](#Ô∏è-development)\n\n# Setup\npython3 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n# Run tests\npytest tests/ -v\n\n# Run with hot reload\npython run.py\n\n* * *\n\n## üî• Common Issues\n\n[](#-common-issues)\n\nProblem\n\nSolution\n\n**OpenAI rate limit**\n\nReduce `MAX_CONCURRENT_REQUESTS`\n\n**DeepL not working**\n\nCheck `DEEPL_API_KEY` is set correctly\n\n**Translations cut off**\n\nIncrease `OPENAI_MAX_TOKENS`\n\n**Wrong language codes**\n\nUse ISO 639-1 codes: `en`, `tr`, `de`, `fr`, etc.\n\n* * *\n\n**Built with üî• because line-by-line subtitle translation is a crime against cinema.**\n\nMIT ¬© [Yiƒüit Konur](https://github.com/yigitkonur)\n\n## About\n\nA repository trying to translate subtitles with GPT 3.5 Turbo without losing context (using the dynamic window context method).\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/context-aware-srt-translation-gpt/activity)\n\n### Stars\n\n[**31** stars](/yigitkonur/context-aware-srt-translation-gpt/stargazers)\n\n### Watchers\n\n[**6** watching](/yigitkonur/context-aware-srt-translation-gpt/watchers)\n\n### Forks\n\n[**2** forks](/yigitkonur/context-aware-srt-translation-gpt/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fcontext-aware-srt-translation-gpt&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/context-aware-srt-translation-gpt/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=context-aware-srt-translation-gpt)\n\nNo packages published  \n\n## Languages\n\n-   [Python 100.0%](/yigitkonur/context-aware-srt-translation-gpt/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n---\n\n## https://github.com/yigitkonur/write-into-menubar\n\nGitHub - yigitkonur/write-into-menubar: Write whatever you want into OSX menubar by using BitBar + Alfred Powerpack.                                             \n\n[Skip to content](#start-of-content)   \n\n## Navigation Menu\n\nToggle navigation\n\n[](/)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fwrite-into-menubar)\n\nAppearance settings\n\n  \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch\n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n# Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\n Include my email address so I can be contacted\n\nCancel Submit feedback\n\n# Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName  \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\nCancel Create saved search\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fwrite-into-menubar)\n\n[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=yigitkonur%2Fwrite-into-menubar)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n[yigitkonur](/yigitkonur) / **[write-into-menubar](/yigitkonur/write-into-menubar)** Public\n\n-   [Notifications](/login?return_to=%2Fyigitkonur%2Fwrite-into-menubar) You must be signed in to change notification settings\n-   [Fork 3](/login?return_to=%2Fyigitkonur%2Fwrite-into-menubar)\n-   [Star 34](/login?return_to=%2Fyigitkonur%2Fwrite-into-menubar)\n    \n\nWrite whatever you want into OSX menubar by using BitBar + Alfred Powerpack.\n\n[34 stars](/yigitkonur/write-into-menubar/stargazers) [3 forks](/yigitkonur/write-into-menubar/forks) [Branches](/yigitkonur/write-into-menubar/branches) [Tags](/yigitkonur/write-into-menubar/tags) [Activity](/yigitkonur/write-into-menubar/activity)\n\n[Star](/login?return_to=%2Fyigitkonur%2Fwrite-into-menubar)\n\n[Notifications](/login?return_to=%2Fyigitkonur%2Fwrite-into-menubar) You must be signed in to change notification settings\n\n# yigitkonur/write-into-menubar\n\n  \n\n¬†master\n\n[Branches](/yigitkonur/write-into-menubar/branches)[Tags](/yigitkonur/write-into-menubar/tags)\n\n[](/yigitkonur/write-into-menubar/branches)[](/yigitkonur/write-into-menubar/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\nName\n\nName\n\nLast commit message\n\nLast commit date\n\n## Latest commit\n\n## History\n\n[6 Commits](/yigitkonur/write-into-menubar/commits/master/)\n\n[](/yigitkonur/write-into-menubar/commits/master/)\n\n[readme.md](/yigitkonur/write-into-menubar/blob/master/readme.md \"readme.md\")\n\n[readme.md](/yigitkonur/write-into-menubar/blob/master/readme.md \"readme.md\")\n\n[write-into-menubar-alfred-workflow.alfredworkflow](/yigitkonur/write-into-menubar/blob/master/write-into-menubar-alfred-workflow.alfredworkflow \"write-into-menubar-alfred-workflow.alfredworkflow\")\n\n[write-into-menubar-alfred-workflow.alfredworkflow](/yigitkonur/write-into-menubar/blob/master/write-into-menubar-alfred-workflow.alfredworkflow \"write-into-menubar-alfred-workflow.alfredworkflow\")\n\n[write-into-menubar-bitbar-plugin.1s.sh](/yigitkonur/write-into-menubar/blob/master/write-into-menubar-bitbar-plugin.1s.sh \"write-into-menubar-bitbar-plugin.1s.sh\")\n\n[write-into-menubar-bitbar-plugin.1s.sh](/yigitkonur/write-into-menubar/blob/master/write-into-menubar-bitbar-plugin.1s.sh \"write-into-menubar-bitbar-plugin.1s.sh\")\n\nView all files\n\n## Repository files navigation\n\n# Write Text into Menubar (BitBar + Alfred)\n\n[](#write-text-into-menubar-bitbar--alfred)\n\nDo you deal with tens of different tasks in a hour and end up with forgetting what you do 10 seconds ago? No prob!\n\n[![How To Use](https://camo.githubusercontent.com/0daf523ada9e1547ed52f72a61c80ffc54b477f8e891d7ee11fe421ec5dc5c51/68747470733a2f2f636c6475702e636f6d2f633145536348754d6a652e676966 \"With Animated GIFs\")](https://camo.githubusercontent.com/0daf523ada9e1547ed52f72a61c80ffc54b477f8e891d7ee11fe421ec5dc5c51/68747470733a2f2f636c6475702e636f6d2f633145536348754d6a652e676966)\n\nYou can write a text string to remember which will stick into your menubar - so that you can remember what you were going to do. Just install BitBar + Alfred Powerpack and start to use.\n\n### Installation\n\n[](#installation)\n\n-   Make sure that you have a BitBar extension.\n-   You BitBar plugin folder have to be installed ~/Documents/Bitbar-Plugins/\n-   Drag and drop your BitBar plugin into ~/Documents/Bitbar-Plugins/write-into-menubar-bitbar-plugin.1s.sh\n-   You need to run this Terminal Command to add chmod into your Bitbar Plugin\n\nchmod +x ~/Documents/Bitbar-Plugins/write-into-menubar-bitbar-plugin.1s.sh\n\n### Customization\n\n[](#customization)\n\nYou can also config this plugin according to you needs - just change variables in BitBar plugin:\n\n-   You can change text color by changing: quote\\_color=\"black\"\n-   You can set length of string by changing: max\\_chars=\"30\"\n\n### Dependencies\n\n[](#dependencies)\n\nYou need to have BitBar (completely free) and Alfred Powerpack (paid one) to use this plugin.\n\nTake a look to download then:\n\n-   [Alfred 3](https://www.alfredapp.com/) to run .AlfredWorkflow\n-   [BitBar](https://getbitbar.com/) to show final result of .sh plugin\n\nThis plugin is made by [Yiƒüit Konur](https://github.com/yigitkonur) for my daily routine at [Zeo](https://zeo.org). If you need to get a support - please open an issue.\n\n_Special thanks for [Jan Gro√ü](https://getbitbar.com/contributors/JanGross) to inspire me by his \"Daily Quote\" plugin to create my own._\n\n## About\n\nWrite whatever you want into OSX menubar by using BitBar + Alfred Powerpack.\n\n### Topics\n\n[alfred](/topics/alfred \"Topic: alfred\") [workflow](/topics/workflow \"Topic: workflow\") [bitbar](/topics/bitbar \"Topic: bitbar\")\n\n### Resources\n\n[Readme](#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](/yigitkonur/write-into-menubar/activity)\n\n### Stars\n\n[**34** stars](/yigitkonur/write-into-menubar/stargazers)\n\n### Watchers\n\n[**3** watching](/yigitkonur/write-into-menubar/watchers)\n\n### Forks\n\n[**3** forks](/yigitkonur/write-into-menubar/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fyigitkonur%2Fwrite-into-menubar&report=yigitkonur+%28user%29)\n\n## [Releases](/yigitkonur/write-into-menubar/releases)\n\nNo releases published\n\n## [Packages 0](/users/yigitkonur/packages?repo_name=write-into-menubar)\n\nNo packages published  \n\n## Languages\n\n-   [Shell 100.0%](/yigitkonur/write-into-menubar/search?l=shell)\n\nYou can‚Äôt perform that action at this time."
    }
  ]
}
